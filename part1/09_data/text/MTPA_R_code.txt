
# Predictive Model for Los Angeles Dodgers Promotion and Attendance 

library(car)  # special functions for linear regression
library(lattice)  # graphics package 

# read in data and create a data frame called dodgers
dodgers <- read.csv("dodgers.csv")
print(str(dodgers))  # check the structure of the data frame

# define an ordered day-of-week variable 
# for plots and data summaries
dodgers$ordered_day_of_week <- with(data=dodgers,
  ifelse ((day_of_week == "Monday"),1,
  ifelse ((day_of_week == "Tuesday"),2,
  ifelse ((day_of_week == "Wednesday"),3,
  ifelse ((day_of_week == "Thursday"),4,
  ifelse ((day_of_week == "Friday"),5,
  ifelse ((day_of_week == "Saturday"),6,7)))))))
dodgers$ordered_day_of_week <- factor(dodgers$ordered_day_of_week, levels=1:7,
labels=c("Mon", "Tue", "Wed", "Thur", "Fri", "Sat", "Sun"))

# exploratory data analysis with standard R graphics: attendance by day of week
with(data=dodgers,plot(ordered_day_of_week, attend/1000, 
xlab = "Day of Week", ylab = "Attendance (thousands)", 
col = "violet", las = 1))

# when do the Dodgers use bobblehead promotions
with(dodgers, table(bobblehead,ordered_day_of_week)) # bobbleheads on Tuesday

# define an ordered month variable 
# for plots and data summaries
dodgers$ordered_month <- with(data=dodgers,
  ifelse ((month == "APR"),4,
  ifelse ((month == "MAY"),5,
  ifelse ((month == "JUN"),6,
  ifelse ((month == "JUL"),7,
  ifelse ((month == "AUG"),8,
  ifelse ((month == "SEP"),9,10)))))))
dodgers$ordered_month <- factor(dodgers$ordered_month, levels=4:10,
labels = c("April", "May", "June", "July", "Aug", "Sept", "Oct"))

# exploratory data analysis with standard R graphics: attendance by month 
with(data=dodgers,plot(ordered_month,attend/1000, xlab = "Month", 
ylab = "Attendance (thousands)", col = "light blue", las = 1))

# exploratory data analysis by R lattice graphics
# looking at attendance and conditioning on day/night
# the skies and whether or not fireworks are displayed






# prepare a graphical summary of the dodgers data
group.labels <- c("No Fireworks","Fireworks")
group.symbols <- c(1,2)
group.colors <- c("dark blue","dark red") 
xyplot(attend/1000 ~ temp | skies + day_night, 
  data = dodgers, groups = fireworks, pch = group.symbols, 
  aspect = 1, cex = 1.25, col = group.colors,
  layout = c(2, 2), type = c("p", "g"),
  strip=strip.custom(strip.levels=TRUE, strip.names=FALSE, style=1),
  xlab = "Temperature (Degrees Fahrenheit)", 
  ylab = "Attendance (thousands)",
  key = list(space = "right", 
  text = list(rev(group.labels), col = rev(group.colors)),
  points = list(pch = rev(group.symbols), col = rev(group.colors))))  

# exploratory data analysis by R lattice graphics
# strip plot of attendance by opponent and day/night game
group.labels <- c("Day","Night")
group.symbols <- c(1,20)
group.symbols.size <- c(1,1.25)
bwplot(opponent ~ attend/1000, data = dodgers, groups = day_night, 
  xlab = "Attendance (thousands)",
  panel = function(x, y, groups, subscripts, ...) {
    panel.grid(h = (length(levels(dodgers$opponent)) - 1), v = -1)
    panel.stripplot(x, y, groups = groups, subscripts = subscripts, 
    cex = group.symbols.size, pch = group.symbols, col = "blue")
    },
  key = list(space = "top", 
  text = list(group.labels,col = "black"),
  points = list(pch = group.symbols, cex = group.symbols.size, col = "blue")))
     
# specify a simple model with bobblehead entered last
my.model <- {attend ~ ordered_month + ordered_day_of_week + bobblehead}

# employ a training-and-test regimen
set.seed(1234) # set seed for repeatability of training-and-test split
training_test <- c(rep(1,length=trunc((2/3)*nrow(dodgers))),
rep(2,length=(nrow(dodgers) - trunc((2/3)*nrow(dodgers)))))
dodgers$training_test <- sample(training_test) # random permutation 
dodgers$training_test <- factor(dodgers$training_test, 
  levels=c(1,2), labels=c("TRAIN","TEST"))
dodgers.train <- subset(dodgers, training_test == "TRAIN")
print(str(dodgers.train)) # check training data frame
dodgers.test <- subset(dodgers, training_test == "TEST")
print(str(dodgers.test)) # check test data frame

# fit the model to the training set
train.model.fit <- lm(my.model, data = dodgers.train)
# obtain predictions from the training set
dodgers.train$predict_attend <- predict(train.model.fit) 




# evaluate the fitted model on the test set
dodgers.test$predict_attend <- predict(train.model.fit, newdata = dodgers.test)

# compute the proportion of response variance
# accounted for when predicting out-of-sample
cat("\n","Proportion of Test Set Variance Accounted for: ",
round((with(dodgers.test,cor(attend,predict_attend)^2)),digits=3),"\n",sep="")

# merge the training and test sets for plotting
dodgers.plotting.frame <- rbind(dodgers.train,dodgers.test)

# R lattice graphics for showing the performance of a predictive model   
group.labels <- c("No Bobbleheads","Bobbleheads")
group.symbols <- c(1,2)
group.colors <- c("dark blue","dark red") 
xyplot(predict_attend/1000 ~ attend/1000 | training_test, 
  data = dodgers.plotting.frame, groups = bobblehead, cex = 2,
  pch = group.symbols, col = group.colors, layout = c(2, 1), 
  xlim = c(20,65), ylim = c(20,65), aspect=1, type = c("p","g"),
  panel=function(x,y, ...) {
    panel.xyplot(x,y,...)
    panel.segments(25,25,60,60,col="black",cex=2)
    },
  strip=function(...) strip.default(..., style=1),
  xlab = "Actual Attendance (thousands)", 
  ylab = "Predicted Attendance (thousands)",
  key = list(space = "top", 
    text = list(rev(group.labels),col = rev(group.colors)),
    points = list(pch = rev(group.symbols), col = rev(group.colors))))    
        
# use the full data set to obtain an estimate of the increase in
# attendance due to bobbleheads, controlling for other factors 
my.model.fit <- lm(my.model, data = dodgers)  # use all available data
print(summary(my.model.fit))
# tests statistical significance of the bobblehead promotion
# type I anova computes sums of squares for sequential tests
print(anova(my.model.fit))  

cat("\n","Estimated Effect of Bobblehead Promotion on Attendance: ", 
  round(my.model.fit$coefficients[length(my.model.fit$coefficients)],
digits = 0),"\n",sep="")

# standard graphics provide diagnostic plots
plot(my.model.fit)

# additional model diagnostics drawn from the car package
library(car)
residualPlots(my.model.fit)
marginalModelPlots(my.model.fit)
print(outlierTest(my.model.fit))




# Traditional Conjoint Analysis

#  user-defined function for spine chart
load(file="mtpa_spine_chart.Rdata")

# spine chart accommodates up to 45 part-worths on one page
# |part-worth| <= 40 can be plotted directly on the spine chart
# |part-worths| > 40 can be accommodated through standardization

print.digits <- 2  # set number of digits on print and spine chart

library(support.CEs)  # package for survey construction 

# generate a balanced set of product profiles for survey
provider.survey <- Lma.design(attribute.names = 
  list(brand = c("AT&T","T-Mobile","US Cellular","Verizon"), 
  startup = c("$100","$200","$300","$400"), 
  monthly = c("$100","$200","$300","$400"),
  service = c("4G NO","4G YES"), 
  retail = c("Retail NO","Retail YES"),
  apple = c("Apple NO","Apple YES"), 
  samsung = c("Samsung NO","Samsung YES"), 
  google = c("Nexus NO","Nexus YES")), nalternatives = 1, nblocks=1, seed=9999)
print(questionnaire(provider.survey))  # print survey design for review

sink("questions_for_survey.txt")  # send survey to external text file
questionnaire(provider.survey)
sink() # send output back to the screen

# user-defined function for plotting descriptive attribute names 
effect.name.map <- function(effect.name) { 
  if(effect.name=="brand") return("Mobile Service Provider")
  if(effect.name=="startup") return("Start-up Cost")
  if(effect.name=="monthly") return("Monthly Cost")
  if(effect.name=="service") return("Offers 4G Service")
  if(effect.name=="retail") return("Has Nearby Retail Store")
  if(effect.name=="apple") return("Sells Apple Products")
  if(effect.name=="samsung") return("Sells Samsung Products")
  if(effect.name=="google") return("Sells Google/Nexus Products")
  } 

# read in conjoint survey profiles with respondent ranks
conjoint.data.frame <- read.csv("mobile_services_ranking.csv")

# set up sum contrasts for effects coding as needed for conjoint analysis
options(contrasts=c("contr.sum","contr.poly"))

# fit linear regression model using main effects only (no interaction terms)
main.effects.model <- lm(ranking ~ brand + startup + monthly + service + 
  retail + apple + samsung + google, data=conjoint.data.frame)
print(summary(main.effects.model)) 


# save key list elements of the fitted model as needed for conjoint measures
conjoint.results <- 
  main.effects.model[c("contrasts","xlevels","coefficients")]
conjoint.results$attributes <- names(conjoint.results$contrasts)

# compute and store part-worths in the conjoint.results list structure
part.worths <- conjoint.results$xlevels  # list of same structure as xlevels
end.index.for.coefficient <- 1  # intitialize skipping the intercept
part.worth.vector <- NULL # used for accumulation of part worths
for(index.for.attribute in seq(along=conjoint.results$contrasts)) {
  nlevels <- length(unlist(conjoint.results$xlevels[index.for.attribute]))
  begin.index.for.coefficient <- end.index.for.coefficient + 1
  end.index.for.coefficient <- begin.index.for.coefficient + nlevels -2
  last.part.worth <- -sum(conjoint.results$coefficients[
    begin.index.for.coefficient:end.index.for.coefficient])
  part.worths[index.for.attribute] <- 
    list(as.numeric(c(conjoint.results$coefficients[
      begin.index.for.coefficient:end.index.for.coefficient],
      last.part.worth)))
  part.worth.vector <- 
    c(part.worth.vector,unlist(part.worths[index.for.attribute]))    
  } 
conjoint.results$part.worths <- part.worths

# compute standardized part-worths
standardize <- function(x) {(x - mean(x)) / sd(x)}
conjoint.results$standardized.part.worths <- 
  lapply(conjoint.results$part.worths,standardize)
 
# compute and store part-worth ranges for each attribute 
part.worth.ranges <- conjoint.results$contrasts
for(index.for.attribute in seq(along=conjoint.results$contrasts)) 
  part.worth.ranges[index.for.attribute] <- 
  dist(range(conjoint.results$part.worths[index.for.attribute]))
conjoint.results$part.worth.ranges <- part.worth.ranges

sum.part.worth.ranges <- sum(as.numeric(conjoint.results$part.worth.ranges))

# compute and store importance values for each attribute 
attribute.importance <- conjoint.results$contrasts
for(index.for.attribute in seq(along=conjoint.results$contrasts)) 
  attribute.importance[index.for.attribute] <- 
  (dist(range(conjoint.results$part.worths[index.for.attribute]))/
  sum.part.worth.ranges) * 100
conjoint.results$attribute.importance <- attribute.importance
  
# data frame for ordering attribute names
attribute.name <- names(conjoint.results$contrasts)
attribute.importance <- as.numeric(attribute.importance)
temp.frame <- data.frame(attribute.name,attribute.importance)
conjoint.results$ordered.attributes <- 
  as.character(temp.frame[sort.list(
  temp.frame$attribute.importance,decreasing = TRUE),"attribute.name"])


# respondent internal consistency added to list structure
conjoint.results$internal.consistency <- summary(main.effects.model)$r.squared 
 
# user-defined function for printing conjoint measures
if (print.digits == 2) 
  pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))} 
if (print.digits == 3) 
  pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))} 
 
# report the conjoint measures to console 
# use pretty.print to provide nicely formated output
for(k in seq(along=conjoint.results$ordered.attributes)) {
  cat("\n","\n")
  cat(conjoint.results$ordered.attributes[k],"Levels: ",
  unlist(conjoint.results$xlevels[conjoint.results$ordered.attributes[k]]))
  
  cat("\n"," Part-Worths:  ")
  cat(pretty.print(unlist(conjoint.results$part.worths
    [conjoint.results$ordered.attributes[k]])))
    
  cat("\n"," Standardized Part-Worths:  ")
  cat(pretty.print(unlist(conjoint.results$standardized.part.worths
    [conjoint.results$ordered.attributes[k]])))  
    
  cat("\n"," Attribute Importance:  ")
  cat(pretty.print(unlist(conjoint.results$attribute.importance
    [conjoint.results$ordered.attributes[k]])))
  }

# plotting of spine chart begins here
# all graphical output is routed to external pdf file
pdf(file = "fig_preference_mobile_services_results.pdf", width=8.5, height=11)
spine.chart(conjoint.results)
dev.off()  # close the graphics output device


# Computer Choice Study... Brand Equity Research

library(caret)  # for confusion matrix for choice performance analysis  

# load spine.chart function 
load(file="mtpa_spine_chart.Rdata")

# load market simulation utilities
load(file="mspa_market_simulation_utilities.RData")

# read in the data from a case study in computer choice.
complete.data.frame <- read.csv("computer_choice_study.csv")

# user-defined function for plotting descriptive attribute names 
effect.name.map <- function(effect.name) { 
  if(effect.name=="brand") return("Manufacturer/Brand")
  if(effect.name=="compat") return("Compatibility with Windows 95")
  if(effect.name=="perform") return("Performance")
  if(effect.name=="reliab") return("Reliability")
  if(effect.name=="learn") return("Learning Time (4 to 32 hours)")
  if(effect.name=="price") return("Price ($1,000 to $2,750)")
  } 

print.digits <- 2
# user-defined function for printing conjoint measures
if (print.digits == 2) 
  pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))} 
if (print.digits == 3) 
  pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))} 
 
# set up sum contrasts for effects coding
options(contrasts=c("contr.sum","contr.poly"))

# employ a training-and-test regimen across survey choice sets/items 
test.set.ids <- c("3","7","11","15")  # select four sets/items
training.set.ids <- setdiff(unique(complete.data.frame$setid),test.set.ids)
training.data.frame <- 
  subset(complete.data.frame,subset=(setid %in% training.set.ids))
test.data.frame <- 
  subset(complete.data.frame,subset=(setid %in% test.set.ids))

# fit a logistic regression model to the training data
main.effects.model <- 
  glm(choice ~ brand + compat + perform + reliab + learn + price, 
    data = training.data.frame, binomial(link = "logit"))

# aggregate analysis using logistic regression
print(summary(main.effects.model))
print(anova(main.effects.model,test="Chisq"))  # likelihood ratio tests

# save key list elements of the fitted model as needed for conjoint measures


# initially we obtain xlevels for the categorical attributes only
conjoint.results <- 
  main.effects.model[c("xlevels","coefficients")]

# continuous attribute names are specific to this study
continuous.attribute.names <- 
  c("compat", "perform", "reliab", "learn","price") 
conjoint.results$attributes <- c(names(conjoint.results$xlevels), 
  continuous.attribute.names) 

# compute and store part-worths for categorical attributes 
# in the conjoint.results list structure
part.worths <- conjoint.results$xlevels  # list of same structure as xlevels
end.index.for.coefficient <- 1  # intitialize skipping the intercept
part.worth.vector <- NULL # used for accumulation of part worths
for (index.for.attribute in seq(along=conjoint.results$xlevels)) {
  nlevels <- length(unlist(conjoint.results$xlevels[index.for.attribute]))
  begin.index.for.coefficient <- end.index.for.coefficient + 1
  end.index.for.coefficient <- begin.index.for.coefficient + nlevels -2
  last.part.worth <- -sum(conjoint.results$coefficients[
    begin.index.for.coefficient:end.index.for.coefficient])
  part.worths[index.for.attribute] <- 
    list(as.numeric(c(conjoint.results$coefficients[
      begin.index.for.coefficient:end.index.for.coefficient],
      last.part.worth)))
  part.worth.vector <- 
    c(part.worth.vector,unlist(part.worths[index.for.attribute]))    
  } 
conjoint.results$part.worths <- part.worths

# add continuous attributes at high and low values 
# obtained from the main.effects.model list structure
# note the index from xlevels prior to working with continuous attributes
index.for.attribute <- length(conjoint.results$xlevels)  
for (index.conatt in seq(along=continuous.attribute.names)) {
  this.att.name <- continuous.attribute.names[index.conatt]
  this.att.min <- min(eval(parse(text=paste("main.effects.model$model$",
    this.att.name,sep=""))))
  this.att.max <- max(eval(parse(text=paste("main.effects.model$model$",
    this.att.name,sep=""))))
  lowest <- this.att.min * conjoint.results$coefficients[this.att.name]
  highest <- this.att.max * conjoint.results$coefficients[this.att.name]
  centered.lowest <- lowest - ((lowest + highest)/2)
  centered.highest <- highest - ((lowest + highest)/2)
  
  # add elements to the conjoint.measures list structure
  index.for.attribute <- index.for.attribute + 1  # increment index
  conjoint.results$xlevels[[index.for.attribute]] <- 
    as.vector(c("Lowest","Highest"))
  conjoint.results$part.worths[[index.for.attribute]] <- 
    as.vector(c(centered.lowest,centered.highest))
  part.worth.vector <- 
    c(part.worth.vector,unlist(part.worths[index.for.attribute])) 
  }
  
 names(conjoint.results$xlevels) <- conjoint.results$attributes 
 names(conjoint.results$part.worths) <- conjoint.results$attributes 
  
# compute standardized part-worths
standardize <- function(x) {(x - mean(x)) / sd(x)}
conjoint.results$standardized.part.worths <- 
  lapply(conjoint.results$part.worths,standardize)
 
# compute and store part-worth ranges for each attribute 
part.worth.ranges <- conjoint.results$xlevels  # initialize as list object
for (index.for.attribute in seq(along=conjoint.results$xlevels)) 
  part.worth.ranges[index.for.attribute] <- 
  dist(range(conjoint.results$part.worths[index.for.attribute]))
conjoint.results$part.worth.ranges <- part.worth.ranges

sum.part.worth.ranges <- sum(as.numeric(conjoint.results$part.worth.ranges))

# compute and store importance values for each attribute 
attribute.importance <- conjoint.results$xlevels  # initialize as list object
for (index.for.attribute in seq(along=conjoint.results$xlevels)) 
  attribute.importance[index.for.attribute] <- 
  (dist(range(conjoint.results$part.worths[index.for.attribute]))/
  sum.part.worth.ranges) * 100
conjoint.results$attribute.importance <- attribute.importance
 
# data frame for ordering attribute names
attribute.name <- names(conjoint.results$xlevel)
attribute.importance <- as.numeric(attribute.importance)
temp.frame <- data.frame(attribute.name,attribute.importance)
conjoint.results$ordered.attributes <- 
  as.character(temp.frame[sort.list(
  temp.frame$attribute.importance,decreasing = TRUE),"attribute.name"])

# respondent internal consistency added to list structure
# internal consistency relates to individuals
# for aggregate logistic regression this is not defined
conjoint.results$internal.consistency <- NA    

# plotting of spine chart from logistic regression solution
# all graphical output is routed to external pdf file
pdf(file = "fig_brand_computer_logistic_regression.pdf", 
  width=8.5, height=11)
spine.chart(conjoint.results, do.standardization = FALSE)
dev.off()  # close the graphics output device

# evaluate model by obtaining predicted choices for the training data
training.predicted.probability <- predict(main.effects.model, 
  type = "response") 
# use choice predictor function for the training data  
training.predicted.choice <- 
  choice.set.predictor(training.predicted.probability)  
training.actual.choice <- factor(training.data.frame$choice, levels = c(0,1), 
  labels = c("NO","YES"))  
  
  
# look for sensitivity > 0.25 for four-profile choice sets 
training.set.performance <- confusionMatrix(data = training.predicted.choice, 
  reference = training.actual.choice, positive = "YES")
# report choice prediction sensitivity for training data
cat("\n\nTraining choice set sensitivity = ",
  sprintf("%1.1f",training.set.performance$byClass[1]*100)," Percent",sep="")


# evaluate model by obtaining predicted choices for the test data
test.predicted.probability <- predict(main.effects.model, 
  newdata = test.data.frame, 
  type = "response") 
# use choice predictor function for the test data  
test.predicted.choice <- choice.set.predictor(test.predicted.probability)  
test.actual.choice <- factor(test.data.frame$choice, levels = c(0,1), 
  labels = c("NO","YES"))  
# look for sensitivity > 0.25 for four-profile choice sets 
test.set.performance <- confusionMatrix(data = test.predicted.choice, 
  reference = test.actual.choice, positive = "YES")
# report choice prediction sensitivity for test data
cat("\n\nTest choice set sensitivity = ",
  sprintf("%1.1f",test.set.performance$byClass[1]*100)," Percent",sep="")


# Hierarchical Bayes Part-Worth Estimation: Training and Test

# load market simulation utilities
load(file="mspa_market_simulation_utilities.RData")

library(ChoiceModelR)  # for Hierarchical Bayes Estimation

library(caret)  # for confusion matrix function

# read in the data from a case study in computer choice.
complete.data.frame <- read.csv("computer_choice_study.csv")

print.digits <- 2
# user-defined function for printing conjoint measures
if (print.digits == 2) 
  pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))} 
if (print.digits == 3) 
  pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))} 
 
# set up sum contrasts for effects coding
options(contrasts=c("contr.sum","contr.poly"))

# employ a training-and-test regimen across survey sets/items 
test.set.ids <- c("3","7","11","15")  # select four sets/items
training.set.ids <- setdiff(unique(complete.data.frame$setid),test.set.ids)
training.data.frame <- 
  subset(complete.data.frame,subset=(setid %in% training.set.ids))
test.data.frame <- 
  subset(complete.data.frame,subset=(setid %in% test.set.ids))

UniqueID <- unique(training.data.frame$id)
# set up zero priors
cc.priors <- matrix(0,nrow=length(UniqueID),ncol=13) 

# we could use coefficients from aggregate model as starting values
# here we comment out the code needed to do that
# aggregate.cc.betas <- c(as.numeric(conjoint.results$coefficients)[2:7],
#  -sum(as.numeric(conjoint.results$coefficients)[2:7]),
#  as.numeric(conjoint.results$coefficients)[8:13])
# clone aggregate part-worths across the individuals in the study
# set up Bayesian priors
# cc.priors <- matrix(0,nrow=length(UniqueID),ncol=length(aggregate.cc.betas)) 
# for(index.for.ID in seq(along=UniqueID))
# cc.priors[index.for.ID,] <- aggregate.cc.betas

colnames(cc.priors) <- c("A1B1","A1B2","A1B3","A1B4","A1B5","A1B6","A1B7",
  "A1B8","A2B1","A3B1","A4B1","A5B1","A6B1")
# note that the actual names are as follows: 
AB.names <- c("Apple","Compaq","Dell","Gateway","HP","IBM","Sony","Sun",
  "Compatibility","Performance","Reliability","Learning","Price")



# set up run parameters for the MCMC
# using aggregate beta estimates to get started
truebetas <- cc.priors
cc.xcoding <- c(0,1,1,1,1,1)  # first variable categorical others continuous
cc.attlevels <- c(8,8,4,2,8,8) # test run with brand price and performance
# no constraint for order on brand so 8x8 matrix of zeroes
c1 <- matrix(0,ncol=8,nrow=8)
# compatibility is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c2 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)
# performance is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c3 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)
# reliability is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c4 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)
# learning has expected order... higher prices less valued
# continuous attributes have 1x1 matrix representation
c5 <- matrix(-1, ncol = 1, nrow = 1, byrow = TRUE)
# price has expected order... higher prices less valued
# continuous attributes have 1x1 matrix representation
c6 <- matrix(-1, ncol = 1, nrow = 1, byrow = TRUE)
cc.constraints <- list(c1,c2,c3,c4,c5,c6)
# controls for length of run and sampling from end of run
# cc.mcmc <- list(R = 10, use = 10) # fast trial run
# set run parameters 10000 total iterations with estimates based on last 2000
cc.mcmc <- list(R = 10000, use = 2000) # run parameters
# run options
cc.options <- list(none=FALSE, save=TRUE, keep=1)

# set up the data frame for analysis
# redefine set ids so they are a complete set 1-12 as needed for HB functions
training.data.frame$newsetid <- training.data.frame$setid
training.data.frame$newsetid <- ifelse((training.data.frame$newsetid == 16),
  3,training.data.frame$newsetid)
training.data.frame$newsetid <- ifelse((training.data.frame$newsetid == 14),
  7,training.data.frame$newsetid)
training.data.frame$newsetid <- ifelse((training.data.frame$newsetid == 13),
  11,training.data.frame$newsetid)

UnitID <- training.data.frame$id
Set <- as.integer(training.data.frame$newsetid)
Alt <- as.integer(training.data.frame$position)
X_1 <- as.integer(training.data.frame$brand) # categories by brand
X_2 <- as.integer(training.data.frame$compat)  # integer values 1 to 8
X_3 <- as.integer(training.data.frame$perform)  # integer values 1 to 4
X_4 <- as.integer(training.data.frame$reliab)  # integer values 1 to 2
X_5 <- as.integer(training.data.frame$learn)  # integer values 1 to 8
X_6 <- as.integer(training.data.frame$price)  # integer values 1 to 8
y <- as.numeric(training.data.frame$choice)  # using special response coding

cc.data <- data.frame(UnitID,Set,Alt,X_1,X_2,X_3,X_4,X_5,X_6,y)



# now for the estimation... be patient
set.seed(9999)  # for reproducible results
out <- choicemodelr(data=cc.data, xcoding = cc.xcoding, 
  mcmc = cc.mcmc, options = cc.options, constraints = cc.constraints)

# out provides a list for the posterior parameter estimates 
# for the runs sampled (use = 2000)

# the MCMC beta parameter estimates are traced on the screen as it runs

# individual part-worth estimates are provided in the output file RBetas.csv
# the final estimates are printed to RBetas.csv with columns labeled as
#  A1B1 = first attribute first level
#  A1B2 = first attribute second level
#  ....
#  A2B1 = second attribute first level
#  ....
# gather data from HB posterior parameter distributions
# we imposed constraints on all continuous parameters so we use betadraw.c
posterior.mean <- matrix(0, nrow = dim(out$betadraw.c)[1], 
  ncol = dim(out$betadraw.c)[2])
posterior.sd <- matrix(0, nrow = dim(out$betadraw.c)[1], 
  ncol = dim(out$betadraw.c)[2])
for(index.row in 1:dim(out$betadraw.c)[1])
for(index.col in 1:dim(out$betadraw.c)[2]) { 
  posterior.mean[index.row,index.col] <- 
    mean(out$betadraw.c[index.row,index.col,])
  posterior.sd[index.row,index.col] <- 
    sd(out$betadraw.c[index.row,index.col,])
  }
# HB program uses effects coding for categorical variables and
# mean-centers continuous variables across the levels appearing in the data
# working with data for one respondent at a time we compute predicted choices
# for both the training and test choice sets
create.design.matrix <- function(input.data.frame.row) {
  xdesign.row <- numeric(12)
  if (input.data.frame.row$brand == "Apple") 
    xdesign.row[1:7] <- c(1,0,0,0,0,0,0)  
  if (input.data.frame.row$brand == "Compaq") 
    xdesign.row[1:7] <- c(0,1,0,0,0,0,0)  
  if (input.data.frame.row$brand == "Dell") 
    xdesign.row[1:7] <- c(0,0,1,0,0,0,0)  
  if (input.data.frame.row$brand == "Gateway") 
    xdesign.row[1:7] <- c(0,0,0,1,0,0,0)  
  if (input.data.frame.row$brand == "HP") 
    xdesign.row[1:7] <- c(0,0,0,0,1,0,0)  
  if (input.data.frame.row$brand == "IBM") 
    xdesign.row[1:7] <- c(0,0,0,0,0,1,0)  
  if (input.data.frame.row$brand == "Sony") 
    xdesign.row[1:7] <- c(0,0,0,0,0,0,1)  
  if (input.data.frame.row$brand == "Sun") 
    xdesign.row[1:7] <- c(-1,-1,-1,-1,-1,-1,-1)    
  
 
 
  xdesign.row[8] <- input.data.frame.row$compat -4.5 
  xdesign.row[9] <- input.data.frame.row$perform -2.5
  xdesign.row[10] <- input.data.frame.row$reliab -1.5 
  xdesign.row[11] <- input.data.frame.row$learn -4.5
  xdesign.row[12] <- input.data.frame.row$price -4.5 
  t(as.matrix(xdesign.row))  # return row of design matrix
  }


# evaluate performance in the training set
training.choice.utility <- NULL  # initialize utility vector
# work with one row of respondent training data frame at a time
# create choice prediction using the individual part-worths
list.of.ids <- unique(training.data.frame$id)
for (index.for.id in seq(along=list.of.ids)) {
  this.id.part.worths <- posterior.mean[index.for.id,] 
  this.id.data.frame <- subset(training.data.frame, 
    subset=(id == list.of.ids[index.for.id]))
  for (index.for.profile in 1:nrow(this.id.data.frame)) {   
    training.choice.utility <- c(training.choice.utility,
      create.design.matrix(this.id.data.frame[index.for.profile,]) %*%
      this.id.part.worths)
    }  
  }  

training.predicted.choice <- 
  choice.set.predictor(training.choice.utility)
training.actual.choice <- factor(training.data.frame$choice, levels = c(0,1), 
  labels = c("NO","YES"))  
# look for sensitivity > 0.25 for four-profile choice sets 
training.set.performance <- confusionMatrix(data = training.predicted.choice, 
  reference = training.actual.choice, positive = "YES")
# report choice prediction sensitivity for training data
cat("\n\nTraining choice set sensitivity = ",
  sprintf("%1.1f",training.set.performance$byClass[1]*100)," Percent",sep="")


# evaluate performance in the test set
test.choice.utility <- NULL  # initialize utility vector
# work with one row of respondent test data frame at a time
# create choice prediction using the individual part-worths
list.of.ids <- unique(test.data.frame$id)
for (index.for.id in seq(along=list.of.ids)) {
  this.id.part.worths <- posterior.mean[index.for.id,] 
  this.id.data.frame <- subset(test.data.frame, 
    subset=(id == list.of.ids[index.for.id]))
  for (index.for.profile in 1:nrow(this.id.data.frame)) {    
    test.choice.utility <- c(test.choice.utility,
      create.design.matrix(this.id.data.frame[index.for.profile,]) %*%
      this.id.part.worths)
    }  
  }  



test.predicted.choice <- 
  choice.set.predictor(test.choice.utility)
test.actual.choice <- factor(test.data.frame$choice, levels = c(0,1), 
  labels = c("NO","YES"))  
# look for sensitivity > 0.25 for four-profile choice sets 
test.set.performance <- confusionMatrix(data = test.predicted.choice, 
  reference = test.actual.choice, positive = "YES")
# report choice prediction sensitivity for test data
cat("\n\nTest choice set sensitivity = ",
  sprintf("%1.1f",test.set.performance$byClass[1]*100)," Percent",sep="")

# having demonstrated the predictive power of the HB model...
# we return to the complete set of 16 choice sets to obtain 
# individual-level part-worths for further analysis
# this we do in the next program listing



# Hierarchical Bayes Part-Worth Estimation and Study of Consumer Preferences

# having demonstrated the predictive power of the HB model...
# we now return to the complete set of 16 choice sets to obtain 
# individual-level part-worths for further analysis 
# analysis guided by ternary model of consumer preference and market response
# brand loyalty... price sensitivity... and feature focus... are key aspects
# to consider in determining pricing policy

library(lattice)  # package for lattice graphics 
library(vcd)  # graphics package with mosaic plots for mosaic and ternary plots
library(ggplot2)  # package ggplot implements Grammar of Graphics approach
library(ChoiceModelR)  # for Hierarchical Bayes Estimation
library(caret)  # for confusion matrix... evaluation of choice set predictions

# load split-plotting utilities for work with ggplot
load("mtpa_split_plotting_utilities.Rdata")
# load market simulation utilities 
load(file="mspa_market_simulation_utilities.RData")

# read in the data from a case study in computer choice.
complete.data.frame <- read.csv("computer_choice_study.csv")
# we employed a training-and-test regimen in previous research work
# here we will be using the complete data from the computer choice study
working.data.frame <- complete.data.frame

# user-defined function for plotting descriptive attribute names 
effect.name.map <- function(effect.name) { 
  if(effect.name=="brand") return("Manufacturer/Brand")
  if(effect.name=="compat") return("Compatibility with Windows 95")
  if(effect.name=="perform") return("Performance")
  if(effect.name=="reliab") return("Reliability")
  if(effect.name=="learn") return("Learning Time (4 to 32 hours)")
  if(effect.name=="price") return("Price ($1,000 to $2,750)")
  } 
print.digits <- 2
# user-defined function for printing conjoint measures
if (print.digits == 2) 
  pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))} 
if (print.digits == 3) 
  pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))} 

# set up sum contrasts for effects coding
options(contrasts=c("contr.sum","contr.poly"))

UniqueID <- unique(working.data.frame$id)
# set up zero priors
cc.priors <- matrix(0,nrow=length(UniqueID),ncol=13) 
colnames(cc.priors) <- c("A1B1","A1B2","A1B3","A1B4","A1B5","A1B6","A1B7",
  "A1B8","A2B1","A3B1","A4B1","A5B1","A6B1")



# note that the actual names are as follows: 
AB.names <- c("Apple","Compaq","Dell","Gateway","HP","IBM","Sony","Sun",
  "Compatibility","Performance","Reliability","Learning","Price")

# set up run parameters for the MCMC
# using aggregate beta estimates to get started
truebetas <- cc.priors
cc.xcoding <- c(0,1,1,1,1,1)  # first variable categorical others continuous
cc.attlevels <- c(8,8,4,2,8,8) # test run with brand price and performance
# no constraint for order on brand so 8x8 matrix of zeroes
c1 <- matrix(0,ncol=8,nrow=8)
# compatibility is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c2 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)
# performance is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c3 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)
# reliability is ordered higher numbers are better
# continuous attributes have 1x1 matrix representation
c4 <- matrix(1, ncol = 1, nrow = 1, byrow = TRUE)
# learning has expected order... higher prices less valued
# continuous attributes have 1x1 matrix representation
c5 <- matrix(-1, ncol = 1, nrow = 1, byrow = TRUE)
# price has expected order... higher prices less valued
# continuous attributes have 1x1 matrix representation
c6 <- matrix(-1, ncol = 1, nrow = 1, byrow = TRUE)
cc.constraints <- list(c1,c2,c3,c4,c5,c6)
# controls for length of run and sampling from end of run
# cc.mcmc <- list(R = 10, use = 10) # fast trial run
# set run parameters 10000 total iterations with estimates based on last 2000
cc.mcmc <- list(R = 10000, use = 2000) # run parameters
# run options
cc.options <- list(none=FALSE, save=TRUE, keep=1)

# set up the data frame for analysis
UnitID <- working.data.frame$id
Set <- as.integer(working.data.frame$setid)
Alt <- as.integer(working.data.frame$position)
X_1 <- as.integer(working.data.frame$brand) # categories by brand
X_2 <- as.integer(working.data.frame$compat)  # integer values 1 to 8
X_3 <- as.integer(working.data.frame$perform)  # integer values 1 to 4
X_4 <- as.integer(working.data.frame$reliab)  # integer values 1 to 2
X_5 <- as.integer(working.data.frame$learn)  # integer values 1 to 8
X_6 <- as.integer(working.data.frame$price)  # integer values 1 to 8
y <- as.numeric(working.data.frame$choice)  # using special response coding

cc.data <- data.frame(UnitID,Set,Alt,X_1,X_2,X_3,X_4,X_5,X_6,y)

# the estimation begins here... be patient
set.seed(9999)  # for reproducible results
out <- choicemodelr(data=cc.data, xcoding = cc.xcoding, 
  mcmc = cc.mcmc, options = cc.options, constraints = cc.constraints)



# out provides a list for the posterior parameter estimates 
# for the runs sampled (use = 2000)
# the MCMC beta parameter estimates are traced on the screen as it runs
# individual part-worth estimates are provided in the output file RBetas.csv
# the final estimates are printed to RBetas.csv with columns labeled as
#  A1B1 = first attribute first level
#  A1B2 = first attribute second level
#  ....
#  A2B1 = second attribute first level
#  ....
# gather data from HB posterior parameter distributions
# we imposed constraints on all continuous parameters so we use betadraw.c
posterior.mean <- matrix(0, nrow = dim(out$betadraw.c)[1], 
  ncol = dim(out$betadraw.c)[2])
posterior.sd <- matrix(0, nrow = dim(out$betadraw.c)[1], 
  ncol = dim(out$betadraw.c)[2])
for(index.row in 1:dim(out$betadraw.c)[1])
for(index.col in 1:dim(out$betadraw.c)[2]) { 
  posterior.mean[index.row,index.col] <- 
    mean(out$betadraw.c[index.row,index.col,])
  posterior.sd[index.row,index.col] <- 
    sd(out$betadraw.c[index.row,index.col,])
  }
# HB program uses effects coding for categorical variables and
# mean-centers continuous variables across the levels appearing in the data
# working with data for one respondent at a time we compute predicted choices
# for the full set of consumer responses
create.design.matrix <- function(input.data.frame.row) {
  xdesign.row <- numeric(12)
  if (input.data.frame.row$brand == "Apple") 
    xdesign.row[1:7] <- c(1,0,0,0,0,0,0)  
  if (input.data.frame.row$brand == "Compaq") 
    xdesign.row[1:7] <- c(0,1,0,0,0,0,0)  
  if (input.data.frame.row$brand == "Dell") 
    xdesign.row[1:7] <- c(0,0,1,0,0,0,0)  
  if (input.data.frame.row$brand == "Gateway") 
    xdesign.row[1:7] <- c(0,0,0,1,0,0,0)  
  if (input.data.frame.row$brand == "HP") 
    xdesign.row[1:7] <- c(0,0,0,0,1,0,0)  
  if (input.data.frame.row$brand == "IBM") 
    xdesign.row[1:7] <- c(0,0,0,0,0,1,0)  
  if (input.data.frame.row$brand == "Sony") 
    xdesign.row[1:7] <- c(0,0,0,0,0,0,1)  
  if (input.data.frame.row$brand == "Sun") 
    xdesign.row[1:7] <- c(-1,-1,-1,-1,-1,-1,-1)    
  xdesign.row[8] <- input.data.frame.row$compat -4.5 
  xdesign.row[9] <- input.data.frame.row$perform -2.5
  xdesign.row[10] <- input.data.frame.row$reliab -1.5 
  xdesign.row[11] <- input.data.frame.row$learn -4.5
  xdesign.row[12] <- input.data.frame.row$price -4.5 
  t(as.matrix(xdesign.row))  # return row of design matrix
  }



# evaluate performance in the full set of consumer responses
working.choice.utility <- NULL  # initialize utility vector
# work with one row of respondent training data frame at a time
# create choice prediction using the individual part-worths
list.of.ids <- unique(working.data.frame$id)
for (index.for.id in seq(along=list.of.ids)) {
  this.id.part.worths <- posterior.mean[index.for.id,] 
  this.id.data.frame <- subset(working.data.frame, 
    subset=(id == list.of.ids[index.for.id]))
  for (index.for.profile in 1:nrow(this.id.data.frame)) {   
    working.choice.utility <- c(working.choice.utility,
      create.design.matrix(this.id.data.frame[index.for.profile,]) %*%
      this.id.part.worths)
    }  
  }  

working.predicted.choice <- 
  choice.set.predictor(working.choice.utility)
working.actual.choice <- factor(working.data.frame$choice, levels = c(0,1), 
  labels = c("NO","YES"))  
# look for sensitivity > 0.25 for four-profile choice sets 
working.set.performance <- confusionMatrix(data = working.predicted.choice, 
  reference = working.actual.choice, positive = "YES")
# report choice prediction sensitivity for the full data
cat("\n\nFull data set choice set sensitivity = ",
  sprintf("%1.1f",working.set.performance$byClass[1]*100)," Percent",sep="")
# 
# results: Full data set choice set sensitivity = 89.1 Percent
#

# to continue with our analysis of consumer preferences...
# we build a data frame for the consumers with the full set of eight brands
ID <- unique(working.data.frame$id)
Apple <- posterior.mean[,1]
Compaq <- posterior.mean[,2]
Dell <- posterior.mean[,3]
Gateway <- posterior.mean[,4]
HP <- posterior.mean[,5]
IBM <- posterior.mean[,6]
Sony <- posterior.mean[,7]
Sun <- -1 * (Apple + Compaq + Dell + Gateway + HP + IBM + Sony)
Compatibility <- posterior.mean[,8]
Performance <- posterior.mean[,9]
Reliability <- posterior.mean[,10]
Learning <- posterior.mean[,11]
Price <- posterior.mean[,12]

# creation of data frame for analysis of consumer preferences and choice
# starting with individual-level part-worths... more to be added shortly
id.data <- data.frame(ID,Apple,Compaq,Dell,Gateway,HP,IBM,Sony,Sun,
  Compatibility,Performance,Reliability,Learning,Price)
  
  
  
# compute attribute importance values for each attribute
id.data$brand.range <- numeric(nrow(id.data))
id.data$compatibility.range <- numeric(nrow(id.data))
id.data$performance.range <- numeric(nrow(id.data))
id.data$reliability.range <- numeric(nrow(id.data))
id.data$learning.range <- numeric(nrow(id.data))
id.data$price.range <- numeric(nrow(id.data))
id.data$sum.range <- numeric(nrow(id.data))
id.data$brand.importance <- numeric(nrow(id.data))
id.data$compatibility.importance <- numeric(nrow(id.data))
id.data$performance.importance <- numeric(nrow(id.data))
id.data$reliability.importance <- numeric(nrow(id.data))
id.data$learning.importance <- numeric(nrow(id.data))
id.data$price.importance <- numeric(nrow(id.data))

for(id in seq(along=id.data$ID)) {
  id.data$brand.range[id] <- max(id.data$Apple[id],
    id.data$Compaq[id],id.data$Dell[id],
    id.data$Gateway[id],id.data$HP[id],
    id.data$IBM[id],id.data$Sony[id],
    id.data$Sun[id]) - 
    min(id.data$Apple[id],
    id.data$Compaq[id],id.data$Dell[id],
    id.data$Gateway[id],id.data$HP[id],
    id.data$IBM[id],id.data$Sony[id],
    id.data$Sun[id])
 
  id.data$compatibility.range[id] <- abs(8*id.data$Compatibility[id])  
  id.data$performance.range[id] <- abs(4*id.data$Performance[id]) 
  id.data$reliability.range[id] <- abs(2*id.data$Reliability[id]) 
  id.data$learning.range[id] <- abs(8*id.data$Learning[id])
  id.data$price.range[id] <-  abs(8*id.data$Price[id])

  id.data$sum.range[id] <- id.data$brand.range[id] + 
    id.data$compatibility.range[id] +
    id.data$performance.range[id] +
    id.data$reliability.range[id] +
    id.data$learning.range[id] +
    id.data$price.range[id]
 
  id.data$brand.importance[id] <- 
    id.data$brand.range[id]/id.data$sum.range[id]
  id.data$compatibility.importance[id] <- 
    id.data$compatibility.range[id]/id.data$sum.range[id]
  id.data$performance.importance[id] <- 
    id.data$performance.range[id]/id.data$sum.range[id]
  id.data$reliability.importance[id] <- 
    id.data$reliability.range[id]/id.data$sum.range[id]
  id.data$learning.importance[id] <- 
    id.data$learning.range[id]/id.data$sum.range[id]
  id.data$price.importance[id] <- 
    id.data$price.range[id]/id.data$sum.range[id]


  
  
# feature importance relates to the most important product feature
# considering product features as not brand and not price
  id.data$feature.importance[id] <- max(id.data$compatibility.importance[id],
    id.data$performance.importance[id], 
    id.data$reliability.importance[id], 
    id.data$learning.importance[id])
  }
 
# identify each individual's top brand defining top.brand factor variable
id.data$top.brand <- integer(nrow(id.data)) 
for(id in seq(along=id.data$ID)) {
  brand.index <- 1:8
  brand.part.worth <- c(id.data$Apple[id],id.data$Compaq[id],
    id.data$Dell[id],id.data$Gateway[id],id.data$HP[id],id.data$IBM[id],
    id.data$Sony[id],id.data$Sun[id])
  temp.data <- data.frame(brand.index,brand.part.worth)
  temp.data <- temp.data[sort.list(temp.data$brand.part.worth, decreasing = TRUE),]
  id.data$top.brand[id] <- temp.data$brand.index[1]
  }
id.data$top.brand <- factor(id.data$top.brand, levels = 1:8,
  labels = c("Apple","Compaq","Dell","Gateway",
  "HP","IBM","Sony","Sun"))

# note that the standard importance measures from conjoint methods are
# ipsative... their sum is always 1 for proportions or 100 for percentages 
# this has advantages for triplots (ternary plots) but because importance
# is so dependent upon the levels of attributes, it has significant
# disadvantages as well... so we consider a relative-value-based measure
# lets us define an alternative to importance called "attribute value"

# compute "attribute value" relative to the consumer group 
# it is a standardized measure... let "attribute value" be mean 50 sd 10
# here are user-defined functions to use to obtain "value"

standardize <- function(x) {
# standardize x so it has mean zero and standard deviation 1
  (x - mean(x))/sd(x)
  }
compute.value <- function(x) {
# rescale x so it has the same mean and standard deviation as y  
  standardize(x) * 10 + 50
 }

id.data$brand.value <- compute.value(id.data$brand.range)
id.data$compatibility.value <- compute.value(id.data$compatibility.range)
id.data$performance.value <- compute.value(id.data$performance.range)
id.data$reliability.value <- compute.value(id.data$reliability.range)
id.data$learning.value <- compute.value(id.data$learning.range)
id.data$price.value <- compute.value(id.data$price.range)






# identify each individual's top value using computed relative attribute values 
id.data$top.attribute <- integer(nrow(id.data)) 
for(id in seq(along=id.data$ID)) {
  attribute.index <- 1:6
  attribute.value <- c(id.data$brand.value[id],id.data$compatibility.value[id],
    id.data$performance.value[id],id.data$reliability.value[id],
    id.data$learning.value[id],id.data$price.value[id])
  temp.data <- data.frame(attribute.index,attribute.value)
  temp.data <- 
    temp.data[sort.list(temp.data$attribute.value, decreasing = TRUE),]
  id.data$top.attribute[id] <- temp.data$attribute.index[1]
  }
id.data$top.attribute <- factor(id.data$top.attribute, levels = 1:6,
  labels = c("Brand","Compatibility","Performance","Reliability",
  "Learning","Price"))


# mosaic plot of joint frequencies top ranked brand by top value
pdf(file="fig_price_top_top_mosaic_plot.pdf", width = 8.5, height = 11)
  mosaic( ~ top.brand + top.attribute, data = id.data, 
  highlighting = "top.attribute",
  highlighting_fill = 
    c("blue", "white", "green","lightgray","magenta","black"),
  labeling_args = 
  list(set_varnames = c(top.brand = "", top.attribute = ""),
  rot_labels = c(left = 90, top = 45),
  pos_labels = c("center","center"),
  just_labels = c("left","center"),
  offset_labels = c(0.0,0.0)))  
dev.off()  


# an alternative representation that is often quite useful in pricing studies
# is a triplot/ternary plot with three features identified for each consumer
# using the idea from importance caluclations we now use price, brand, and 
# feature importance measures to obtain data for three-way plots
# as the basis for three relative measures, which we call brand.loyalty,
# price.sensitivity, and feature_focus...


id.data$brand.loyalty <- numeric(nrow(id.data)) 
id.data$price.sensitivity <- numeric(nrow(id.data)) 
id.data$feature.focus <- numeric(nrow(id.data)) 
for(id in seq(along=id.data$ID)) {
  sum.importances <- id.data$brand.importance[id] + 
  id.data$price.importance[id] +
  id.data$feature.importance[id]  # less than 1.00 feature is an average
  id.data$brand.loyalty[id] <- id.data$brand.importance[id]/sum.importances
  id.data$price.sensitivity[id] <- id.data$price.importance[id]/sum.importances
  id.data$feature.focus[id] <- id.data$feature.importance[id]/sum.importances
  }



# ternary model of consumer response... the plot  
pdf("fig_price_ternary_three_brands.pdf", width = 11, height = 8.5)  
ternaryplot(id.data[,c("brand.loyalty","price.sensitivity","feature.focus")], 
dimnames = c("Brand Loyalty","Price Sensitivity","Feature Focus"),
prop_size = ifelse((id.data$top.brand == "Apple"), 0.8, 
            ifelse((id.data$top.brand == "Dell"),0.7,
            ifelse((id.data$top.brand == "HP"),0.7,0.5))),
pch = ifelse((id.data$top.brand == "Apple"), 20, 
      ifelse((id.data$top.brand == "Dell"),17,
      ifelse((id.data$top.brand == "HP"),15,1))),
col = ifelse((id.data$top.brand == "Apple"), "red",
      ifelse((id.data$top.brand == "Dell"),"mediumorchid4",
      ifelse((id.data$top.brand == "HP"),"blue","darkblue"))),
grid_color = "#626262",
bg = "#E6E6E6",
dimnames_position = "corner", main = ""
) 
grid_legend(0.725, 0.8, pch = c(20, 17, 15, 1),
col = c("red", "mediumorchid4", "blue", "darkblue"), 
c("Apple", "Dell", "HP", "Other"), title = "Top-Ranked Brand")
dev.off()   
# another way of looking at these data is to employ comparative densities
# for the three selected brands: Apple, Dell, and HP
# using those individual how selected these as the top brand
selected.brands <- c("Apple","Dell","HP")
selected.data <- subset(id.data, subset = (top.brand %in% selected.brands))
# plotting objects for brand.loyalty, price.sensitivity, and feature.focus
# create these three objects and then plot them together on one page
pdf("fig_price_density_three_brands.pdf", width = 8.5, height = 11)  
first.object <- ggplot(selected.data, 
  aes(x = brand.loyalty, fill = top.brand))  +
  labs(x = "Brand Loyalty", 
       y = "f(x)") +
  theme(axis.title.y = element_text(angle = 0, face = "italic", size = 10)) +     
  geom_density(alpha = 0.4) +
  coord_fixed(ratio = 1/15) +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("red","white","blue"), 
    guide = guide_legend(title = NULL)) +
  scale_x_continuous(limits = c(0,1)) +
  scale_y_continuous(limits = c(0,5)) 
second.object <- ggplot(selected.data, 
  aes(x = price.sensitivity, fill = top.brand))  +
  labs(x = "Price Sensitivity", 
       y = "f(x)") +
  theme(axis.title.y = element_text(angle = 0, face = "italic", size = 10)) +      
  geom_density(alpha = 0.4) +
  coord_fixed(ratio = 1/15) +
  theme(legend.position = "none") +
  scale_fill_manual(values = c("red","white","blue"), 
    guide = guide_legend(title = NULL)) +
  scale_x_continuous(limits = c(0,1))  +
  scale_y_continuous(limits = c(0,5))  



third.object <- ggplot(selected.data, 
  aes(x = feature.focus, fill = top.brand))  +
  labs(x = "Feature Focus", 
       y = "f(x)") +
  theme(axis.title.y = element_text(angle = 0, face = "italic", size = 10)) +      
  geom_density(alpha = 0.4) +
  coord_fixed(ratio = 1/15) +
  theme(legend.position = "bottom") +
  scale_fill_manual(values = c("red","white","blue"), 
    guide = guide_legend(title = NULL)) +
  scale_x_continuous(limits = c(0,1)) +
  scale_y_continuous(limits = c(0,5))  

three.part.ggplot.print.with.margins(ggfirstplot.object.name = first.object,
  ggsecondplot.object.name = second.object,
  ggthirdplot.object.name = third.object,
  left.margin.pct=5,right.margin.pct=5,
  top.margin.pct=10,bottom.margin.pct=9,
  first.plot.pct=25,second.plot.pct=25,
  third.plot.pct=31)
dev.off()

# to what extent are consumers open to switching from one brand to another
# can see this trough parallel coordinates plots for the brand part-worths
pdf(file = "fig_price_parallel_coordinates_individuals.pdf", 
  width = 8.5, height = 11)
parallelplot(~selected.data[,c("Apple","Compaq","Dell","Gateway",
  "HP","IBM","Sony","Sun")] | top.brand, selected.data, layout = c (3,1))
dev.off()  
  
# these get a little messy or cluttered...  
# more easily interpreted are parallel coordinate plots of mean part-worths
# for brand part-worth columns and aggreate by top brand (Apple, Dell, or HP)
brands.data <- aggregate(x = selected.data[,2:9], 
  by = selected.data[29], mean)

pdf(file = "fig_price_parallel_coordinates_groups.pdf", 
  width = 8.5, height = 11)
parallelplot(~brands.data[,c("Apple","Compaq","Dell","Gateway",
  "HP","IBM","Sony","Sun")] | top.brand, brands.data, layout = c (3,1), 
   lwd = 3, col = "mediumorchid4") 
dev.off()

# market simulation for hypothetical set of products in the marketplace  
# suppose we work for Apple and we focus upon a market with three 
# competitors: Dell, Gateway, and HP.... we define the products in the
# market using values from the computer choice study just as we did
# in fitting the HB model... we create the simuation input data frame
# and use the previously designed function create.design.matrix
# along with simulation utility functions 





# first product in market is Dell Computer defined as follows:
brand <- "Dell"
compat <- 8  # 100 percent compatibility
perform <- 4 # four times as fast as earlier generation system
reliab <- 2  # Less likely to fail 
learn <- 4  # 16 hours to learn
price <- 4  # $1750
dell.competitor <- data.frame(brand,compat,perform,reliab,learn,price)

# second product in market is Gateway defined as follows:
brand <- "Gateway"
compat <- 6  # 90 percent compatibility
perform <- 2 # twice as fast as earlier generation system
reliab <- 1  # just as likely to fail 
learn <- 2  # 8 hours to learn
price <- 2  # $1250
gateway.competitor <- data.frame(brand,compat,perform,reliab,learn,price)

# third product in market is HP defined as follows:
brand <- "HP"
compat <- 6  # 90 percent compatibility
perform <- 3 # three times as fast as earlier generation system
reliab <- 2  # less likely to fail 
learn <- 2  # 8 hours to learn
price <- 3  # $1500
hp.competitor <- data.frame(brand,compat,perform,reliab,learn,price)

# Apple product has price varying across many choice sets:
brand <- "Apple"
compat <- 5  # 50 percent compatibility
perform <- 4 # four times as fast as earlier generation system
reliab <- 2  # less likely to fail 
learn <- 1  # 4 hours to learn
price <- 1  # $1000 Apple price in first choice set 
apple1000 <- data.frame(brand,compat,perform,reliab,learn,price)
price <- 2  # $1250 Apple price in second choice set
apple1250 <- data.frame(brand,compat,perform,reliab,learn,price) 
price <- 3  # $1500 Apple price in third choice set
apple1500 <- data.frame(brand,compat,perform,reliab,learn,price)  
price <- 4  # $1750 Apple price in fourth choice set
apple1750 <- data.frame(brand,compat,perform,reliab,learn,price)  
price <- 5  # $2000 Apple price in fifth choice set
apple2000 <- data.frame(brand,compat,perform,reliab,learn,price)  
price <- 6  # $2250 Apple price in sixth choice set
apple2250 <- data.frame(brand,compat,perform,reliab,learn,price)  
price <- 7  # $2500 Apple price in seventh choice set
apple2500 <- data.frame(brand,compat,perform,reliab,learn,price)
price <- 8  # $2750 Apple price in eighth choice set
apple2750 <- data.frame(brand,compat,perform,reliab,learn,price)

# the competitive products are fixed from one choice set to the next
competition <- rbind(dell.competitor,gateway.competitor,hp.competitor)



# build the simulation choice sets with Apple varying across choice sets
simulation.choice.sets <- 
  rbind(competition, apple1000, competition, apple1250,
  competition, apple1500, competition, apple1750, competition, apple2000, 
  competition, apple2250, competition, apple2500, competition, apple2750)

# add set id to the simuation.choice sets for ease of analysis
setid <- NULL
for(index.for.set in 1:8) setid <- c(setid,rep(index.for.set, times = 4))
simulation.choice.sets <- cbind(setid,simulation.choice.sets)
  
# list the simulation data frame to check it out
print(simulation.choice.sets)

# create the simulation data frame for all individuals in the study
# by cloning the simulation choice sets for each individual
simulation.data.frame <- NULL  # initialize
list.of.ids <- unique(working.data.frame$id)  # ids from original study
for (index.for.id in seq(along=list.of.ids)) {
  id <- rep(list.of.ids[index.for.id], times = nrow(simulation.choice.sets))
  this.id.data <- cbind(data.frame(id),simulation.choice.sets)
  simulation.data.frame <- rbind(simulation.data.frame, this.id.data)
  }
  
# check structure of simulation data frame
print(str(simulation.data.frame))
print(head(simulation.data.frame))
print(tail(simulation.data.frame))
  
# using create.design.matrix function we evalutate the utility 
# of each product profile in each choice set for each individual 
# in the study... HP part-worths are used for individuals
# this code is similar to that used previously for original data
# from the computer choice study... except now we have simulation data
simulation.choice.utility <- NULL  # initialize utility vector
# work with one row of respondent training data frame at a time
# create choice prediction using the individual part-worths
list.of.ids <- unique(simulation.data.frame$id)
simulation.choice.utility <- NULL  # intitialize
for (index.for.id in seq(along=list.of.ids)) {
  this.id.part.worths <- posterior.mean[index.for.id,] 
  this.id.data.frame <- subset(simulation.data.frame, 
    subset=(id == list.of.ids[index.for.id]))
  for (index.for.profile in 1:nrow(this.id.data.frame)) {   
    simulation.choice.utility <- c(simulation.choice.utility,
      create.design.matrix(this.id.data.frame[index.for.profile,]) %*%
      this.id.part.worths)
    }  
  }  
# use choice.set.predictor function to predict choices in market simulation
simulation.predicted.choice <- 
  choice.set.predictor(simulation.choice.utility)
 
 
  
# add simulation predictions to simulation data frame for analysis
# of the results from the market simulation
simulation.analysis.data.frame <- 
  cbind(simulation.data.frame,simulation.predicted.choice)

# contingency table shows results of market simulation  
with(simulation.analysis.data.frame,
  table(setid,brand,simulation.predicted.choice))
  
# summary table of preference shares
YES.data.frame <- subset(simulation.analysis.data.frame, 
  subset = (simulation.predicted.choice == "YES"), select = c("setid","brand"))

# check YES.data.frame to see that it reproduces the information
# from the contingency table 
print(with(YES.data.frame,table(setid,brand)))

# create market share estimates by dividing by number of individuals
# no need for a spreasheet program to work with tables
table.work <- with(YES.data.frame,as.matrix(table(setid,brand)))
table.work <- table.work[,c("Apple","Dell","Gateway","HP")] # order columns
table.work <- round(100 *table.work/length(list.of.ids), digits = 1)  # percent 
Apple.Price <- c(1000,1250,1500,1750,2000,2250,2500,2750)  # new column
table.work <- cbind(Apple.Price,table.work) # add price column to table
print(table.work)  # print the market/preference share table

# data visualization of market/preference share estimates from the simulation 
mosaic.data.frame <- YES.data.frame
mosaic.data.frame$setid <- factor(mosaic.data.frame$setid, levels = 1:8,
  labels = c("$1,000","$1,250","$1,500","$1,750",
  "$2,000","$2,250","2,500","$2,750"))

# mosaic plot of joint frequencies from the market simulation
# length/width of the tiles in each row reflects market share
# rows relate to Apple prices... simulation choice sets
pdf(file="fig_price_market_simulation_results.pdf", width = 8.5, height = 11)
  mosaic( ~ setid + brand, data = mosaic.data.frame, 
  highlighting = "brand",
  highlighting_fill = 
    c("mediumorchid4", "green", "blue","red"),
  labeling_args = 
  list(set_varnames = c(brand = "", setid = "Price of Apple Computer"),
  rot_labels = c(left = 90, top = 45),
  pos_labels = c("center","center"),
  just_labels = c("left","center"),
  offset_labels = c(0.0,0.0)))  
dev.off()  


# Analysis of Economic Time Series 

library(quantmod) # use for gathering and charting economic data
library(lubridate) # date functions
library(latticeExtra) # package used for horizon plot
library(forecast) # functions for time series forecasting 
library(lmtest) # for Granger test of causality

par(mfrow = c(2,2)) # four plots on one window/page

# Economic Data from Federal Reserve Bank of St. Louis (FRED system)
# National Civilian Unemployment Rate (monthly, percentage)
getSymbols("UNRATENSA", src="FRED", return.class = "xts")
ER <- 100 - UNRATENSA # convert to employment rate
dimnames(ER)[2] <- "ER"
chartSeries(ER,theme="white")
ER.data.frame <- as.data.frame(ER)
ER.data.frame$date <- ymd(rownames(ER.data.frame))
ER.time.series <- ts(ER.data.frame$ER, 
  start = c(year(min(ER.data.frame$date)),month(min(ER.data.frame$date))),
  end = c(year(max(ER.data.frame$date)),month(max(ER.data.frame$date))),
  frequency=12)


# Manufacturers' New Orders: Durable Goods (millions of dollars) 
getSymbols("DGORDER", src="FRED", return.class = "xts")
DGO <- DGORDER/1000 # convert to billions of dollars
dimnames(DGO)[2] <- "DGO" # use simple name for index
chartSeries(DGO, theme="white") 
DGO.data.frame <- as.data.frame(DGO)
DGO.data.frame$DGO <- DGO.data.frame$DGO
DGO.data.frame$date <- ymd(rownames(DGO.data.frame))
DGO.time.series <- ts(DGO.data.frame$DGO, 
  start = c(year(min(DGO.data.frame$date)),month(min(DGO.data.frame$date))),
  end = c(year(max(DGO.data.frame$date)),month(max(DGO.data.frame$date))),
  frequency=12)


# University of Michigan Index of Consumer Sentiment (1Q 1966 = 100)
getSymbols("UMCSENT", src="FRED", return.class = "xts")
ICS <- UMCSENT # use simple name for xts object
dimnames(ICS)[2] <- "ICS" # use simple name for index
chartSeries(ICS, theme="white")
ICS.data.frame <- as.data.frame(ICS)
ICS.data.frame$ICS <- ICS.data.frame$ICS
ICS.data.frame$date <- ymd(rownames(ICS.data.frame))
ICS.time.series <- ts(ICS.data.frame$ICS, 
  start = c(year(min(ICS.data.frame$date)), month(min(ICS.data.frame$date))),
  end = c(year(max(ICS.data.frame$date)),month(max(ICS.data.frame$date))),
  frequency=12)




# New Homes Sold in the US, not seasonally adjusted (monthly, millions)
getSymbols("HSN1FNSA",src="FRED",return.class = "xts")
NHS <- HSN1FNSA
dimnames(NHS)[2] <- "NHS" # use simple name for index
chartSeries(NHS, theme="white")
NHS.data.frame <- as.data.frame(NHS)
NHS.data.frame$NHS <- NHS.data.frame$NHS
NHS.data.frame$date <- ymd(rownames(NHS.data.frame))
NHS.time.series <- ts(NHS.data.frame$NHS, 
  start = c(year(min(NHS.data.frame$date)),month(min(NHS.data.frame$date))),
  end = c(year(max(NHS.data.frame$date)),month(max(NHS.data.frame$date))),
  frequency=12)

# define multiple time series object
economic.mts <- cbind(ER.time.series, DGO.time.series, ICS.time.series,
  NHS.time.series) 
  dimnames(economic.mts)[[2]] <- c("ER","DGO","ICS","NHS") # keep simple names 
modeling.mts <- na.omit(economic.mts) # keep overlapping time intervals only

# plot multiple time series 
plot(modeling.mts,main="")

# create new indexed series IER using base date March 1997
ER0 <- mean(as.numeric(window(ER.time.series,start=c(1997,3),end=c(1997,3))))
IER.time.series <- (ER.time.series/ER0) * 100  

# create new indexed series IDGO using base date March 1997
DGO0 <- mean(as.numeric(window(DGO.time.series,start=c(1997,3),end=c(1997,3))))
IDGO.time.series <- (DGO.time.series/DGO0) * 100  

# create new indexed series INHS using base date March 1997
NHS0 <- mean(as.numeric(window(NHS.time.series,start=c(1997,3),end=c(1997,3))))
INHS.time.series <- (NHS.time.series/NHS0) * 100  

# create a multiple time series object from the index series
economic.mts <- cbind(IER.time.series,
IDGO.time.series,
ICS.time.series,
INHS.time.series) 
dimnames(economic.mts)[[2]] <- c("IER","IDGO","ICS","INHS")
working.economic.mts <- na.omit(economic.mts) # months complete for all series

# plot multiple economic time series as horizon plot
# using the index 100 as the reference point
# use ylab rather than strip.left, for readability
# also shade any times with missing data values.
print(horizonplot(working.economic.mts, colorkey = TRUE,
  layout = c(1,4), strip.left = FALSE, origin = 100,
  ylab = list(rev(colnames(working.economic.mts)), rot = 0, cex = 0.7)) +
  layer_(panel.fill(col = "gray90"), panel.xblocks(..., col = "white")))
  
 
 
 
 
  
# return to the individual economic time series prior to indexing  

# ARIMA model fit to the employment rate data
ER.auto.arima.fit <- auto.arima(ER.time.series, d=NA, D=NA, max.p=3, max.q=3,
  max.P=2, max.Q=2, max.order=3, start.p=2, start.q=2,
  start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
  ic=c("aic"), stepwise=TRUE, trace=FALSE,
  approximation=FALSE, xreg=NULL,
  test=c("kpss","adf","pp"), seasonal.test=c("ocsb","ch"),
  allowdrift=FALSE, lambda=NULL, parallel=FALSE, num.cores=NULL)
print(summary(ER.auto.arima.fit))
# national employment rate two-year forecast (horizon h = 24 months) 
ER.forecast <- forecast.Arima(ER.auto.arima.fit, h=24, level=c(90), 
  fan=FALSE, xreg=NULL, bootstrap=FALSE)
# plot national employment rate time series with two-year forecast 
plot(ER.forecast,main="", ylab="Employment Rate (100 - Unemployment Rate)",
  xlab = "Time", las = 1, lwd = 1.5)

# ARIMA model fit to the manufacturers durable goods orders
DGO.auto.arima.fit <- auto.arima(DGO.time.series, d=NA, D=NA, max.p=3, max.q=3,
  max.P=2, max.Q=2, max.order=3, start.p=2, start.q=2,
  start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
  ic=c("aic"), stepwise=TRUE, trace=FALSE,
  approximation=FALSE, xreg=NULL,
  test=c("kpss","adf","pp"), seasonal.test=c("ocsb","ch"),
  allowdrift=FALSE, lambda=NULL, parallel=FALSE, num.cores=NULL)
print(summary(DGO.auto.arima.fit))
# durable goods orders two-year forecast (horizon h = 24 months) 
DGO.forecast <- forecast.Arima(DGO.auto.arima.fit, h=24, level=c(90), 
  fan=FALSE, xreg=NULL, bootstrap=FALSE)
# plot durable goods time series with two-year forecast 
plot(DGO.forecast,main="", ylab="Durable Goods Orders (billions of dollars)",
  xlab = "Time", las = 1, lwd = 1.5)

# ARIMA model fit to index of consumer sentiment
ICS.auto.arima.fit <- auto.arima(ICS.time.series, d=NA, D=NA, max.p=3, max.q=3,
  max.P=2, max.Q=2, max.order=3, start.p=2, start.q=2,
  start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
  ic=c("aic"), stepwise=TRUE, trace=FALSE,
  approximation=FALSE, xreg=NULL,
  test=c("kpss","adf","pp"), seasonal.test=c("ocsb","ch"),
  allowdrift=FALSE, lambda=NULL, parallel=FALSE, num.cores=NULL)
print(summary(ICS.auto.arima.fit))
# index of consumer sentiment two-year forecast (horizon h = 24 months) 
ICS.forecast <- forecast.Arima(ICS.auto.arima.fit, h=24, level=c(90), 
  fan=FALSE, xreg=NULL, bootstrap=FALSE)
# plot index of consumer sentiment time series with two-year forecast 
plot(ICS.forecast,main="", ylab="Index of Consumer Sentiment (1Q 1966 = 100)",
  xlab = "Time", las = 1, lwd = 1.5)






# ARIMA model fit to new home sales
NHS.auto.arima.fit <- auto.arima(NHS.time.series, d=NA, D=NA, max.p=3, max.q=3,
  max.P=2, max.Q=2, max.order=3, start.p=2, start.q=2,
  start.P=1, start.Q=1, stationary=FALSE, seasonal=TRUE,
  ic=c("aic"), stepwise=TRUE, trace=FALSE,
  approximation=FALSE, xreg=NULL,
  test=c("kpss","adf","pp"), seasonal.test=c("ocsb","ch"),
  allowdrift=FALSE, lambda=NULL, parallel=FALSE, num.cores=NULL)
print(summary(NHS.auto.arima.fit))
# new home sales two-year forecast (horizon h = 24 months) 
NHS.forecast <- forecast.Arima(NHS.auto.arima.fit, h=24, level=c(90), 
  fan=FALSE, xreg=NULL, bootstrap=FALSE)
# plot new home sales time series with two-year forecast 
plot(NHS.forecast,main="", ylab="New Homes Sold (millions)",
  xlab = "Time", las = 1, lwd = 1.5)

# Which regressors have potential as leading indicators?
# look for relationships across three of the time series
# using the period of overlap for those series
grangertest(ICS~ER, order = 3, data=modeling.mts)
grangertest(ICS~DGO, order = 3, data=modeling.mts)
grangertest(DGO~ER, order = 3, data=modeling.mts)
grangertest(DGO~ICS, order = 3, data=modeling.mts)
grangertest(ER~DGO, order = 3, data=modeling.mts)
grangertest(ER~ICS, order = 3, data=modeling.mts)


# Workforce Scheduling for Anonymous Bank Call Center

library(lubridate)  # date functions
library(grid)  # graphics utilities needed for split-plotting
library(ggplot2)  # graphics package with ribbon plot
library(queueing)  # queueing functions, including Erlang C
library(lpSolve)  # linear programming package
load("mtpa_split_plotting_utilities.Rdata")  # utilities based on grid graphics
load("mtpa_wait_time_ribbon_utility.Rdata")  # wait-time ribbon plot

put.title.on.plots <- TRUE  # put title on wait-time ribbon plots

# The call center data from "Anonymous Bank" in Israel were provided 
# by Avi Mandelbaum, with the help of Ilan Guedj.
# data source: http://ie.technion.ac.il/serveng/callcenterdata/index.html
# variable names and definitions from documentation 
# VRU  Voice Response Unit automated service
# vru.line  6 digits Each entering phone-call is first routed through a VRU: 
#           There are 6 VRUs labeled AA01 to AA06. Each VRU has several lines
#           labeled 1-16. There are a total of 65 lines. Each call is assigned 
#           a VRU number and a line number.
# call.id  unique call identifier
# customer.id  unique identifier for existing customer, zero for non-customer  
# priority  0 or 1 for inidentified or regular customers
#           2 for priority customers who receive advanced position in queue
# type  type of service
#       PS  regular activity (coded 'PS' for 'Peilut Shotefet')
#       PE  regular activity in English (coded 'PE' for 'Peilut English')
#       IN  internet consulting (coded 'IN' for 'Internet')
#       NE  stock exchange activity (coded 'NE' for 'Niarot Erech') 
#       NW  potential customer getting information
#       TT  customers who left a message asking the bank to return their call 
#           but, while the system returned their call, the calling-agent became 
#           busy hence the customers were put on hold in the queue.
# date  year-month-day
# vru_entry  time that the phone-call enters the call-center or VRU
# vru_exit  time of exit from VRU directly to service or to queue
# vru_time  time in seconds spent in the VRU 
#           (calculated by exit_time  entry_time)
# q_start  time of joining the queue (00:00:00 for customers who abandon VRU
#          or do not enter the queue) 
# q_exit  time in seconds of exiting queue to receive service or abandonment
# q_time  time spent in queue (calculated by q_exit  q_start)
# outcome  AGENT = service
#          HANG = hang up
#          PHANTOM = a virtual call to be ignored
# ser_start  time of beginning of service by agent
# ser_exit  time of end of service by agent
# ser_time  service duration in seconds (calculated by ser_exit  ser_start)
# server  name of agent, NO_SERVER if no service provided



# focus upon February 1999
call.center.input.data <- read.table("data_anonymous_bank_february.txt", 
  header = TRUE, colClasses = c("character","integer","numeric",
  "integer","character","character","character","character","integer",
  "character","character","integer","factor","character","character",
  "integer","character"))
  
# check data frame object and variable values
print(summary(call.center.input.data))

# delete PHANTOM calls
call.center.data <- subset(call.center.input.data, subset = (outcome != "PHANTOM"))

# negative VRU times make no sense... drop these rows from data frame
call.center.data <- subset(call.center.data, subset = (vru_time >= 0))

# calculate wait time as sum of vru_time and q_time
call.center.data$wait_time <- 
  call.center.data$vru_time + call.center.data$q_time

# define date variable
call.center.data$date <- ymd(call.center.data$date)

# identify day of the week 1 = Sunday ... 7 = Saturday
call.center.data$day_of_week <- wday(call.center.data$date)
call.center.data$day_of_week <- factor(call.center.data$day_of_week,
  levels = c(1:7), labels = c("Sunday","Monday","Tuesday",
  "Wednesday","Thursday","Friday","Saturday"))

# check frequency of calls by day of week
print(table(call.center.data$day_of_week))

# decompose date into list structure and extract the hour
time.list <- strsplit(call.center.data$vru_entry,":")
call.hour <- numeric(nrow(call.center.data))
for (index.for.call in 1:nrow(call.center.data)) 
  call.hour[index.for.call] <- as.numeric(time.list[[index.for.call]][1])
call.center.data$call_hour <- call.hour

# check frequency of calls by hour and day of week
print(with(call.center.data, table(day_of_week, call_hour)))

# select first week of February 1999 for data visualization and analysis
# that week began on Monday, February 1 and ended on Sunday, February 7
selected.week <- subset(call.center.data, subset = (date < ymd("990208")))

# loop for day of week ignoring Saturdays in Isreal
day.of.week.list <- c("Monday","Tuesday",
  "Wednesday","Thursday","Friday","Sunday")
  
  
  
  
  
  
# wait-time ribbon plots for the six selected days
# call upon utility function wait.time.ribbon
# the utility makes use of grid split-plotting 
# place ribbon plot and text table/plot on each file
# each plot goes to its own external pdf file
for(index.day in seq(along=day.of.week.list)) {
  this.day.of.week <- day.of.week.list[index.day]
  pdf(file = paste("fig_operations_management_ribbon_",
  tolower(this.day.of.week),".pdf",sep=""), width = 11, height = 8.5)  
  if(put.title.on.plots) {
    ribbon.plot.title <- paste(this.day.of.week,"Call Center Operations")
    }
    else {
    ribbon.plot.title <- "" 
    }
  selected.day <- subset(selected.week, 
    subset = (day_of_week == this.day.of.week),
    select = c("call_hour","wait_time","ser_time","server"))
  colnames(selected.day) <- c("hour","wait","service","server")
  wait.time.ribbon(wait.service.data = selected.day, 
    title = ribbon.plot.title,
    use.text.tagging = TRUE, wait.time.goal = 30, wait.time.max = 90,
    plotting.min = 0, plotting.max = 250)    
  dev.off()  
  }

# select Wednesdays in February for the queueing model
wednesdays <- subset(call.center.data, subset = (day_of_week == "Wednesday"))

# compute arrival rate of calls as calls for hour  
# we do not use table() here because some hours could have zero calls
calls.for.hour <- numeric(24)
for(index.for.hour in 1:24) { 
# 24-hour clock has first hour coded as zero in input data file
  coded.index.for.hour <- index.for.hour - 1  
  this.hour.calls <- 
    subset(wednesdays, subset = (call_hour == coded.index.for.hour))  
  if(nrow(this.hour.calls) > 0) 
    calls.for.hour[index.for.hour] <- nrow(this.hour.calls)  
  }

# compute arrival rate as average number of calls into VRU per hour
hourly.arrival.rate <- calls.for.hour/4  # four Wednesdays in February

# service times can vary hour-by-hour due to differences 
# in service requests and individuals calling hour-by-hour
# begin by selecting calls that receive service
wednesdays.served <- subset(wednesdays, subset = (server != "NO_SERVER"))

hourly.mean.service.time <- numeric(24)
served.for.hour <- numeric(24)



for(index.for.hour in 1:24) { 
# 24-hour clock has first hour coded as zero in input data file
  coded.index.for.hour <- index.for.hour - 1  
  this.hour.calls <- 
    subset(wednesdays.served, subset = (call_hour == coded.index.for.hour))
  if(nrow(this.hour.calls) > 0) {
    served.for.hour[index.for.hour] <- nrow(this.hour.calls)
    hourly.mean.service.time[index.for.hour] <- mean(this.hour.calls$ser_time)
    }
  } 
  
# hourly service rate given the current numbers of service operators
hourly.served.rate <- served.for.hour/4  # four Wednesdays in February

# build data frame for plotting arrival and service rates
hour <- 1:24  # hour for horizontal axix of line chart
type <- rep("Arrived", length = 24)
value <- hourly.arrival.rate
arrival.data.frame <- data.frame(hour, value, type) 
type <- rep("Served", length = 24)
value <- hourly.served.rate
service.data.frame <- data.frame(hour, value, type) 
arrival.service.data.frame <- rbind(arrival.data.frame, service.data.frame)

pdf(file = "fig_operations_management_wednesdays_arrived_served.pdf", 
  width = 11, height = 8.5)
plotting.object <- ggplot(data = arrival.service.data.frame, 
  aes(x = hour, y = value, fill = type)) + 
  geom_line() +
  geom_point(size = 4, shape = 21) +
  scale_x_continuous(breaks = c(1,3,5,7,9,11,13,15,17,19,21,23,25),
    labels = 
      c("00","02","04","06","08","10","12","14","16","18","20","22","24")) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) +
  labs(x = "Hour of Day (24-Hour Clock)", y = "Average Calls per Hour") +
  scale_fill_manual(values = c("yellow","dark green"), 
    guide = guide_legend(title = NULL))  +
  theme(legend.position = c(1,1), legend.justification = c(1,1)) +
  theme(legend.text = element_text(size=15)) +
  coord_fixed(ratio = 1/10)    
print(plotting.object)
dev.off()

# examine service times per service operator
# for hours with no service time information use the mean as value
hourly.mean.service.time <- 
  ifelse((hourly.mean.service.time == 0),
    mean(wednesdays.served$ser_time),
    hourly.mean.service.time) 
# compute service rate noting that there are 3600 seconds in an hour
# adding 60 seconds to each mean service time for time between calls
# this 60 seconds is the wrap up time or time an service agent remains 
# unavailable to answer a new call after a call has been completed



hourly.service.rate <- 3600/(hourly.mean.service.time + 60)

# we observe that mean service times do not vary that much hour-by-hour
# so we use the mean hourly service rate in queueing calculations
# mean(hourly.service.rate) is 14.86443
# so we use 15 calls per hour as the rate for one service operator
SERVICE.RATE <- 15

# C_erlang function from the queueing package
# inputs c = number of servers
#        r = ratio of rate of arrivals and rate of service
# returns the propability of waiting in queue because all servers are busy
# let us set a target for the probability of waiting in queue to be 0.50
# using while-loop iteration we determine the number of servers needed 
# we do this for each hour of the day knowing the hourly arrival rate

PROBABILITY.GOAL <- 0.50
servers.needed <- integer(24)  # initialize to zero
for(index.for.hour in 1:24) {
  if (hourly.arrival.rate[index.for.hour] > 0) {
    erlang.probability <- 1.00  # intialization prior to entering while-loop
    while (erlang.probability > PROBABILITY.GOAL) {
      servers.needed[index.for.hour] <- servers.needed[index.for.hour] + 1
      erlang.probability <- C_erlang(c = servers.needed[index.for.hour], 
          r = hourly.arrival.rate[index.for.hour]/SERVICE.RATE)
      }  # end while-loop for defining servers needed given probability goal 
    }  # end if-block for hours with calls
  }  # end for-loop for the hour

# the result for servers.needed is obtained as
# 1  1  1  0  1  1  1  4  8  9 10  9  8 16 10 10  6  7  8  8  6  6  5  4
# we will assume the bank call center will be closed hours 00 through 05
# but use the other values as the bank's needed numbers of servers
servers.needed[1:6] <- 0
cat("\n","----- Hourly Operator Requirements -----","\n")
print(servers.needed)

# read in case data
bank.shifts.data.frame <- read.csv("data_anonymous_bank_shifts.csv")

# examine the structure of the case data frame
print(str(bank.shifts.data.frame))

constraint.matrix <- as.matrix(bank.shifts.data.frame[,3:10])
cat("\n","----- Call Center Shift Constraint Matrix -----","\n")
print(constraint.matrix)

# six-hour shift salaries in Israeli sheqels 
# 1 ILS = 3.61 USD in June 2013
# these go into the objective function for integer programing
# with the objective of minimizing total costs
cost.vector <- c(252,288,180,180,180,288,288,288) 


call.center.schedule <- lp(const.mat=constraint.matrix,
const.rhs = servers.needed,
const.dir = rep(">=",times=8),
int.vec = 1:8,
objective = cost.vector,
direction = "min")
# printed summary of the results for the call center problem
ShiftID <- 1:8
StartTime <- c(0,6,8,10,12,2,4,6)
# c("Midnight","6 AM","8 AM","10 AM","Noon","2 PM","4 PM","6 PM")
ShiftDuration <- rep(6,times=8)
HourlyShiftSalary <- c(42,48,30,30,30,48,48,48)
HourlyShiftCost <- call.center.schedule$objective # six x hourly shift salary
Solution <- call.center.schedule$solution  
ShiftCost <- call.center.schedule$solution * call.center.schedule$objective
call.center.summary <- 
  data.frame(ShiftID,StartTime,ShiftDuration,HourlySalary,
  HourlyShiftCost,Solution,ShiftCost) 
cat{"\n\n","Call Center Summary","\n\n")
print(call.center.summary)
# the solution is obtained by print(call.center.schedule) 
# or by summing across the hourly solution times the cost objective
 print(call.center.schedule) 
cat("\n\n","Call Center Summary Minimum Cost Solution:",sum(ShiftCost),"\n\n")
# build data frame for plotting the solution compared with need
hour <- 1:24  # hour for horizontal axix of line chart
type <- rep("Hourly Need", length = 24)
value <- servers.needed
needs.data.frame <- data.frame(hour, value, type) 
type <- rep("Optimal Solution", length = 24)
value <- schedule.fit.to.need
solution.data.frame <- data.frame(hour, value, type) 
plotting.data.frame <- rbind(needs.data.frame, solution.data.frame)

# plot the solution... solution match to the workforce need
pdf(file = "fig_operations_management_solution.pdf", width = 11, height = 8.5)
plotting.object <- ggplot(data = plotting.data.frame, 
  aes(x = hour, y = value, fill = type)) + 
  geom_line() +
  geom_point(size = 4, shape = 21) +
  scale_x_continuous(breaks = c(1,3,5,7,9,11,13,15,17,19,21,23,25),
    labels = 
      c("00","02","04","06","08","10","12","14","16","18","20","22","24")) +
  theme(axis.text.x = element_text(angle = 30, hjust = 1, vjust = 1)) +
  labs(x = "Hour of Day (24-Hour Clock)", y = "Number of Service Operators") +
  scale_fill_manual(values = c("white","blue"), 
    guide = guide_legend(title = NULL)) +
  theme(legend.position = c(1,1), legend.justification = c(1,1)) +
  theme(legend.text = element_text(size=15)) +
  coord_fixed(ratio = 2/2.25)    
print(plotting.object)



# Association Rules for Market Basket Analysis

library(arules)  # association rules
library(arulesViz)  # data visualization of association rules
library(RColorBrewer)  # color palettes for plots
library(cluster)  # cluster analysis for market segmentation

data(Groceries)  # grocery transcations object from arules package

# show the dimensions of the transactions object
print(dim(Groceries))

print(dim(Groceries)[1])  # 9835 market baskets for shopping trips
print(dim(Groceries)[2])  # 169 initial store items  

# examine frequency for each item with support greater than 0.025
pdf(file="fig_market_basket_initial_item_support.pdf", 
  width = 8.5, height = 11)
itemFrequencyPlot(Groceries, support = 0.025, cex.names=0.8, xlim = c(0,0.3),
  type = "relative", horiz = TRUE, col = "dark red", las = 1,
  xlab = paste("Proportion of Market Baskets Containing Item",
    "\n(Item Relative Frequency or Support)"))
dev.off()    

# explore possibilities for combining similar items
print(head(itemInfo(Groceries))) 
print(levels(itemInfo(Groceries)[["level1"]]))  # 10 levels... too few 
print(levels(itemInfo(Groceries)[["level2"]]))  # 55 distinct levels

# aggregate items using the 55 level2 levels for food categories
# to create a more meaningful set of items
groceries <- aggregate(Groceries, itemInfo(Groceries)[["level2"]])  

print(dim(groceries)[1])  # 9835 market baskets for shopping trips
print(dim(groceries)[2])  # 55 final store items (categories)  

pdf(file="fig_market_basket_final_item_support.pdf", width = 8.5, height = 11)
itemFrequencyPlot(groceries, support = 0.025, cex.names=1.0, xlim = c(0,0.5),
  type = "relative", horiz = TRUE, col = "blue", las = 1,
  xlab = paste("Proportion of Market Baskets Containing Item",
    "\n(Item Relative Frequency or Support)"))
dev.off()   

# obtain large set of association rules for items by category and all shoppers
# this is done by setting very low criteria for support and confidence
first.rules <- apriori(groceries, 
  parameter = list(support = 0.001, confidence = 0.05))
print(summary(first.rules))  # yields 69,921 rules... too many




# select association rules using thresholds for support and confidence 
second.rules <- apriori(groceries, 
  parameter = list(support = 0.025, confidence = 0.05))
print(summary(second.rules))  # yields 344 rules
  
# data visualization of association rules in scatter plot
pdf(file="fig_market_basket_rules.pdf", width = 8.5, height = 8.5)
plot(second.rules, 
  control=list(jitter=2, col = rev(brewer.pal(9, "Greens")[4:9])),
  shading = "lift")   
dev.off()    
  
# grouped matrix of rules 
pdf(file="fig_market_basket_rules_matrix.pdf", width = 8.5, height = 8.5)
plot(second.rules, method="grouped",   
  control=list(col = rev(brewer.pal(9, "Greens")[4:9])))
dev.off()    

# select rules with vegetables in consequent (right-hand-side) item subsets
vegie.rules <- subset(second.rules, subset = rhs %pin% "vegetables")
inspect(vegie.rules)  # 41 rules

# sort by lift and identify the top 10 rules
top.vegie.rules <- head(sort(vegie.rules, decreasing = TRUE, by = "lift"), 10)
inspect(top.vegie.rules) 

pdf(file="fig_market_basket_farmer_rules.pdf", width = 11, height = 8.5)
plot(top.vegie.rules, method="graph", 
  control=list(type="items"), 
  shading = "lift")
dev.off()  



# Game-day Simulator for Baseball
library(lattice)  # graphics package used to create probability matrix visual
simulator <- function(away.mean,home.mean,niterations) { 
# input is runs scored means output is probability of winning for away team
away.game.score <- numeric(niterations)
home.game.score <- numeric(niterations)
away.win <- numeric(niterations)
i <- 1
while (i < niterations + 1) { 
  away.game.score[i] <- rnbinom(1,mu=away.mean, size = 4)
  home.game.score[i] <- rnbinom(1,mu=home.mean, size = 4)
  if(away.game.score[i]>home.game.score[i]) away.win[i] <- 1
  if(away.game.score[i]>home.game.score[i] || 
  away.game.score[i]<home.game.score[i]) i <- i + 1 
  }
n.away.win <- sum(away.win)
n.away.win/niterations  # return probability of away team winning 
} 
set.seed(1234)  # set to obtain reproducible results 
niterations <- 100000  # set to smaller number for testing
# probability matrix for results... home team is rows, away team is columns
probmat <- matrix(data = NA, nrow = 9, ncol = 9,  
  dimnames = list(c(as.character(1:9)), c(as.character(1:9)))) 
for (index.home in 1:9)
for (index.away in 1:9)
if (index.home > index.away) {
  probmat[index.home,index.away] <- 
    simulator(index.away, index.home, niterations) 
  probmat[index.away,index.home] <- 1 - probmat[index.home, index.away] 
  }
# create probability matrix visual
x <- rep(1:nrow(probmat),times=ncol(probmat))
y <- NULL
for (i in 1:ncol(probmat)) y <- c(y,rep(i,times=nrow(probmat)))
probtext <- sprintf("%0.3f", as.numeric(probmat))  # fixed format 0.XXX
text.data.frame <- data.frame(x, y, probtext)
text.data.frame$probtext <- as.character(text.data.frame$probtext)
text.data.frame$probtext <- ifelse((text.data.frame$probtext == "NA"),
    NA,text.data.frame$probtext)  # define diagonal cells as missing
text.data.frame <- na.omit(text.data.frame)  # diagonal cells
levelplot(probmat, cuts = 25, tick.number = 9,
  col.regions=colorRampPalette(c("violet", "white", "light blue")),
  xlab = "Visiting Team Runs Expected", 
  ylab = "Home Team Runs Expected",
  panel = function(...) {
    panel.levelplot(...)  
    panel.text(text.data.frame$x, text.data.frame$y, 
    labels = text.data.frame$probtext)
    })


# Conjoint Analysis Spine Chart

# spine chart accommodates up to 45 part-worths on one page
# |part-worth| <= 40 can be plotted directly on the spine chart
# |part-worths| > 40 can be accommodated through standardization

print.digits <- 2  # set number of digits on print and spine chart

# user-defined function for printing conjoint measures
if (print.digits == 2) 
  pretty.print <- function(x) {sprintf("%1.2f",round(x,digits = 2))} 
if (print.digits == 3) 
  pretty.print <- function(x) {sprintf("%1.3f",round(x,digits = 3))} 
 
# --------------------------------------------------
# user-defined function for spine chart
# --------------------------------------------------
spine.chart <- function(conjoint.results,  
  color.for.part.worth.point = "blue",
  color.for.part.worth.line = "blue",
  left.side.symbol.to.print.around.part.worths = "(",
  right.side.symbol.to.print.around.part.worths = ")",
  left.side.symbol.to.print.around.importance = "",
  right.side.symbol.to.print.around.importance = "",
  color.for.printing.importance.text = "dark red",  
  color.for.printing.part.worth.text = "black",  
  draw.gray.background = TRUE,  
  draw.optional.grid.lines = TRUE,  
  print.internal.consistency = TRUE,  
  fix.max.to.4 = FALSE,  
  put.title.on.spine.chart = FALSE,
  title.on.spine.chart = paste("TITLE GOES HERE IF WE ASK FOR ONE",sep=""),
  plot.framing.box = TRUE,  
  do.standardization = TRUE,  
  do.ordered.attributes = TRUE) {

  # fix.max.to.4  option to override the range for part-worth plotting 
   
  if(!do.ordered.attributes) effect.names <- conjoint.results$attributes   
  if(do.ordered.attributes) effect.names <- 
    conjoint.results$ordered.attributes      
   
  number.of.levels.of.attribute <- NULL
  for(index.for.factor in seq(along=effect.names))
    number.of.levels.of.attribute <- c(number.of.levels.of.attribute,
      length(conjoint.results$xlevels[[effect.names[index.for.factor]]]))
    
  # total number of levels needed for vertical length of spine the spine plot
  total.number.of.levels <- sum(number.of.levels.of.attribute)




 
  # define size of spaces based upon the number of part-worth levels to plot
   if(total.number.of.levels <= 20) {
    smaller.space <- 0.01
    small.space <- 0.02
    medium.space <- 0.03
    large.space <- 0.04
    }
  if(total.number.of.levels > 20) {
    smaller.space <- 0.01 * 0.9
    small.space <- 0.02 * 0.9
    medium.space <- 0.03 * 0.9
    large.space <- 0.04 * 0.9
    }
  if(total.number.of.levels > 22) {
    smaller.space <- 0.01 * 0.85
    small.space <- 0.02 * 0.85
    medium.space <- 0.03 * 0.825
    large.space <- 0.04 * 0.8
    }
  if(total.number.of.levels > 25) {
    smaller.space <- 0.01 * 0.8
    small.space <- 0.02 * 0.8
    medium.space <- 0.03 * 0.75
    large.space <- 0.04 * 0.75
    }
  if(total.number.of.levels > 35) {
    smaller.space <- 0.01 * 0.65
    small.space <- 0.02 * 0.65
    medium.space <- 0.03 * 0.6
    large.space <- 0.04 * 0.6
    }
  
  # of course there is a limit to how much we can plot on one page  
  if (total.number.of.levels > 45) 
    stop("\n\nTERMINATED: More than 45 part-worths on spine chart\n")
 
  if(!do.standardization) 
    part.worth.plotting.list <- conjoint.results$part.worths
 
  if(do.standardization) 
    part.worth.plotting.list <- conjoint.results$standardized.part.worths 
  
  # check the range of part-worths to see which path to go down for plotting
  # initialize these toggles to start
  
  max.is.less.than.40 <- FALSE 
  max.is.less.than.20 <- FALSE
  max.is.less.than.10 <- FALSE
  max.is.less.than.4 <- FALSE
  max.is.less.than.2 <- FALSE
  max.is.less.than.1 <- FALSE 



  if (max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 40) {
    max.is.less.than.40 <- TRUE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.10 <- FALSE
    max.is.less.than.4 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE
    }  
  if (max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 20) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- TRUE
    max.is.less.than.10 <- FALSE
    max.is.less.than.4 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE
    }
  if(max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 10) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.10 <- TRUE
    max.is.less.than.4 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE
    }
  if (max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 4) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.4 <- TRUE
    max.is.less.than.10 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE
    } 
  if(max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 2) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.4 <- FALSE
    max.is.less.than.10 <- FALSE
    max.is.less.than.2 <- TRUE
    max.is.less.than.1 <- FALSE
    }
   if(max(abs(min(unlist(part.worth.plotting.list),na.rm=TRUE)),
    max(unlist(part.worth.plotting.list),na.rm=TRUE)) <= 1) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.4 <- FALSE
    max.is.less.than.10 <- FALSE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- TRUE
    }

  # sometimes we override the range for part-worth plotting
  # this is not usually done... but it is an option
  if (fix.max.to.4) {
    max.is.less.than.40 <- FALSE 
    max.is.less.than.20 <- FALSE
    max.is.less.than.10 <- FALSE
    max.is.less.than.4 <- TRUE
    max.is.less.than.2 <- FALSE
    max.is.less.than.1 <- FALSE 
    }
  
  if (!max.is.less.than.1 & !max.is.less.than.2 & !max.is.less.than.4 & 
    !max.is.less.than.10 & !max.is.less.than.20 & !max.is.less.than.40) 
      stop("\n\nTERMINATED: Spine chart cannot plot |part-worth| > 40")

  # determine point positions for plotting part-worths on spine chart  
  if (max.is.less.than.1 | max.is.less.than.2 | max.is.less.than.4 | 
    max.is.less.than.10 | max.is.less.than.20 | max.is.less.than.40) {
  # begin if-block plotting when all part-worths in absolute value 
  # are less than one of the tested range values
  # part-worth positions for plottting 
  # end if-block plotting when all part-worths in absolute value 
  # are less than one of the tested range values
  # offsets for plotting vary with the max.is.less.than setting
    if(max.is.less.than.1) {
      list.scaling <- function(x) {0.75 + x/5}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }
    if(max.is.less.than.2) {
      list.scaling <- function(x) {0.75 + x/10}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }  
  
    if(max.is.less.than.4) {
      list.scaling <- function(x) {0.75 + x/20}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }
    
     if(max.is.less.than.10) {
      list.scaling <- function(x) {0.75 + x/50}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }
    
    if(max.is.less.than.20) {
      list.scaling <- function(x) {0.75 + x/100}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }
    
    
    
    if(max.is.less.than.40) {
      list.scaling <- function(x) {0.75 + x/200}
      part.worth.point.position <- 
        lapply(part.worth.plotting.list,list.scaling)
      }
      
    part.worth.point.position <- lapply(part.worth.plotting.list,list.scaling)
    } 

  if (plot.framing.box) plot(c(0,0,1,1),c(0,1,0,1),xlab="",ylab="",
    type="n",xaxt="n",yaxt="n")

  if (!plot.framing.box) plot(c(0,0,1,1),c(0,1,0,1),xlab="",ylab="",
    type="n",xaxt="n",yaxt="n", bty="n")

  if (put.title.on.spine.chart) {
    text(c(0.50),c(0.975),pos=3,labels=title.on.spine.chart,cex=01.5)
    y.location <- 0.925  # starting position with title
    }

  if (!put.title.on.spine.chart) y.location <- 0.975  # no-title start 
  
  # store top of vertical line for later plotting needs
  y.top.of.vertical.line <- y.location 

  x.center.position <- 0.75  # horizontal position of spine

  # begin primary plotting loop 
  # think of a plot as a collection of text and symbols on screen or paper
  # we are going to construct a plot one text string and symbol at a time
  # (note that we may have to repeat this process at the end of the program)
  for(k in seq(along=effect.names)) { 
    y.location <- y.location - large.space
    text(c(0.4),c(y.location),pos=2,
      labels=paste(effect.name.map(effect.names[k])," ",sep=""),cex=01.0)
    text(c(0.525),c(y.location),pos=2,col=color.for.printing.importance.text,
    labels=paste(" ",left.side.symbol.to.print.around.importance,
    pretty.print(
      unlist(conjoint.results$attribute.importance[effect.names[k]])),"%",
      right.side.symbol.to.print.around.importance,sep=""),cex=01.0)

  # begin loop for printing part-worths
    for(m in seq(1:number.of.levels.of.attribute[k])) { 
      y.location <- y.location - medium.space
      text(c(0.4),c(y.location),pos=2,
      conjoint.results$xlevel[[effect.names[k]]][m],cex=01.0)
   #   part.worth.label.data.frame[k,m],cex=01.0)

      text(c(0.525),c(y.location),pos=2,
      col=color.for.printing.part.worth.text,
      labels=paste(" ",left.side.symbol.to.print.around.part.worths,
      pretty.print(part.worth.plotting.list[[effect.names[k]]][m]),
      right.side.symbol.to.print.around.part.worths,sep=""),cex=01.0)
      
      
      points(part.worth.point.position[[effect.names[k]]][m],y.location, 
        type = "p", pch = 20, col = color.for.part.worth.point, cex = 2)
      segments(x.center.position, y.location, 
      part.worth.point.position[[effect.names[k]]][m], y.location,
         col = color.for.part.worth.line, lty = 1, lwd = 2)      
      } 
    } 
  y.location <- y.location - medium.space

  # begin center axis and bottom plotting
  y.bottom.of.vertical.line <- y.location  # store top of vertical line

  below.y.bottom.of.vertical.line <- y.bottom.of.vertical.line - small.space/2

  if (!draw.gray.background) {
  # four optional grid lines may be drawn on the plot parallel to the spine
    if (draw.optional.grid.lines) {
      segments(0.55, y.top.of.vertical.line, 0.55, 
        y.bottom.of.vertical.line, col = "black", lty = "solid", lwd = 1)  
      segments(0.65, y.top.of.vertical.line, 0.65, 
        y.bottom.of.vertical.line, col = "gray", lty = "solid", lwd = 1)  
      segments(0.85, y.top.of.vertical.line, 0.85, 
        y.bottom.of.vertical.line, col = "gray", lty = "solid", lwd = 1)    
      segments(0.95, y.top.of.vertical.line, 0.95, 
        y.bottom.of.vertical.line, col = "black", lty = "solid", lwd = 1)  
      }
    }
       
  # gray background for plotting area of the points
  if (draw.gray.background) {
    rect(xleft = 0.55, ybottom = y.bottom.of.vertical.line, 
      xright = 0.95, ytop = y.top.of.vertical.line, density = -1, angle = 45,
      col = "light gray", border = NULL, lty = "solid", lwd = 1)
     
  # four optional grid lines may be drawn on the plot parallel to the spine
    if (draw.optional.grid.lines) {
      segments(0.55, y.top.of.vertical.line, 0.55, 
        y.bottom.of.vertical.line, col = "black", lty = "solid", lwd = 1)  

      segments(0.65, y.top.of.vertical.line, 0.65, 
        y.bottom.of.vertical.line, col = "white", lty = "solid", lwd = 1)  
  
      segments(0.85, y.top.of.vertical.line, 0.85, 
        y.bottom.of.vertical.line, col = "white", lty = "solid", lwd = 1)    

      segments(0.95, y.top.of.vertical.line, 0.95, 
        y.bottom.of.vertical.line, col = "black", lty = "solid", lwd = 1)  
      }     
    }   

  # draw the all-important spine on the plot
  segments(x.center.position, y.top.of.vertical.line, x.center.position, 
    y.bottom.of.vertical.line, col = "black", lty = "dashed", lwd = 1)  

       
  # horizontal line at top           
  segments(0.55, y.top.of.vertical.line, 0.95, y.top.of.vertical.line,
       col = "black", lty = 1, lwd = 1)      

  # horizontal line at bottom       
  segments(0.55, y.bottom.of.vertical.line, 0.95, y.bottom.of.vertical.line,
         col = "black", lty = 1, lwd = 1)          
       
  # plot for ticks and labels 
  segments(0.55, y.bottom.of.vertical.line, 
     0.55, below.y.bottom.of.vertical.line,
     col = "black", lty = 1, lwd = 1)   # tick line at bottom
  segments(0.65, y.bottom.of.vertical.line, 
     0.65, below.y.bottom.of.vertical.line,
     col = "black", lty = 1, lwd = 1)   # tick line at bottom
  segments(0.75, y.bottom.of.vertical.line, 
     0.75, below.y.bottom.of.vertical.line,
     col = "black", lty = 1, lwd = 1)   # tick line at bottom      
  segments(0.85, y.bottom.of.vertical.line, 
     0.85, below.y.bottom.of.vertical.line,
     col = "black", lty = 1, lwd = 1)   # tick line at bottom      
  segments(0.95, y.bottom.of.vertical.line, 
     0.95, below.y.bottom.of.vertical.line,
     col = "black", lty = 1, lwd = 1)   # tick line at bottom      
              
  # axis labels vary with the max.is.less.than range being used
  if (max.is.less.than.1) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-1","-0.5","0","+0.5","+1"),cex=0.75)
  if (max.is.less.than.2) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-2","-1","0","+1","+2"),cex=0.75)
  if (max.is.less.than.4) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-4","-2","0","+2","+4"),cex=0.75)
  if (max.is.less.than.10) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-10","-5","0","+5","+10"),cex=0.75)
  if (max.is.less.than.20) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-20","-10","0","+10","+20"),cex=0.75)
  if (max.is.less.than.40) text(c(0.55,0.65,0.75,0.85,0.95),
    rep(below.y.bottom.of.vertical.line,times=5),
    pos=1,labels=c("-40","-20","0","+20","+40"),cex=0.75)
  y.location <- below.y.bottom.of.vertical.line - small.space

  if(do.standardization) 
    text(.75,y.location,pos=1,labels=c("Standardized Part-Worth"),cex=0.95)
    
  if(!do.standardization) text(.75,y.location,pos=1,labels=c("Part-Worth"),
    cex=0.95)

  y.location <- below.y.bottom.of.vertical.line - small.space


  if(do.standardization) 
    text(0.75,y.location,pos=1,labels=c("Standardized Part-Worth"),cex=0.95)
   
  if(!do.standardization) text(0.75,y.location,pos=1,labels=c("Part-Worth"),
    cex=0.95) 

  if(print.internal.consistency) {
    y.location <- y.location - medium.space
    text(c(0.525),c(y.location),pos=2,labels=paste("Internal consistency: ",
    pretty.print(conjoint.results$internal.consistency),
    sep=""))
    }

  # if we have grid lines we may have plotted over part-worth points
  # if we have a gray background then we have plotted over part-worth points
  # so let us plot those all-important part-worth points and lines once again
  if(draw.gray.background || draw.optional.grid.lines) {
    y.location <- y.top.of.vertical.line  # retreive the starting value 

  # repeat the primary plotting loop 
  for(k in seq(along=effect.names)) { 
    y.location <- y.location - large.space
    text(c(0.4),c(y.location),pos=2,
      labels=paste(effect.name.map(effect.names[k])," ",sep=""),cex=01.0)
    text(c(0.525),c(y.location),pos=2,col=color.for.printing.importance.text,
      labels=paste(" ",left.side.symbol.to.print.around.importance,
      pretty.print(
      unlist(conjoint.results$attribute.importance[effect.names[k]])),"%",
      right.side.symbol.to.print.around.importance,sep=""),cex=01.0)

 # begin loop for printing part-worths
      for(m in seq(1:number.of.levels.of.attribute[k])) { 
         y.location <- y.location - medium.space
         text(c(0.4),c(y.location),pos=2,
         conjoint.results$xlevel[[effect.names[k]]][m],cex=01.0)
         text(c(0.525),c(y.location),
           pos=2,col=color.for.printing.part.worth.text,
           labels=paste(" ",left.side.symbol.to.print.around.part.worths,
           pretty.print(part.worth.plotting.list[[effect.names[k]]][m]),
           right.side.symbol.to.print.around.part.worths,sep=""),cex=01.0)

      points(part.worth.point.position[[effect.names[k]]][m],y.location, 
         type = "p", pch = 20, col = color.for.part.worth.point, cex = 2)
      segments(x.center.position, y.location, 
      part.worth.point.position[[effect.names[k]]][m], y.location,
         col = color.for.part.worth.line, lty = 1, lwd = 2)      
      } 
    } 
  } 
}
# save spine.chart function for future work
save(spine.chart,file="mtpa_spine_chart.Rdata")



# Market Simulation Utilities

# user-defined function for first-choice simulation rule
first.choice.simulation.rule <- function(response, alpha = 1) {
  # begin function for first-choice rule
  # returns binary vector or response vector with equal division 
  # of 1 across all locations at the maximum
  # use alpha for desired sum across respondents
  # alpha useful when the set of tested profiles is not expected to be one
  if(alpha < 0 || alpha > 1) stop("alpha must be between zero and one")
  response.vector <- numeric(length(response))
  for(k in seq(along=response))
    if(response[k] == max(response)) response.vector[k] <- 1
  alpha*(response.vector/sum(response.vector))  
  }  # end first-choice rule function

# user-defined function for predicted choices from four-profile choice sets 
choice.set.predictor <- function(predicted.probability) {
  predicted.choice <- length(predicted.probability)  # initialize 
  index.fourth <- 0  # initialize block-of-four choice set indices
  while (index.fourth < length(predicted.probability)) {
    index.first  <- index.fourth + 1
    index.second <- index.fourth + 2
    index.third  <- index.fourth + 3
    index.fourth <- index.fourth + 4
    this.choice.set.probability.vector <- 
      c(predicted.probability[index.first],
      predicted.probability[index.second],
      predicted.probability[index.third],
      predicted.probability[index.fourth])
    predicted.choice[index.first:index.fourth] <- 
      first.choice.simulation.rule(this.choice.set.probability.vector)  
    }
  predicted.choice <- factor(predicted.choice, levels = c(0,1), 
    labels = c("NO","YES"))
  predicted.choice  
  } # end choice.set.predictor function 
  
# save market simulation utilities for future work
save(first.choice.simulation.rule,
  choice.set.predictor,
  file="mtpa_market_simulation_utilities.Rdata")
  
  
  
# Split-Plotting Utilities with grid Graphics

library(grid)  # grid graphics foundation of split-plotting utilities

# functions used with ggplot2 graphics to split the plotting region
# to set margins and to plot more than one ggplot object on one page/screen

vplayout <- function(x, y) 
viewport(layout.pos.row=x, layout.pos.col=y) 


# grid graphics utility plots one plot with margins
ggplot.print.with.margins <- function(ggplot.object.name,left.margin.pct=10,
  right.margin.pct=10,top.margin.pct=10,bottom.margin.pct=10)
{ # begin function for printing ggplot objects with margins
  # margins expressed as percentages of total... use integers
 grid.newpage() 
pushViewport(viewport(layout=grid.layout(100,100)))
print(ggplot.object.name, 
  vp=vplayout((0 + top.margin.pct):(100 - bottom.margin.pct),
  (0 + left.margin.pct):(100 - right.margin.pct))) 
} # end function for printing ggplot objects with margins



# grid graphics utility plots two ggplot plotting objects in one column
special.top.bottom.ggplot.print.with.margins <- 
  function(ggplot.object.name,ggplot.text.tagging.object.name,
  left.margin.pct=5,right.margin.pct=5,top.margin.pct=5,
  bottom.margin.pct=5,plot.pct=80,text.tagging.pct=10) { 
# begin function for printing ggplot objects with margins 
# and text tagging at bottom of plot
# margins expressed as percentages of total... use integers
  if((top.margin.pct + bottom.margin.pct + plot.pct + text.tagging.pct) != 100) 
    stop(paste("function special.top.bottom.ggplot.print.with.margins()",
    "execution terminated:\n   top.margin.pct + bottom.margin.pct + ",
    "plot.pct + text.tagging.pct not equal to 100 percent",sep=""))  
  grid.newpage() 
  pushViewport(viewport(layout=grid.layout(100,100)))
  print(ggplot.object.name, 
  vp=vplayout((0 + top.margin.pct):
    (100 - (bottom.margin.pct + text.tagging.pct)),
  (0 + left.margin.pct):(100 - right.margin.pct))) 

  print(ggplot.text.tagging.object.name, 
    vp=vplayout((0 + (top.margin.pct + plot.pct)):(100 - bottom.margin.pct),
    (0 + left.margin.pct):(100 - right.margin.pct))) 
} # end function for printing ggplot objects with margins and text tagging





# grid graphics utility plots three ggplot plotting objects in one column
three.part.ggplot.print.with.margins <- function(ggfirstplot.object.name,
ggsecondplot.object.name,
ggthirdplot.object.name,
left.margin.pct=5,right.margin.pct=5,
top.margin.pct=10,bottom.margin.pct=10,
first.plot.pct=25,second.plot.pct=25,
third.plot.pct=30) { 
# function for printing ggplot objects with margins and top and bottom plots
# margins expressed as percentages of total... use integers
if((top.margin.pct + bottom.margin.pct + first.plot.pct + 
  second.plot.pct  + third.plot.pct) != 100) 
    stop(paste("function special.top.bottom.ggplot.print.with.margins()",
         "execution terminated:\n   top.margin.pct + bottom.margin.pct",
         "+ first.plot.pct + second.plot.pct  + third.plot.pct not equal",
         "to 100 percent",sep=""))  
grid.newpage() 
pushViewport(viewport(layout=grid.layout(100,100)))

print(ggfirstplot.object.name, vp=vplayout((0 + top.margin.pct):
  (100 - (second.plot.pct  + third.plot.pct + bottom.margin.pct)),
  (0 + left.margin.pct):(100 - right.margin.pct))) 

print(ggsecondplot.object.name, 
  vp=vplayout((0 + top.margin.pct + first.plot.pct):
  (100 - (third.plot.pct + bottom.margin.pct)),
  (0 + left.margin.pct):(100 - right.margin.pct))) 

print(ggthirdplot.object.name, 
  vp=vplayout((0 + top.margin.pct + first.plot.pct + second.plot.pct):
  (100 - (bottom.margin.pct)),(0 + left.margin.pct):
  (100 - right.margin.pct))) 
} 

# save split-plotting utilities for future work
save(vplayout,
  ggplot.print.with.margins,
  special.top.bottom.ggplot.print.with.margins,
  three.part.ggplot.print.with.margins,
  file="mtpa_split_plotting_utilities.Rdata")


# Wait-Time Ribbon Plot

wait.time.ribbon <- function(wait.service.data, title = "", 
  wait.time.goal = 30, wait.time.max = 90, 
  plotting.min = 0, plotting.max = 250,
  use.text.tagging = TRUE) {
  
#  requires ggplot2 package
#  data visualization for operations management
#  wait.service.data is input data frame with the named columns as follows: 
#    hour: integer hour of the day on 24-hour clock
#    wait: integer call wait time in seconds
#    service:  integer call service time in seconds (NA for no service)
#    server:  character string for server name or code
#             assumes that there is a distict character string for no server
#             this string is coded as NO_SERVER
#  wait.time.goal:  desired maximum wait time (30 seconds default)
#                   represented as bottom of yellow region
#  wait.time.max: when wait time becomes intolerable (90 seconds default)
#                 represented as top of yellow region
# use.text.tagging default is TRUE for added text at bottom of plot

# set constants for ribbon plotting
MIN.SAMPLE <- 5  # min sample size for hourly calcuations
PERCENTILE.MIN <- 0.50  # used for bottom of acceptable wait time
PERCENTILE.MAX <- 0.90  # used for bottom of acceptable wait time

add_footnote_at_bottom_of_ribbon_plot <- TRUE
percentile.footnote <- paste("Bottom of ribbon = ",
  100*PERCENTILE.MIN, "th percentile of wait times",
  "    Top of ribbon = ", 100*PERCENTILE.MAX, "th percentile of wait times.", 
  sep = "")

x.hour <- seq(from=0,to=23) # for horixontal axis scale

# code for ribbon region counts
calls.per.hour <- numeric(24)  # total calls initialized as zero
served.calls <- numeric(24)  # served calls initialized as zero
dropped.calls <- numeric(24)  # dropped/abandoned calls initialize as zero
ymin.percentile <- rep(NA,times=24)  # store for minimum percentile values
ymax.percentile <- rep(NA,times=24)  # store maximum percentile values

# compute number of calls per hour
# code more versatile than table command
# to accommodate hours with no calls

for(index.for.hour in 1:24) { 
# begin for-loop for wait-time data call counts and percentile calculations

# 24-hour clock has first hour coded as zero in input data file
  coded.index.for.hour <- index.for.hour - 1  
  temporary.vector <- na.omit(wait.service.data$hour)
  calls.per.hour[index.for.hour] <- 
    sum(ifelse(temporary.vector==coded.index.for.hour,1,0))

    if(calls.per.hour[index.for.hour] >= MIN.SAMPLE) { 
# begin if-block for computing ymin and ymax values and number of servers
# when there are at least MIN.SAMPLE calls in the hour
    this.hour.wait.service.data <- 
      wait.service.data[(wait.service.data$hour == coded.index.for.hour),]

    ymin.percentile[index.for.hour] <- 
      quantile(this.hour.wait.service.data$wait,
      probs=c(PERCENTILE.MIN),na.rm = TRUE,names=FALSE,type=8)

    ymax.percentile[index.for.hour] <- 
      quantile(this.hour.wait.service.data$wait,
      probs=c(PERCENTILE.MAX),na.rm = TRUE,names=FALSE,type=8)      
    } # end if-block for computing ymin and ymax values 

# if insufficient data we set min and max to be wait.time.goal
  if(calls.per.hour[index.for.hour] < MIN.SAMPLE) {
    ymin.percentile[index.for.hour] <- wait.time.goal
    ymax.percentile[index.for.hour] <- wait.time.goal
    }
  } # end for-loop for wait-time data call counts and percentile calculations  

# compute number.of.servers data and served and dropped calls
number.of.servers <- numeric(24)  # initialize to zero
for(index.for.hour in 1:24) { 
# begin for-loop for obtaining server data for the ribbon plot
# 24-hour clock has first hour coded as zero in input data file
  coded.index.for.hour <- index.for.hour - 1  
  temporary.vector <- na.omit(wait.service.data$hour)
  calls.per.hour[index.for.hour] <- 
    sum(ifelse(temporary.vector==coded.index.for.hour,1,0))
  this.hour.wait.service.data <- 
      wait.service.data[(wait.service.data$hour == coded.index.for.hour),]   
      
  served.calls[index.for.hour] <- 
    nrow(subset(this.hour.wait.service.data, subset=(server != "NO_SERVER")))   
  dropped.calls[index.for.hour] <- 
    nrow(subset(this.hour.wait.service.data, subset=(server == "NO_SERVER")))    
      
  if (nrow(this.hour.wait.service.data) > 0) {
# count is based upon the number of unique server names less NO_SERVER
    servers <- 
      na.omit((unique(this.hour.wait.service.data$server)))
    valid.servers <- setdiff(servers, "NO_SERVER")
    number.of.servers[index.for.hour] <- length(valid.servers)
    }
  } # end for-loop for obtaining server data for the ribbon plot
  
       
greenmin <- rep(plotting.min, length=24)
greenmax <- rep(wait.time.goal, length=24)

yellowmin <- rep(wait.time.goal, length=24)
yellowmax <- rep(wait.time.max, length=24)

redmin <- rep(wait.time.max, length=24)
redmax <- rep(plotting.max, length=24)

ymax.topwhite <- rep(plotting.max,length=24)
ymin.topwhite <- ymax.percentile

ymax.bottomwhite <- ymin.percentile
ymin.bottomwhite <- rep(plotting.min,length=24)

# define data frame for plotting wait and service information for this day  
call.center.plotting.frame <- 
  data.frame(x.hour, ymin.percentile, ymax.percentile, 
    calls.per.hour, number.of.servers,
    greenmin,greenmax,
    yellowmin,yellowmax,
    redmin,redmax,
    ymin.bottomwhite,ymax.bottomwhite,
    ymin.topwhite,ymax.topwhite)  

#cat("\n\n","------------- ",title," -------------","\n")
#print(call.center.plotting.frame)

ggobject <- ggplot() + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=greenmin, ymax=greenmax), 
stat="identity",colour="white",fill="darkgreen") + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=yellowmin, ymax=yellowmax), 
stat="identity",colour="white",fill="yellow") + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=redmin, ymax=redmax), 
stat="identity",colour="white",fill="red") + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=ymin.topwhite, ymax=ymax.topwhite), 
stat="identity",colour="white",fill="white") + 
geom_ribbon(data=call.center.plotting.frame, 
mapping=aes(x=x.hour, ymin=ymin.bottomwhite, ymax=ymax.bottomwhite), 
stat="identity",colour="white",fill="white") + 
geom_hline(data=call.center.plotting.frame, 
mapping=aes(yintercept=yellowmin[1])) + 
geom_hline(data=call.center.plotting.frame, 
mapping=aes(yintercept=redmin[1])) + 
labs(title = title) + theme_bw(base_size = 12) + 
scale_y_continuous(limits = c(greenmin[1], redmax[1])) + 
xlab("Hour of Day (24-Hour Clock)") + 
ylab("Wait Time (Seconds)") 

# plotting with all default margins no text at bottom
if(!use.text.tagging) ggplot.print.with.margins(ggobject) 

# plotting with text tagging requires the creation of a ggplot text object
if(use.text.tagging) 
{

# define character data for the text taggging at bottom of plot
hour.title <- "Hour:"
hour.00 <- "00"
hour.01 <- "01"
hour.02 <- "02"
hour.03 <- "03"
hour.04 <- "04"
hour.05 <- "05"
hour.06 <- "06"
hour.07 <- "07"
hour.08 <- "08"
hour.09 <- "09"
hour.10 <- "10"
hour.11 <- "11"
hour.12 <- "12"
hour.13 <- "13"
hour.14 <- "14"
hour.15 <- "15"
hour.16 <- "16"
hour.17 <- "17"
hour.18 <- "18"
hour.19 <- "19"
hour.20 <- "20"
hour.21 <- "21"
hour.22 <- "22"
hour.23 <- "23"

calls.title <- "Calls:"  
calls.00 <- as.character(calls.per.hour[1])
calls.01 <- as.character(calls.per.hour[2])
calls.02 <- as.character(calls.per.hour[3])
calls.03 <- as.character(calls.per.hour[4])
calls.04 <- as.character(calls.per.hour[5])
calls.05 <- as.character(calls.per.hour[6])
calls.06 <- as.character(calls.per.hour[7])
calls.07 <- as.character(calls.per.hour[8])
calls.08 <- as.character(calls.per.hour[9])
calls.09 <- as.character(calls.per.hour[10])
calls.10 <- as.character(calls.per.hour[11])
calls.11 <- as.character(calls.per.hour[12])
calls.12 <- as.character(calls.per.hour[13])
calls.13 <- as.character(calls.per.hour[14])
calls.14 <- as.character(calls.per.hour[15])
calls.15 <- as.character(calls.per.hour[16])
calls.16 <- as.character(calls.per.hour[17])
calls.17 <- as.character(calls.per.hour[18])
calls.18 <- as.character(calls.per.hour[19])
calls.19 <- as.character(calls.per.hour[20])
calls.20 <- as.character(calls.per.hour[21])
calls.21 <- as.character(calls.per.hour[22])
calls.22 <- as.character(calls.per.hour[23])
calls.23 <- as.character(calls.per.hour[24])

servers.title <- "Servers:" 
servers.00 <- as.character(number.of.servers[1])
servers.01 <- as.character(number.of.servers[2])
servers.02 <- as.character(number.of.servers[3])
servers.03 <- as.character(number.of.servers[4])
servers.04 <- as.character(number.of.servers[5])
servers.05 <- as.character(number.of.servers[6])
servers.06 <- as.character(number.of.servers[7])
servers.07 <- as.character(number.of.servers[8])
servers.08 <- as.character(number.of.servers[9])
servers.09 <- as.character(number.of.servers[10])
servers.10 <- as.character(number.of.servers[11])
servers.11 <- as.character(number.of.servers[12])
servers.12 <- as.character(number.of.servers[13])
servers.13 <- as.character(number.of.servers[14])
servers.14 <- as.character(number.of.servers[15])
servers.15 <- as.character(number.of.servers[16])
servers.16 <- as.character(number.of.servers[17])
servers.17 <- as.character(number.of.servers[18])
servers.18 <- as.character(number.of.servers[19])
servers.19 <- as.character(number.of.servers[20])
servers.20 <- as.character(number.of.servers[21])
servers.21 <- as.character(number.of.servers[22])
servers.22 <- as.character(number.of.servers[23])
servers.23 <- as.character(number.of.servers[24])

served.title <- "Served:" 
served.00 <- as.character(served.calls[1])
served.01 <- as.character(served.calls[2])
served.02 <- as.character(served.calls[3])
served.03 <- as.character(served.calls[4])
served.04 <- as.character(served.calls[5])
served.05 <- as.character(served.calls[6])
served.06 <- as.character(served.calls[7])
served.07 <- as.character(served.calls[8])
served.08 <- as.character(served.calls[9])
served.09 <- as.character(served.calls[10])
served.10 <- as.character(served.calls[11])
served.11 <- as.character(served.calls[12])
served.12 <- as.character(served.calls[13])
served.13 <- as.character(served.calls[14])
served.14 <- as.character(served.calls[15])
served.15 <- as.character(served.calls[16])
served.16 <- as.character(served.calls[17])
served.17 <- as.character(served.calls[18])
served.18 <- as.character(served.calls[19])
served.19 <- as.character(served.calls[20])
served.20 <- as.character(served.calls[21])
served.21 <- as.character(served.calls[22])
served.22 <- as.character(served.calls[23])
served.23 <- as.character(served.calls[24])

dropped.title <- "Dropped:" 
dropped.00 <- as.character(dropped.calls[1])
dropped.01 <- as.character(dropped.calls[2])
dropped.02 <- as.character(dropped.calls[3])
dropped.03 <- as.character(dropped.calls[4])
dropped.04 <- as.character(dropped.calls[5])
dropped.05 <- as.character(dropped.calls[6])
dropped.06 <- as.character(dropped.calls[7])
dropped.07 <- as.character(dropped.calls[8])
dropped.08 <- as.character(dropped.calls[9])
dropped.09 <- as.character(dropped.calls[10])
dropped.10 <- as.character(dropped.calls[11])
dropped.11 <- as.character(dropped.calls[12])
dropped.12 <- as.character(dropped.calls[13])
dropped.13 <- as.character(dropped.calls[14])
dropped.14 <- as.character(dropped.calls[15])
dropped.15 <- as.character(dropped.calls[16])
dropped.16 <- as.character(dropped.calls[17])
dropped.17 <- as.character(dropped.calls[18])
dropped.18 <- as.character(dropped.calls[19])
dropped.19 <- as.character(dropped.calls[20])
dropped.20 <- as.character(dropped.calls[21])
dropped.21 <- as.character(dropped.calls[22])
dropped.22 <- as.character(dropped.calls[23])
dropped.23 <- as.character(dropped.calls[24])

# set up spacing and positioning for the table
y.current.level <- 1.0  # initialze position
y.large.space <- 0.175
y.medium.space <- 0.125
y.small.space <- 0.075

table.left.margin <- 0.1  # needed for row labels at left
horizontal.offset <- (1-table.left.margin)/24  # spacing in the text table

y.current.level <- y.current.level - y.medium.space

ggtextobject <- ggplot(data=data.frame(x = 0.5,y = y.current.level),
  aes(x=x,y=y,xmin=0,xmax=1,ymin=0,ymax=1), 
  stat="identity", position="identity") + labs(x=NULL,y=NULL) +  
geom_text(x = 0.025,y = y.current.level,label = hour.title,
aes(size=10.55),colour="black")  + 
geom_text(x = (00*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.00, aes(size=10.55),colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.01, aes(size=10.55),colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.02, aes(size=10.55),colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.03, aes(size=10.55),colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.04, aes(size=10.55),colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.05, aes(size=10.55),colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.06, aes(size=10.55),colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.07, aes(size=10.55),colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.08, aes(size=10.55),colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.09, aes(size=10.55),colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.10, aes(size=10.55),colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.11, aes(size=10.55),colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.12, aes(size=10.55),colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.13, aes(size=10.55),colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.14, aes(size=10.55),colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y= y.current.level,label = hour.15, aes(size=10.55),colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.16, aes(size=10.55),colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.17, aes(size=10.55),colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.18, aes(size=10.55),colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.19, aes(size=10.55),colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.20, aes(size=10.55),colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.21, aes(size=10.55),colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.22, aes(size=10.55),colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level,label = hour.23, aes(size=10.55),colour="black")   


y.current.level <- y.current.level - y.medium.space

ggtextobject <- ggtextobject + geom_text(x = 0.025,
y = y.current.level, label = servers.title,aes(size=10.55),colour="black") + 
geom_text(x = (00*horizontal.offset + table.left.margin), 
y = y.current.level, label = servers.00,aes(size=10.55),colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.01,aes(size=10.55),colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.02,aes(size=10.55),colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.03,aes(size=10.55),colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.04,aes(size=10.55),colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.05,aes(size=10.55),colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.06,aes(size=10.55),colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.07,aes(size=10.55),colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.08,aes(size=10.55),colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.09,aes(size=10.55),colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.10,aes(size=10.55),colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.11,aes(size=10.55),colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.12,aes(size=10.55),colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.13,aes(size=10.55),colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.14,aes(size=10.55),colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.15,aes(size=10.55),colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.16,aes(size=10.55),colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.17,aes(size=10.55),colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.18,aes(size=10.55),colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.19,aes(size=10.55),colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.20,aes(size=10.55),colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.21,aes(size=10.55),colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.22,aes(size=10.55),colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level, label = servers.23,aes(size=10.55),colour="black")  

# store line position for bottom of text segment of the visualization

y.level.divider.line <- y.current.level - y.medium.space
# temporary data frame needed to input to geom_hline later
middle.line.data <- data.frame(y.level.divider.line) 

y.current.level <- y.level.divider.line - y.medium.space

ggtextobject <- ggtextobject +  
geom_text(x = 0.025,y = y.current.level,
label = calls.title, aes(size=10.55),colour="black") + 
geom_text(x = (00*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.00, aes(size=10.55),colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.01, aes(size=10.55),colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.02, aes(size=10.55),colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.03, aes(size=10.55),colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.04, aes(size=10.55),colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.05, aes(size=10.55),colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.06, aes(size=10.55),colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.07, aes(size=10.55),colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.08, aes(size=10.55),colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.09, aes(size=10.55),colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.10, aes(size=10.55),colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.11, aes(size=10.55),colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.12, aes(size=10.55),colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.13, aes(size=10.55),colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.14, aes(size=10.55),colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.15, aes(size=10.55),colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.16, aes(size=10.55),colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.17, aes(size=10.55),colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.18, aes(size=10.55),colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.19, aes(size=10.55),colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.20, aes(size=10.55),colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.21, aes(size=10.55),colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.22, aes(size=10.55),colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level, label = calls.23, aes(size=10.55),colour="black")   

y.current.level <- y.current.level - y.medium.space

ggtextobject <- ggtextobject +  
geom_text(x = 0.025,y = y.current.level,
label = served.title, aes(size=10.55),colour="black") + 
geom_text(x = (00*horizontal.offset + table.left.margin),
y = y.current.level, label = served.00, aes(size=10.55),colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level, label = served.01, aes(size=10.55),colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level, label = served.02, aes(size=10.55),colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level, label = served.03, aes(size=10.55),colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level, label = served.04, aes(size=10.55),colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level, label = served.05, aes(size=10.55),colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level, label = served.06, aes(size=10.55),colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level, label = served.07, aes(size=10.55),colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level, label = served.08, aes(size=10.55),colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level, label = served.09, aes(size=10.55),colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level, label = served.10, aes(size=10.55),colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level, label = served.11, aes(size=10.55),colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level, label = served.12, aes(size=10.55),colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level, label = served.13, aes(size=10.55),colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level, label = served.14, aes(size=10.55),colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y = y.current.level, label = served.15, aes(size=10.55),colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level, label = served.16, aes(size=10.55),colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level, label = served.17, aes(size=10.55),colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level, label = served.18, aes(size=10.55),colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level, label = served.19, aes(size=10.55),colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level, label = served.20, aes(size=10.55),colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level, label = served.21, aes(size=10.55),colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level, label = served.22, aes(size=10.55),colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level, label = served.23, aes(size=10.55),colour="black")   

y.current.level <- y.current.level - y.medium.space

ggtextobject <- ggtextobject +  
geom_text(x = 0.025,y = y.current.level,
label = dropped.title, aes(size=10.55),colour="black") + 
geom_text(x = (00*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.00, aes(size=10.55),colour="black") + 
geom_text(x = (01*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.01, aes(size=10.55),colour="black") + 
geom_text(x = (02*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.02, aes(size=10.55),colour="black") + 
geom_text(x = (03*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.03, aes(size=10.55),colour="black") + 
geom_text(x = (04*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.04, aes(size=10.55),colour="black") + 
geom_text(x = (05*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.05, aes(size=10.55),colour="black") + 
geom_text(x = (06*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.06, aes(size=10.55),colour="black") + 
geom_text(x = (07*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.07, aes(size=10.55),colour="black") + 
geom_text(x = (08*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.08, aes(size=10.55),colour="black") + 
geom_text(x = (09*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.09, aes(size=10.55),colour="black") + 
geom_text(x = (10*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.10, aes(size=10.55),colour="black") + 
geom_text(x = (11*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.11, aes(size=10.55),colour="black") + 
geom_text(x = (12*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.12, aes(size=10.55),colour="black") + 
geom_text(x = (13*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.13, aes(size=10.55),colour="black") + 
geom_text(x = (14*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.14, aes(size=10.55),colour="black") + 
geom_text(x = (15*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.15, aes(size=10.55),colour="black") + 
geom_text(x = (16*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.16, aes(size=10.55),colour="black") + 
geom_text(x = (17*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.17, aes(size=10.55),colour="black") + 
geom_text(x = (18*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.18, aes(size=10.55),colour="black") + 
geom_text(x = (19*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.19, aes(size=10.55),colour="black") + 
geom_text(x = (20*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.20, aes(size=10.55),colour="black") + 
geom_text(x = (21*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.21, aes(size=10.55),colour="black") + 
geom_text(x = (22*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.22, aes(size=10.55),colour="black") + 
geom_text(x = (23*horizontal.offset + table.left.margin),
y = y.current.level, label = dropped.23, aes(size=10.55),colour="black")  

y.level.divider.line <- y.current.level - y.medium.space
# temporary data frame needed to input to geom_hline later
bottom.line.data <- data.frame(y.level.divider.line) 

y.current.level <- y.level.divider.line - y.medium.space

# add footnote centered at bottom of plot if requested
if (add_footnote_at_bottom_of_ribbon_plot)
  ggtextobject <- ggtextobject +  
  geom_text(x = 0.5,y = y.current.level,
  label = percentile.footnote, aes(size=10.55), colour="black") 

# finish up the plot with background definition and divider lines
ggtextobject <- ggtextobject + geom_hline(aes(yintercept=1)) + 
geom_hline(data=bottom.line.data, 
  mapping = aes(yintercept = y.level.divider.line)) + 
geom_hline(data=middle.line.data, 
  mapping = aes(yintercept = y.level.divider.line)) + 
theme(legend.position = "none")  + 
theme(panel.grid.minor = element_blank()) + 
theme(panel.grid.major = element_blank())  + 
theme(panel.background = element_blank()) + 
theme(axis.ticks = element_blank()) + 
scale_y_continuous(breaks=c(0,1),label=c("","")) + 
scale_x_continuous(breaks=c(0,1),label=c("",""))

# user-defined function plots with text annotation/tagging at the bottom
special.top.bottom.ggplot.print.with.margins(ggobject,ggtextobject,
plot.pct=55,text.tagging.pct=35) 
}
} # end of wait-time ribbon function

# save wait-time ribbon utility for future work
save(wait.time.ribbon,
  file="mtpa_wait_time_ribbon_utility.Rdata")


# Text Analysis of Movie Tag Lines

library(tm)  # text mining and document management
library(stringr)  # character manipulation with regular expressions
library(grid)  # grid graphics utilities
library(ggplot2)  # graphics
library(latticeExtra)  # package used for text horizon plot
library(wordcloud)  # provides utility for plotting non-overlapping text
library(cluster)  # cluster analysis

# load user-defined plotting utilities
load("mtpa_split_plotting_utilities.Rdata")

# standardization needed for text measures
standardize <- function(x) {(x - mean(x)) / sd(x)}

# convert to bytecodes to avoid "invalid multibyte string" messages
bytecode.convert <- function(x) {iconv(enc2utf8(x), sub = "byte")}

# read in positive and negative word lists from Hu and Liu (2004)
positive.data.frame <- read.table(file = "Hu_Liu_positive_word_list.txt",
  header = FALSE, colClasses = c("character"), row.names = NULL, col.names = "positive.words")
positive.data.frame$positive.words <- 
  bytecode.convert(positive.data.frame$positive.words)
  
negative.data.frame <- read.table(file = "Hu_Liu_negative_word_list.txt",
  header = FALSE, colClasses = c("character"), row.names = NULL, col.names = "negative.words")  
negative.data.frame$negative.words <- 
  bytecode.convert(negative.data.frame$negative.words)

# NLINES <- 21  # for development and test runs
# input.data.file.name <- "taglines_list_sample.txt"
#  scan("taglines_list_sample.txt", what = "character")  # development/test runs
# nlines_to_read <- 21  # for development and test runs

# there are 345317 records in the full taglines data file
# the number of lines in the input data file
# or maximum number of lines to read
NLINES <- 345317   
input.data.file.name <- "taglines_copy_data.txt"  # production runs
# read the data in blocks of nlines_to_read at a time
nlines_to_read <- 10000  # size of block of lines to read

# debug print was used during the code development process
debub.print.mode <- FALSE
debug.print <- function(title,date,tagline,status) {
  cat("\n title =",title,"  date = ", date," tagline",
    tagline, " status = ",status,"\n")
  }

# this user-defined function shows how R can be used to parse text input
tagline.parser <- function(input.list) {
# where we start depends upon the status on entry
# tagline parser can only be in one status at a time
# begin
# indicator
# title (actually a title and date status)
# moretitle (another title and data status, but following a previous title)
# tagline
# comment

# data are not clean... if you get a new movie indicator "#" start a new movie
# we may lose a few movies this way... but that is better than editing a file
# with about 40 thousand movies..

# at this time all valid dates should look be six characters long
# four numbers surrounded by parentheses 
# lets use The Birth of a Nation (1915) as the earliest possible valid date
# and the current year as the latest possible valid date 
# obtained by as.numeric(format(Sys.time(), "%Y"))
valid.years <- 1915:as.numeric(format(Sys.time(), "%Y"))
valid.years.strings.four <- paste("(",as.character(valid.years),sep="")

   text <- input.list[[1]]
   status <- input.list[[2]]
   title <- input.list[[3]]
   date <- input.list[[4]]
   tagline <- input.list[[5]]
   
   nitems <- length(text)
   ncount <- 1  # initialize on entry
   tagline_data.store <- NULL
   
   while(ncount < nitems) {   
# debug printing was used in the development and testing of parsing logic   
     if (debub.print.mode) debug.print(title,date,tagline,status) 
     if (status == "indicator" | status == "begin") {
       if (ncount <= nitems) {
         ncount <- ncount + 1
         status <- "initialtitle"
         title <- " "  # blank title to start
         date <- " "   # blank date to start
         tagline <- " "  # blank tagline to start
         }
       }
       
     if (status == "initialtitle") {
       if (ncount <= nitems) {
         title <- text[ncount]
         ncount <- ncount + 1
         if (ncount <= nitems) {
           test_date <- text[ncount]
           if (substring(test_date,1,5) %in% valid.years.strings.four) {
             date <- test_date
             ncount <- ncount + 1
             status <- "tagline"
             }
           if (!(substring(test_date,1,5) %in% valid.years.strings.four)) {     
             if (test_date == "#") {
                 status <- "indicator"
                 }           
             if (test_date != "#") {
                 title <- paste(title, test_date)  
                 ncount <- ncount + 1 
                 status <- "moretitle"
                 }               
             } 
           }  
         }
       }
                                        
     if (status == "moretitle") {
       if (ncount <= nitems) {
         ncount <- ncount + 1
         if (ncount <= nitems) {
           test_date <- text[ncount]
           if (substring(test_date,1,5) %in% valid.years.strings.four) {
             date <- test_date
             ncount <- ncount + 1
             status <- "tagline"
             }
           if (!(substring(test_date,1,5) %in% valid.years.strings.four)) {     
             if (test_date == "#") {
                 status <- "indicator"
                 }           
             if (test_date != "#") {
                 title <- paste(title, test_date)  
                 ncount <- ncount + 1  
                 }               
             } 
           }  
         }
       }                    
                                       
       if (status == "tagline") {
         if (ncount <= nitems) {
           new_text <- text[ncount]
           if (new_text == "#") {
             tagline_data.store <- rbind(tagline_data.store,
                data.frame(title, date, tagline, stringsAsFactors = FALSE))
                status <- "indicator"
             }           
           if (new_text != "#") {
             if (substring(new_text,1,1) == "{") {
               ncount <- ncount + 1
               status <- "comment"
               }
             if (substring(new_text,1,1) != "{") {
               tagline <- paste(tagline, new_text)
               ncount <- ncount + 1
               }
             }             
           }          
         }
  
       if (status == "comment") {
         if (ncount <= nitems) {
           new_text <- text[ncount]         
           if (substring(new_text,nchar(new_text),nchar(new_text)) == "}") {
             ncount <- ncount + 1
             status <- "tagline"
             }
           if (substring(new_text,nchar(new_text),nchar(new_text)) != "}") {
             ncount <- ncount + 1
             }
           }
         }  
  } # end of primary while-loop
list(tagline_data.store, status, title, date, tagline)  # return list
}  # end of function

cat("\n\n","NUMBER OF LINES READ: ")

skip <- 0  # initialize the number of lines to skip
nlines_read_so_far <- 0  # intitialze number of lines read so far


status <- "begin"  # initial status
title <- " "  # blank title to start
date <- " "   # blank date to start
tagline <- " "  # blank tagline to start

data.store <- NULL  # initialize the data frame for storing text data

while(nlines_read_so_far < NLINES)  {

if ((NLINES - nlines_read_so_far) < nlines_to_read) 
  nlines_to_read <- (NLINES - nlines_read_so_far)
  
text <- scan(file = input.data.file.name, what = "character",
    skip = nlines_read_so_far, nlines = nlines_to_read)
 
# convert individual text items to bytecodes 
# to avoid to avoid "invalid multibyte string" error messages going forward
text <- bytecode.convert(text)

input.list <- list(text, status, title, date, tagline)  
 
# parse this block of text with the tagline parser function 
output.list <- tagline.parser(input.list) 
  
  new_data_for_store <- output.list[[1]]
  status <- output.list[[2]]
  title <- output.list[[3]]
  date <- output.list[[4]]
  tagline <- output.list[[5]]
      
  data.store <- rbind(data.store, new_data_for_store)
  
  nlines_read_so_far <- nlines_read_so_far + nlines_to_read
  
  cat(" ","nlines_read_so_far:",nlines_read_so_far)
  }
  
# if there is full movie info in output list 
# add this last movie to the end of the data.store

if ((!is.null(output.list[[3]])) & 
   (!is.null(output.list[[4]])) &
   (!is.null(output.list[[5]]))) {
       title <- output.list[[3]]
       date <- output.list[[4]]
       tagline <- output.list[[5]] 
    data.store <- rbind(data.store, 
      data.frame(title, date, tagline, stringsAsFactors = FALSE))
  }
    
# data cleaning... check the date field... 
# if it does not start with "(" or end with ")"
# strip any character other than numeric in the date field
# using regular experessions coding and the replace function from stringr
data.store$replace.date <- str_replace_all(data.store$date, "[^.(0-9)]", "")

# at this time all valid dates should look be six characters long
# four numbers surrounded by parentheses 
# lets use The Birth of a Nation (1915) as the earliest possible valid date
# and the current year as the latest possible valid date 
# obtained by as.numeric(format(Sys.time(), "%Y"))
valid.years <- 1915:as.numeric(format(Sys.time(), "%Y"))
valid.years.strings <- paste("(",as.character(valid.years),")",sep="")

# valid observations must have dates with valid.years.strings
data.store$valid <- 
  ifelse((data.store$replace.date %in% valid.years.strings),"YES","NO")

# use the subset of movies with valid data
valid.data.store <- subset(data.store, subset = (valid == "YES"))

# add date field to title field to create unique identifier for each movie
valid.data.store$movie <- paste(valid.data.store$title, valid.data.store$date)

# strip the parenteses from replace.date and create an integer variable for year
valid.data.store$replace.date <- 
  str_replace(valid.data.store$replace.date,"[(]","")
valid.data.store$replace.date <- 
  str_replace(valid.data.store$replace.date,"[)]","")
valid.data.store$year <- as.integer(valid.data.store$replace.date)

# merge title and tagline text into new movie text variable for text analysis
valid.data.store$text <- 
  paste(valid.data.store$title, valid.data.store$tagline)

# drop replace.date and reorder variables for text analysis
# at this point we have one large data frame with text columns
movies <- valid.data.store[,c("movie","year","title","tagline","text")]

# plot frequency of movies by year
pdf(file = "fig_text_movies_by_year_histogram.pdf", width = 11, height = 8.5)
ggplot.object <- ggplot(data = movies, aes(x = year)) +
  geom_histogram(binwidth = 1, fill = "blue", colour = "black") +
    labs(x = "Year of Release", 
         y = "Number of Movies in Database") +
         coord_fixed(ratio = 1/50) 

ggplot.print.with.margins(ggplot.object.name = ggplot.object,
  left.margin.pct=10, right.margin.pct=10,
  top.margin.pct=10,bottom.margin.pct=10) 
dev.off()  
    
# let us work with movies from 1974 to 2013
# creating an aggregate tagline_text collection for each year of interest
years.list <- 1974:2013
document.collection <- NULL  # initialize
for (index.for.year in seq(along=years.list)) {
 
  working.year.data.frame = 
    subset(movies, subset = (year == years.list[index.for.year]))

  tagline_text <- NULL
  for(index.for.movie in seq(along = working.year.data.frame$movie)) 
    tagline_text <- 
      paste(tagline_text, working.year.data.frame$tagline[index.for.movie])
   
  document <- PlainTextDocument(x = tagline_text, author = "Tom",
    description = paste("movie taglines for ",
    as.character(years.list[index.for.year]),sep = ""),
    id = paste("movies_",as.character(years.list[index.for.year]),sep=""), 
    heading = "taglines",
    origin = "IMDb", language = "en_US", 
    localmetadata = list(year = years.list[index.for.year])) 

# give each created document a unique name  
  if (years.list[index.for.year] == 1974) Y1974 <- document  
  if (years.list[index.for.year] == 1975) Y1975 <- document  
  if (years.list[index.for.year] == 1976) Y1976 <- document  
  if (years.list[index.for.year] == 1977) Y1977 <- document  
  if (years.list[index.for.year] == 1978) Y1978 <- document  
  if (years.list[index.for.year] == 1979) Y1979 <- document     
  if (years.list[index.for.year] == 1980) Y1980 <- document  
  if (years.list[index.for.year] == 1981) Y1981 <- document   
  if (years.list[index.for.year] == 1982) Y1982 <- document  
  if (years.list[index.for.year] == 1983) Y1983 <- document  
  if (years.list[index.for.year] == 1984) Y1984 <- document  
  if (years.list[index.for.year] == 1985) Y1985 <- document  
  if (years.list[index.for.year] == 1986) Y1986 <- document  
  if (years.list[index.for.year] == 1987) Y1987 <- document  
  if (years.list[index.for.year] == 1988) Y1988 <- document  
  if (years.list[index.for.year] == 1989) Y1989 <- document  
  if (years.list[index.for.year] == 1990) Y1990 <- document  
  if (years.list[index.for.year] == 1991) Y1991 <- document   
  if (years.list[index.for.year] == 1992) Y1992 <- document  
  if (years.list[index.for.year] == 1993) Y1993 <- document  
  if (years.list[index.for.year] == 1994) Y1994 <- document  
  if (years.list[index.for.year] == 1995) Y1995 <- document  
  if (years.list[index.for.year] == 1996) Y1996 <- document  
  if (years.list[index.for.year] == 1997) Y1997 <- document  
  if (years.list[index.for.year] == 1998) Y1998 <- document  
  if (years.list[index.for.year] == 1999) Y1999 <- document  
  if (years.list[index.for.year] == 2000) Y2000 <- document  
  if (years.list[index.for.year] == 2001) Y2001 <- document   
  if (years.list[index.for.year] == 2002) Y2002 <- document  
  if (years.list[index.for.year] == 2003) Y2003 <- document  
  if (years.list[index.for.year] == 2004) Y2004 <- document  
  if (years.list[index.for.year] == 2005) Y2005 <- document  
  if (years.list[index.for.year] == 2006) Y2006 <- document  
  if (years.list[index.for.year] == 2007) Y2007 <- document  
  if (years.list[index.for.year] == 2008) Y2008 <- document  
  if (years.list[index.for.year] == 2009) Y2009 <- document  
  if (years.list[index.for.year] == 2010) Y2010 <- document  
  if (years.list[index.for.year] == 2011) Y2011 <- document  
  if (years.list[index.for.year] == 2012) Y2012 <- document  
  if (years.list[index.for.year] == 2013) Y2013 <- document  
  } # end of for-loop for selected years
  
document.collection <- c(Y1974,Y1975,Y1976,Y1977,Y1978,Y1979,
  Y1980,Y1981,Y1982,Y1983,Y1984,Y1985,Y1986,Y1987,Y1988,Y1989,
  Y1990,Y1991,Y1992,Y1993,Y1994,Y1995,Y1996,Y1997,Y1998,Y1999,
  Y2000,Y2001,Y2002,Y2003,Y2004,Y2005,Y2006,
  Y2007,Y2008,Y2009,Y2010,Y2011,Y2012,Y2013)

# strip whitspace from the documents in the collection
document.collection <- tm_map(document.collection, stripWhitespace)

# convert uppercase to lowercase in the document collection
document.collection <- tm_map(document.collection, tolower)

# remove numbers from the document collection
document.collection <- tm_map(document.collection, removeNumbers)

# remove punctuation from the document collection
document.collection <- tm_map(document.collection, removePunctuation)

# using a standard list, remove English stopwords from the document collection
document.collection <- tm_map(document.collection, 
  removeWords, stopwords("english"))

# there is more we could do in terms of data preparation 
# stemming... looking for contractions... pronoun possessives... 

# we take what is clearly a "bag of words" approach here
# the workhorse technique will be TermDocumentMatrix()
# for creating a terms-by-documents matrix across the document collection
initial.movies.tdm <- TermDocumentMatrix(document.collection)

# remove sparse terms from the matrix and report the most common terms
# looking for additional stop words and stop word contractions to drop
examine.movies.tdm <- removeSparseTerms(initial.movies.tdm, sparse = 0.25)
top.words <- Terms(examine.movies.tdm)
print(top.words)  

# an analysis of this initial list of top terms shows a number of word 
# contractions which we might like to drop from further analysis, 
# recognizing them as stop words to be dropped from the document collection
more.stop.words <- c("cant","didnt","doesnt","dont","goes","isnt","hes",
  "shes","thats","theres","theyre","wont","youll","youre","youve") 
document.collection <- tm_map(document.collection, 
  removeWords, more.stop.words)
  
# create terms-by-documents matrix across the final document collection
movies.tdm <- TermDocumentMatrix(document.collection)

# save movie documents and document collection (corpus)
save("movies","document.collection","movies.tdm",
  file = "000_movies_data.Rdata")  

# remove sparse terms from the matrix and report the most common terms
examine.movies.tdm <- removeSparseTerms(movies.tdm, sparse = 0.25)
top.words <- Terms(examine.movies.tdm)
print(top.words)  # the result of this is a bag of 200 words

# now comes a test... 
# does looking at taglines hold promise as a way of identifying movie trends?
# if it does, then years closest in time should be closest to one
# another in a text measurement space as reflected, say, 
# by multidimensional scaling... 
# create a dictionary of the top words from the corpus
top.words.dictionary <- Dictionary(c(top.words))
  
# create term-by-document matrix using the mtpa.Dictionary
top.words.movies.tdm <- TermDocumentMatrix(document.collection, 
  list(dictionary = top.words.dictionary))

# dissimilarity measures and multidimensional scaling
# with wordlayout from the wordcloud package for non-overlapping labels
pdf(file = "fig_text_mds_1974_2013.pdf", width = 7, height = 7)
years.dissimilarity.matrix <- 
  dissimilarity(x = top.words.movies.tdm, y = NULL, method = "cosine")
years.mds.solution <- cmdscale(years.dissimilarity.matrix, k = 2, eig = TRUE)
x <- years.mds.solution$points[,1]
y <- years.mds.solution$points[,2]
w <- c("1974","1975","1976","1977","1978","1979",
  "1980","1981","1982","1983","1984","1985","1986",
  "1987","1988","1989","1990","1991","1992","1993",
  "1994","1995","1996","1997","1998","1999","2000",
  "2001","2002","2003","2004","2005","2006","2007",
  "2008","2009","2010","2011","2012","2013")
plot(x,y,type="n", xlim = c(-0.075,0.075), ylim = c(-0.075,0.075),
  xlab = "First Dimension", ylab = "Second Dimension") 
lay <- wordlayout(x, y, w, xlim = c(-0.075,0.075), ylim = c(-0.075,0.075)) 
text(lay[,1]+.5*lay[,3],lay[,2]+.5*lay[,4],w)
dev.off()

# classification of words into groups for further analysis
# use transpose of the terms-by-document matrix and cluster analysis
words.distance.object <- 
  dist(x = as.matrix(top.words.movies.tdm), method = "euclidean")

pdf(file = "fig_text_hcluster_top_words.pdf", width = 11, height = 8.5)
top.words.hierarchical.clustering <- 
  agnes(words.distance.object,diss=TRUE,
    metric = "euclidean", stand = FALSE, method = "ward") 
plot(top.words.hierarchical.clustering, cex.lab = 0.05)
dev.off()

# hierarchical solution suggests that four or five clusters may work
# examine possible clustering soltions with partitioning
number.of.clusters.test <- NULL
for(number.of.clusters in 2:20) {
  try.words.clustering <- pam(words.distance.object,diss=TRUE,
    metric = "euclidean", stand = FALSE, k = number.of.clusters) 
  number.of.clusters.test <- 
    rbind(number.of.clusters.test,
      data.frame(number.of.clusters, 
        ave.sil.width = try.words.clustering$silinfo$avg.width)) 
   cat("\n\n","Number of clusters: ",number.of.clusters,
     " Average silhouette width: ",try.words.clustering$silinfo$avg.width,
     "\nKey identified concepts: ",try.words.clustering$medoids,
     "\nCluster average silhouette widths: ")
   print(try.words.clustering$silinfo$clus.avg.widths)
  }  # end of for-loop for number-of-clusters test 
print(number.of.clusters.test)

# results suggest that five clusters may work best here 
# we examine these clusters and give them names corresponding to medoids
top.words.clustering <- pam(words.distance.object,diss=TRUE,
    metric = "euclidean", stand = FALSE, k = 5)

# review the clustering results
print(summary(top.words.clustering))

# the medoid identified through the clustering process
# is an object at the center of the cluster... 
# it is used to define the cluster here we identify their names
cat("\nKey Words Identified by Cluster Analysis: \n")
key.word.set <- top.words.clustering$medoids
print(key.word.set)

# convert the distance object to an actual distance matrix 
# for doing word searches directly on the matrix calculations
words.distance.matrix <- as.matrix(words.distance.object)

# for each medoid... identify the closest words from distance matrix
# let us choose the nine closest words to have five lists of ten words
# for further analysis... note that there is some overlap in word sets
for(index.for.key.word in seq(along=key.word.set)) {
  # identify the column for the key word
  key.word.column <- words.distance.matrix[,c(key.word.set[index.for.key.word])]
  # sort the key word column by distance
  sorted.key.word.column <- sort(key.word.column)
  # the smallest distance will be the distance of the key word to itself
  # so choose the second through tenth words in from the sorted column
  print(sorted.key.word.column[1:10])
  if (index.for.key.word == 1) 
    loved.word.set <- names(sorted.key.word.column[1:10])
  if (index.for.key.word == 2) 
    worlds.word.set <- names(sorted.key.word.column[1:10])
  if (index.for.key.word == 3) 
    truth.word.set <- names(sorted.key.word.column[1:10])
  if (index.for.key.word == 4) 
    life.word.set <- names(sorted.key.word.column[1:10])
  if (index.for.key.word == 5) 
    story.word.set <- names(sorted.key.word.column[1:10])
  }

# turn the word sets into dictionaries for analysis 
loved.words.dictionary <- Dictionary(c(loved.word.set))
worlds.words.dictionary <- Dictionary(c(worlds.word.set))
truth.words.dictionary <- Dictionary(c(truth.word.set))
life.words.dictionary <- Dictionary(c(life.word.set))
story.words.dictionary <- Dictionary(c(story.word.set))

# do word counts across the dictionaries
year <- 1974:2013
total.words <- integer(length(year))
loved.words <- integer(length(year))
worlds.words <- integer(length(year))
truth.words <- integer(length(year))
life.words <- integer(length(year))
story.words <- integer(length(year))

for(index.for.document in seq(along=year)) {
  loved.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = loved.words.dictionary)))
    
  worlds.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = worlds.words.dictionary)))  
    
  truth.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = truth.words.dictionary))) 
    
  life.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = life.words.dictionary)))     
    
  story.words[index.for.document] <- 
    sum(termFreq(document.collection[[index.for.document]], 
    control = list(dictionary = story.words.dictionary)))     
    
  total.words[index.for.document] <- length(movies.tdm[,index.for.document][["i"]])
  }

# gather the results up in a data frame
movie.analytics.data.frame <- data.frame(year, total.words,
  loved.words, worlds.words, truth.words, life.words, story.words) 

# compute text measures as percentages of words in each set

movie.analytics.data.frame$LOVED <- 
  100 * movie.analytics.data.frame$loved.words / movie.analytics.data.frame$total.words
LOVED <- ts(standardize(movie.analytics.data.frame$LOVED), 
  start = c(1974,1),
  end = c(2013,1),
  frequency = 1)
  
movie.analytics.data.frame$WORLDS <- 
  100 * movie.analytics.data.frame$worlds.words / 
    movie.analytics.data.frame$total.words
WORLDS <- ts(standardize(movie.analytics.data.frame$WORLDS), 
  start = c(1974,1),
  end = c(2013,1),
  frequency = 1)
  
movie.analytics.data.frame$TRUTH <- 
  100 * movie.analytics.data.frame$truth.words / 
    movie.analytics.data.frame$total.words
TRUTH <- ts(standardize(movie.analytics.data.frame$TRUTH), 
  start = c(1974,1),
  end = c(2013,1),
  frequency = 1)
  
movie.analytics.data.frame$LIFE <- 
  100 * movie.analytics.data.frame$life.words / 
    movie.analytics.data.frame$total.words
LIFE <- ts(standardize(movie.analytics.data.frame$LIFE), 
  start = c(1974,1),
  end = c(2013,1),
  frequency = 1)
  
movie.analytics.data.frame$STORY <- 
  100 * movie.analytics.data.frame$story.words / 
    movie.analytics.data.frame$total.words
STORY <- ts(standardize(movie.analytics.data.frame$STORY), 
  start = c(1974,1),
  end = c(2013,1),
  frequency = 1)

# multiple time series object for text measures
movie.mts <- cbind(LOVED, WORLDS, TRUTH, LIFE, STORY) 

# text horizons for forty years of movies
pdf(file = "fig_text_horizon_1974_2013.pdf", width = 8.5, height = 11)
print(horizonplot(movie.mts, colorkey = TRUE,
  layout = c(1,5), strip.left = FALSE, horizonscale = 1,
  origin = 0,
  ylab = list(rev(colnames(movie.mts)), rot = 0, cex = 0.7)) +
  layer_(panel.fill(col = "gray90"), panel.xblocks(..., col = "white")))
dev.off()
  
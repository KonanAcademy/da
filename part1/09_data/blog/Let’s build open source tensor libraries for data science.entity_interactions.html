<html>
    <head>
        <title>Let’s build open source tensor libraries for data science Interactions</title>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
    </head>
    <body>
<strong>Data</strong> <strong>scientists</strong> frequently find themselves dealing with high-dimensional <strong>feature</strong> spaces. As an <strong>example</strong>, <strong>text mining</strong> usually involves <strong>vocabularies</strong> comprised of 10,000+ different words. Many analytic <strong>problems</strong> involve linear <strong>algebra</strong>, particularly 2D <strong>matrix factorization</strong> <strong>techniques</strong>, for which several open <strong>source</strong> implementations are available. <strong>Anyone</strong> working on implementing <strong>machine</strong> learning <strong>algorithms ends</strong> up needing a good <strong>library</strong> for <strong>matrix analysis</strong> and operations. But why stop at 2D representations? In a recent <strong>Strata + Hadoop World San Jose</strong> <strong>presentation</strong>, <strong>UC Irvine</strong> <strong>professor</strong> <strong>Anima Anandkumar</strong> described how <strong>techniques</strong> developed for higher-dimensional <strong>arrays</strong> can be applied to machine learning. <strong>Tensors</strong> are <strong>generalizations</strong> of <strong>matrices</strong> that let you look beyond <strong>pairwise</strong> <strong>relationships</strong> to higher-dimensional <strong>models</strong> (a <strong>matrix</strong> is a second-order tensor). For <strong>instance</strong>, one can examine <strong>patterns</strong> between any three (or more) <strong>dimensions</strong> in data sets. In a <strong>text mining application</strong>, this <strong>leads</strong> to <strong>models</strong> that incorporate the <strong>co-occurrence</strong> of three or more <strong>words</strong>, and in social <strong>networks</strong>, you can use <strong>tensors</strong> to encode arbitrary <strong>degrees</strong> of <strong>influence</strong> (<strong>e.g.</strong>, “<strong>friend</strong> of <strong>friend</strong> of <strong>friend</strong>” of a user). Being able to capture higher-order relationships proves to be quite useful. In her <strong>talk</strong>, <strong>Anandkumar</strong> described <strong>applications</strong> to latent variable <strong><strong>models</strong></strong> — including <strong>text</strong> mining <strong>(</strong>topic <strong><strong>models</strong></strong>), <strong>information science</strong> <strong>(</strong>social <strong>network analysis</strong>), <strong>recommender</strong> <strong>systems</strong>, and deep neural networks. A natural <strong>entry point</strong> for <strong>applications</strong> is to look at <strong>generalizations</strong> of <strong>matrix</strong> (2D) <strong>techniques</strong> to higher-dimensional arrays.more For <strong>example</strong>, the <strong>image</strong> that follows is an <strong>attempt</strong> to illustrate one <strong>form</strong> of <strong>eigen <strong>decomposition</strong></strong>:
[<strong>caption</strong> id="<strong>attachment_74836</strong>" <strong>align=</strong>"<strong>aligncenter</strong>" <strong>width=</strong>"570"] <strong>Spectral</strong> <strong>decomposition</strong> of tensors. <strong>Source</strong>: <strong>Anima Anandkumar</strong>, used with permission. [/caption]
<strong>Tensor</strong> <strong><strong>methods</strong></strong> are accurate and embarrassingly parallel
<strong>Latent</strong> variable <strong>models</strong> and deep neural <strong>networks</strong> can be solved using other <strong><strong>methods</strong></strong>, including <strong>maximum likelihood</strong> and local <strong>search</strong> <strong>techniques</strong> (<strong>gradient descent</strong>, variational <strong>inference</strong>, EM). So, why use tensors at all? Unlike variational <strong>inference</strong> and <strong>EM</strong>, <strong>tensor</strong> <strong>methods</strong> produce global and not local <strong>optima</strong>, under reasonable conditions. In her <strong>talk</strong>, <strong>Anandkumar</strong> described some recent <strong>examples</strong> — topic <strong>models</strong> and social <strong>network analysis</strong> — where <strong>tensor</strong> <strong>methods</strong> proved to be faster and more accurate than other <strong>methods</strong>. [caption id="<strong>attachment_74838</strong>" <strong>align=</strong>"<strong>aligncenter</strong>" <strong>width=</strong>"570"] <strong>Error</strong> <strong>rates</strong> & <strong>recovery</strong> <strong>ratios</strong> from recent <strong>community detection</strong> <strong>experiments</strong>  (running <strong>time</strong> measured in seconds). <strong>Source</strong>: <strong>Anima Anandkumar</strong>, used with permission. [/caption]
<strong>Scalability</strong> is another important <strong>reason</strong> why <strong>tensors</strong> are generating interest. <strong>Tensor</strong> <strong>decomposition</strong> <strong>algorithms</strong> have been parallelized using <strong>GPUs</strong>, and more recently using <strong>Apache REEF (</strong>a distributed <strong>framework</strong> originally developed by Microsoft). To summarize, early <strong>results</strong> are promising (in <strong>terms</strong> of <strong>speed</strong> and <strong>accuracy</strong>), and <strong>implementations</strong> in distributed <strong>systems</strong> lead to <strong>algorithms</strong> that <strong>scale</strong> to extremely large data sets. [caption id="<strong>attachment_74837</strong>" <strong>align=</strong>"<strong>aligncenter</strong>" <strong>width=</strong>"570"] <strong>General</strong> framework. <strong>Source</strong>: <strong>Anima Anandkumar</strong>, used with permission. [/caption]
<strong>Hierarchical</strong> <strong>decomposition</strong> <strong>models</strong>
<strong>Their</strong> <strong>ability</strong> to model multi-way <strong>relationships</strong> make <strong>tensor</strong> <strong>methods</strong> particularly useful for <strong>uncovering</strong> hierarchical <strong>structures</strong> in high-dimensional data sets. In a recent <strong>paper</strong>, <strong>Anandkumar</strong> and her <strong>collaborators</strong> automatically found <strong>patterns</strong> and “...<strong>concepts</strong> reflecting <strong>co-occurrences</strong> of particular <strong>diagnoses</strong> in <strong>patients</strong> in <strong>outpatient</strong> and intensive <strong>care settings.”</strong>
Why aren’t tensors more popular? If they’re <strong>faster</strong>, more accurate, and embarrassingly parallel, why haven’t <strong>tensor</strong> methods become more common? It comes down to libraries. Just as matrix <strong><strong>libraries</strong></strong> are needed to implement many <strong>machine</strong> learning <strong>algorithms</strong>, open <strong>source</strong> <strong><strong>libraries</strong></strong> for tensor analysis need to become more common. While it’s <strong>true</strong> that <strong><strong>tensor</strong></strong> <strong>computations</strong> are more demanding than matrix <strong>algorithms</strong>, recent <strong>improvements</strong> in <strong>parallel</strong> and distributed computing <strong>systems</strong> have made <strong><strong>tensor</strong></strong> techniques feasible. There are some early <strong>libraries</strong> for <strong>tensor analysis</strong> in <strong>MATLAB</strong>, <strong>Python</strong>, <strong>TH++</strong> from <strong>Facebook</strong>, and many <strong>others</strong> from the scientific computing community. For <strong>applications</strong> to machine <strong>learning</strong>, <strong>software</strong> <strong>tools</strong> that include <strong>tensor decomposition</strong> methods are essential. As a first <strong>step</strong>, <strong>Anandkumar</strong> and her <strong>UC Irvine</strong> colleagues have released <strong>code</strong> for <strong>tensor</strong> <strong>methods</strong> for topic <strong>modeling</strong> and social <strong>network</strong> <strong>modeling</strong> that <strong>run</strong> on single servers. But for <strong>data scientists</strong> to embrace these <strong>techniques</strong>, we’ll <strong>need</strong> well-developed <strong>libraries</strong> accessible from the <strong>languages</strong> (<strong>Python</strong>, <strong>R</strong>, <strong>Java</strong>, Scala) and frameworks (Apache Spark) we’re already already familiar with. <strong>(</strong>Coincidentally, <strong>Spark</strong> <strong>developers</strong> just recently introduced distributed matrices.) <strong>It’s</strong> fun to see a <strong>tool</strong> that I first encountered in <strong>math</strong> and <strong>physics courses</strong> having an <strong>impact</strong> in machine learning. But the primary <strong>reason I’m</strong> writing this <strong>post</strong> is to get <strong>readers</strong> excited enough to build open <strong>source tensor</strong> (<strong>decomposition</strong>) libraries. Once these basic <strong>libraries</strong> are in <strong>place</strong>, tensor-based algorithms become easier to implement. <strong>Anandkumar</strong> and her <strong>collaborators</strong> are in the early <strong>stages</strong> of porting some of their <strong>code</strong> to <strong>Apache Spark</strong>, and I’m <strong>hoping</strong> other <strong>groups</strong> will jump into the fray. To view <strong>Anima Anandkumar’s</strong> <strong>talk</strong> at <strong>Strata + Hadoop World</strong> in <strong>San Jose</strong>, <strong>Tensor Methods</strong> for Large-scale <strong>Unsupervised Learning</strong>: <strong>Applications</strong> to <strong>Topic</strong> and <strong>Community Modeling</strong>, <strong>sign-up</strong> for a free <strong>trial</strong> of Safari Books Online. <strong>Cropped</strong> <strong>image</strong> on <strong>article</strong> and <strong>category</strong> <strong>pages</strong> by <strong>Gerwin Sturm</strong> on <strong>Flickr</strong>, used under a <strong>Creative Commons</strong> license.

    
</body>
</html>
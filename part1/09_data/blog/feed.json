[
 {
  "content": "\nOver the past five years, international agencies such as the World Bank, OECD, and UNESCO have created portals to make their data available for everyone to explore. Many non-profits are also visualizing masses of data in the hope that it will give policymakers, funders, and the general public a better understanding of the issues they are trying to solve.\nData visualization plays a key role in telling the stories behind the data. For most audiences, data sets are hard to use and interpret \u2014 the average user will need a technical guide just to navigate through the complicated hierarchies of categories let alone interpret the information. But data visualizations trigger interest and insight because they are immediate, clear, and tangible.\nAt FFunction, we visualize a lot of data. Most of the time our clients send us Excel spreadsheets or CSV files, so we were happily surprised when we started to work with UNESCO Institute for Statistics on two fascinating education-related projects \u2014 Out-of-School Children and Left Behind \u2014 and realized that they had been working on a data API. As we began to work through the data ourselves, we uncovered several reasons why using an API helps immeasurably with data visualization.more\nWhy APIs are the best way to share data\nIf you look at the data available in most data portals, you\u2019re likely to find XML, CSV files, Excel spreadsheets (for those who haven\u2019t heard of Open Document format), or PDF documents (god forbid!). Although many organizations do have an API, more should.\nIf you\u2019ve worked a little bit with data, whether internal corporate data or open data, you\u2019ll probably know from experience that data very rarely comes in an ideal format. Very often, you\u2019ll find your data contains a multitude of formats (dates are a common offender there, where you\u2019ll find YYYY-MM-DD along with DD/MM/YYY and, for good measure, MM/DD/YYYY).\nAs a result, you almost always need to normalize your data to ensure that all the fields are in the same format. Normalization is not the only chore, though. If you want to keep your data up to date, you\u2019ll also need to make sure that your new data format hasn\u2019t changed since the last time (fields shuffled, added, removed, etc.), which means you\u2019ll need to create a program to validate your new data. And this necessitates having an automated way to retrieve the data. Once you finally have valid, normalized data, you can eventually process the data into a format that is suitable for a visualization.\n\nPipeline to prepare data for visualization\nHowever, when you are working with an API, you can skip the normalization and validation phases and go directly to processing. Here is why:\n\nAPI data is URL accessible: instead of preparing and sending a data file, you just send the URL. Getting the most up-to-date data is a breeze.\nAPI data is already normalized: because it is machine generated, it follows a format that is usually documented. For instance, here is the documentation for the U.S. Consumer Complaints Database.\nAPI data is already validated: because it is extracted from an underlying, structured, information system, you eliminate the risk of inconsistent information. The content is already structured (as opposed to a tabular format) because APIs output either XML or JSON.\n\nWhy using an API improves visualizations\nThe two projects we recently worked on with UNESCO were complex. Let\u2019s take Out-of-School Children as a case study: our brief was to create an interactive visualization that allows the user to explore UNESCO data on children who are out of school in different regions around the world. The main idea of the visualization was to show the proportion of children in school (compared to the children out of school) among different sub-groups of a country\u2019s population (e.g. girls versus boys, children living in rural areas versus urban zones, etc.). You can check out the end result here.\nThrough experience, we have found that one of the key factors in creating a successful visualization is how quickly you can \u201cget\u201d the data. And by \u201cget,\u201d I mean both getting access to the data files and being able to understand the data\u2019s structure, dimensions, and meaning.\nHaving an API to query made it incredibly easy for us to quickly dive into the material. UNESCO\u2019s online data center, UIS.Stats, allows you to browse the data to get an overview of the numbers. Most importantly for our purposes, the data are also available through an API. With a JSON \u201cprettyfier\u201d plug-in, the data look like this:\n[caption id=\"attachment_74920\" align=\"aligncenter\" width=\"570\"] Gender parity index, UNESCO UIS.[/caption]\nThis might not look very clear if you're not familiar with JSON, but compared to the raw data, the structure is relatively clear, so you can quickly create tools to assess and visualize the data. For the UNESCO Out of School project, one of the first things to assess was the number of countries with complete (100%) coverage for the key indicators versus the number with missing data.\nHaving instant access to the data and being able to feed it live to the tools really made a big difference in our ability to explore the material and quickly start prototyping.\nData APIs allow us to focus on storytelling\nBy live-querying the API, our interactive visualization was able to immediately incorporate changes to the data as we went. This meant we had the freedom to create unique, specific design elements to differentiate the data and heighten the impact of the visualization.\nFor instance, in Out of School Children, you can see how many children living in rural locations go to school versus children living in urban areas, how boys\u2019 access to education compares to girls\u2019, or if the richest and poorest of children have equal chances for education.\n\nIn the example above, we\u2019re looking at Nigeria\u2019s rural versus urban children, divided into out-of-school and in-school groups. The approximate numbers of children in each group are displayed at the top (19.5 million rural versus 8.3 million urban), and the corresponding rates appear in the lower section of the product (e.g. 37% of children living in rural areas in Nigeria are out of school compared to 13% of urban children).\nNow, what\u2019s interesting is that each little animated character that you see on the page represents the same number of individuals. In the example above, there are around two rural children for each urban child, so you have twice as many little characters on the left side (rural) as the right side (urban).\nIf you look closely, you\u2019ll also notice that the characters are dressed differently depending on their group and gender (pictured below). Indeed, we\u2019ve created many different characters to give a feeling of variety in the visualization \u2014 and also to stress that these numbers represent actual people.\n\nThe client perspective\nBy now, you probably have a good understanding of the benefits of an API accruing to the developer. But the client also benefits considerably in terms of investment, reputation, and outreach.\nUNESCO\u2019s education database is updated three times each year. \u201cWith the API, our data visualizations are being automatically updated with the latest available data,\u201d explains Amy Otchet, head of communications at the UNESCO Institute for Statistics. \u201cThis means that our data visualizations are basically maintenance free, and we avoid the limitations of a shelf-life. This results in clear gains in terms of human and financial resources.\u201d\n\u201cThe API also helps to protect our reputation and credibility,\u201d Otchet says. \u201cAs the official source of international education data, we have to be certain that our audiences get the latest data available. This also helps with outreach. Partners can directly embed our products as permanent resources without any concerns about maintenance in the long term.\u201d\nTips to successfully create an API for data publication\nIf you decide to make your data accessible through an API, here are some key points to help you succeed:\n\nTechnology (platform) does not really matter too much. This is the beauty of the Web: all that matters is that you output your data in an open format that is developer friendly (i.e. JSON, XML or CSV).\nJSON is more readable than XML, which is easier to parse than CSV. This might sound weird, as XML is actually more a document format than a data format, but in practice, XML is very verbose and does a poor job at representing the structure, especially when you have numeric data types. JSON is the better choice, provided you have plug-ins to doll it up.\nAPIs can have different formats (if you feel the need), but JSONP is probably the most important. The main reason is that it allows for cross-domain query. For instance, our Out of School interactive is pulling live data from UNESCO\u2019s API, which is only possible because the API offers a JSONP format (P=Procedure call).\nBe mindful of performance. In the case of the Out of School visualization, the UNESCO data API is queried live by the browser, which means that the API needs to support a potentially high number of queries. A side effect of that is that the visualization\u2019s load time is directly dependent on the speed of the API.\n\nIf you want more information on how to turn an ordinary API into a great API, this slideshare from OSCON 2012 outlines five \"keys\" that might also prove useful.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/MEFV_ucIE7g/data-apis-design-and-visual-storytelling.html", 
  "title": "Data APIs, design, and visual storytelling"
 }, 
 {
  "content": "\nSwarmfarm Robotics -- His previous weed sprayer weighed 21 tonnes, measured 36 metres across its spray unit, guzzled diesel by the bucketload and needed a paid driver who would only work limited hours. Two robots working together on Bendee effortlessly sprayed weeds in a 70ha mung-bean crop last month. Their infra-red beams picked up any small weeds among the crop rows and sent a message to the nozzle to eject a small chemical spray. Bate hopes to soon use microwave or laser technology to kill the weeds. Best of all, the robots do the work without guidance. They work 24 hours a day. They have in-built navigation and obstacle detection, making them robust and able to decide if an area of a paddock should not be traversed. Special swarming technology means the robots can detect each other and know which part of the paddock has already been assessed and sprayed.\nRoute to Market (Matt Webb) -- The route to market is not what makes the product good. [...] So the way you design the product to best take it to market is not the same process to make it great for its users.\nExplorable Explanations -- points to many sweet examples of interactive explorable simulations/explanations.\nI-JSON (Tim Bray) -- I-JSON is just a note saying that if you construct a chunk of JSON and avoid the interop failures described in RFC 7159, you can call it an \u201cI-JSON Message.\u201d If any known JSON implementation creates an I-JSON message and sends it to any other known JSON implementation, the chance of software surprises is vanishingly small.\n\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/l8ntnyRQKs0/four-short-links-23-march-2015.html", 
  "title": "Four short links: 23 March 2015"
 }, 
 {
  "content": "Each week our design editors curate the most notable, interesting, and important material they come across. Below you\u2019ll find their recent selections. You can get these and more in our weekly design newsletter.\nThe evolution of UX\nFrom da Vinci to Dreyfuss to Disney to Donald Norman, interactions between humans and technology have marked each key milestone in the longer-than-you-think history of user experience design. Here's how UX design's past sheds light on its future.\nSource: Image by U.S. Army on Flickr\nmore\n\nQuote of the week\n\"If you can't toot your own horn, don't expect the band to show up.\" \u2014 @BreneBrown\n\n13 ways to blow it\nIn this rousing, rambunctious rant, Mike Monteiro of Mule Design tears down the myth that good design sells itself. Part pep talk, part scolding session, his IXDA keynote asserts that designers need sales skills and unveils the 13 biggest mistakes designers make during presentations.\nhttps://vimeo.com/121082134\n\nStay in the experience design loop by signing up for our free weekly newsletter.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/mjkg1g86THk/experience-design-links-and-fodder-march-20-2015.html", 
  "title": "Experience Design Links and Fodder: March 20, 2015"
 }, 
 {
  "content": "\nIn this week's Radar Podcast episode, Jon Follett, editor of Designing for Emerging Technologies, chats with Matt Nish-Lapidus, partner and design director at Normative. Their discussion circles around the evolution of design, characteristics of post-Industrial design, and aesthetic intricacies of designing in networked systems. Also note, Nish-Lapidus will present a free webcast on these topics March 24, 2015.\nPost-Industrial design relationships\nNish-Lapidus shares an interesting take on design evolution, from pre-Industrial to post-Industrial times, through the lens of eyeglasses. He uses eyeglasses as a case study, he says, because they're a piece of technology that's been used through a broad span of history, longer than many of the things we still use today. Nish-Lapidus walks us through the pre-Industrial era \u2014 so, Medieval times through about the 1800s \u2014 where a single craftsperson designed one product for a single individual; through the Industrial era, where mass-production took the main stage; to our modern post-Industrial era, where embedded personalization capabilities are bringing design almost full circle, back to a focus on the individual user:\n\"Once we move into this post-Industrial era, which we're kind of entering now, the relationship's starting to shift again, and glasses are a really interesting example. We go from having a single pair of glasses made for a single person, hand-made usually, to a pair of glasses designed and then mass-manufactured for a countless number of people, to having a pair of glasses that expresses a lot of different things. On one hand, you have something like Google Glass, which is still mass-produced, but the glasses actually contain embedded functionality. Then we also have, with the emergence of 3D printing and small-scale manufacturing, a return to a little bit of that artisan, one-to-one relationship, where you could get something that someone's made just for you.\n\"These post-Industrial objects are more of an expression of the networked world in which we now live. We [again] have a way of building relationships with individual crafts-people. We also have objects that exist in the network themselves, as a physical instantiation of the networked environment that we live in.\"\n more\n\nSubscribe to the O'Reilly Radar Podcast\nTuneIn, iTunes, SoundCloud, RSS\n\n\nBeyond the object, back to the user\nThis movement toward personalization and personalized services is affecting post-Industrial design in a number of ways. Nish-Lapidus talks about this growing complexity and how it affects design considerations:\n\"There's a lot of new kinds of complexity that you have to think about when producing something, even if that thing is just a physical object with no inherent functionality or technology built into it. Means of production have changed quite a bit, so the options are much more vast than they used to be, but the scales are very different. You have the ability to kind of micro-produce things, but also to produce single one-off things, so the designer in this case has to go back to thinking about the individuals or small subsets of people that the product serves.\n\"Then when we get into embedded technology and objects that actually are part of the network infrastructure, it's not just about the physical object anymore. The object becomes a container for software, and that software can do things that are invisible and that affect all sorts of other aspects of the environment and [the user's] relationship with the objects and with the other people that that object puts them in touch with.\"\nHow it feels to be inside of a complex system\nNish-Lapidus also outlines a set of aesthetic qualities of networked systems for designers to use not only as foundational elements for designing objects within a network, but to better describe and understand the network itself. Using \"texture\" as an example, he explains:\n\"This whole idea comes from a tradition of design foundations, which has been established for quite a while and is much more commonplace in industrial design and graphic design and architecture, where there are physical properties of objects that you have to understand in order to be able to make something with them. When we get into designing software and networked things, where the physical limitations are a little different, and the material properties become much more abstract, we haven't yet done a really great job of thinking about the aesthetics of those types of material properties, those invisible properties. What's interesting about invisible properties is that they quickly become visible when you're trying to interact with the system at hand.\n\"Taking texture as the first one, for example, texture is looking at the way the system connects to all of the other pieces that exist within it, or how those pieces interact with each other. When you have a system that has multiple nodes, and those nodes could be human or inhuman or physical objects or pieces of software, how does the communication between those happen? ... When you start to talk about that in terms of a texture of connection, you can look at how obvious the connections are. If they're really obvious and easy, and you can access them, and you understand what you're supposed to put in them or what you're going to get out of them, you could talk about that as a 'smooth' form of communication. If it's really confusing, or you get something unexpected, or there's interference from other aspects of the system, you could start talking about that as rough or as other types of descriptors. This gives us a kind of a fuzzy aesthetic language to start talking about how it feels to be inside of a complex system.\"\nYou can listen to the podcast in the player embedded above or download it through TuneIn, SoundCloud, or iTunes.\nCropped image on article and category pages by Natesh Ramasamy on Flickr, used under a Creative Commons license.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vd-b56E0ijk/designs-return-to-artisan-at-scale.html", 
  "title": "Design&#8217;s return to artisan, at scale"
 }, 
 {
  "content": "Editor\u2019s note: This is an excerpt by Claire Rowland from our upcoming book Designing Connected Products. This excerpt is included in our curated collection of chapters from the O\u2019Reilly Design library. Download a free copy of the Designing for the Internet of Things ebook here.\nIn systems where functionality and interactions are distributed across more than one device, it\u2019s not enough to design individual UIs in isolation. Designers need to create a coherent UX across all the devices with which the user interacts. That means thinking about how UIs work together to create a coherent understanding of the overall system, and how the user may move between using different devices.\nCross-platform UX and usability\n[caption id=\"attachment_74387\" align=\"alignright\" width=\"237\"] Download the free ebook.[/caption]Many of the tools of UX design and HCI originate from a time when an interaction was usually a single user using a single device. This was almost always a desktop computer, which they\u2019d be using to complete a work-like task, giving it more or less their full attention.\nThe reality of our digital lives moved on from this long ago. Many of us own multiple Internet-capable devices, such as smartphones, tablets, and connected TVs, used for leisure as well as for work. They have different form factors; may be used in different contexts; and some of them come with specific sensing capabilities, such as mobile location.\nCross-platform UX is an area of huge interest to the practitioner community. But academic researchers have given little attention to defining the properties of good cross-platform UX. This has left a gap between practice and theory that needs addressing.\nIn industry practice, cross-platform UX has often proceeded device by device. Designers begin with a key reference device and subsequent interfaces are treated as adaptations. In the early days of smartphones, this reference device was often the desktop. In recent years, the \"mobile first\" approach has encouraged us to start with mobile Web or apps as a way to focus on optimizing key functionality and minimize \"feature-itis.\" Such services usually have overarching design guidelines spanning all platforms to ensure a degree of consistency. The aim is usually on making the different interfaces feel like a family, rather than on making the devices work together as a system.more\nThis works when each device is delivering broadly the same functionality. Evernote, eBay, and Dropbox are typical examples: each offers more or less the same features via a responsive website and smartphone apps. The design is optimized for each device, but provides the same basic service functionality (bar a few admin functions that might only be available on the desktop).\nBut this approach breaks down when the system involves very diverse devices with different capabilities working in concert. In IoT, many devices do not even have screens, or an on-device user interface. Multiple devices might have UIs with very different forms or specialized functionality. Even if the UI is only on one device, the service still depends on all the devices working together in concert.\nIt\u2019s not possible to design a system like this by thinking about one device at a time: this is likely to create a disjointed experience.\nIn order to use it effectively, the user has to form a coherent mental image of the overall system. This includes its various parts, what each does, and how different objectives can be achieved using the system as a whole. Traditional single-device usability doesn\u2019t tell us very much about how to do this.\nWhat is interusability?\nCharles Denis and Laurent Karsenty first coined the term \"inter-usability\" in 2004 to describe UX across multiple devices. Conventional usability theory is under-equipped to cope with cross-platform design. However, one 2010 paper (PDF) by Minna W\u00e4ljas, Katarina Segerst\u00e5hl, Kaisa V\u00e4\u00e4n\u00e4nen-Vainio-Mattila, and Harri Oinas-Kukkonen proposes a practical model of interusability.\nW\u00e4ljas, et al, propose that the ultimate goal of cross-platform design is that the experience should feel coherent. Does the service feel like the devices are working in concert, or does the UX feel fragmented?\nThey define three key concepts for cross-platform service UX, which together ensure a coherent experience:\n\nComposition: how devices and functionality are organized\nAppropriate consistency of interfaces across different devices\nContinuity of content and data to ensure smooth transitions between platforms\n\nThe paper was published in 2010, and the services evaluated (including Nike+ and Nokia Sportstracker) now inevitably feel a little dated. But we have found the model still holds up well in our own work designing IoT services, and it\u2019s a key reference for the rest of this chapter.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/VHK26SM70Y0/cross-device-interactions-and-interusability.html", 
  "title": "Cross-device interactions and interusability"
 }, 
 {
  "content": "\nDavid Recordon Joins US Govt -- This afternoon, President Barack Obama will announce a newly created position for David Recordon, who has worked as one of Facebook\u2019s engineering directors since 2009. Recordon will join the White House as the director of information technology.  Obama building an A team from Foo Campers.\nMagicLeap/Weta Workshop FPS in AR (YouTube) -- fun!\nTheoretical Computer Science Cheat Sheet (PDF) -- how to appear smart.\nCarbon3D -- Traditional 3D printing requires a number of mechanical steps, repeated over and over again in a layer-by-layer approach. CLIP is a chemical process that carefully balances light and oxygen to eliminate the mechanical steps and the layers. It works by projecting light through an oxygen-permeable window into a reservoir of UV curable resin. The build platform lifts continuously as the object is grown.\n\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/Eqq84mx4sPY/four-short-links-20-march-2015.html", 
  "title": "Four short links: 20 March 2015"
 }, 
 {
  "content": "Adrian Gropper co-authored this post.\nAfter a short period of excitement and rosy prospects in the movement we\u2019ve come to call the Internet of Things (IoT), designers are coming to realize that it will survive or implode around the twin issues of security and user control: a few electrical failures could scare people away for decades, while a nagging sense that someone is exploiting our data without our consent could sour our enthusiasm. Early indicators already point to a heightened level of scrutiny \u2014 Senator Ed Markey\u2019s office, for example, recently put the automobile industry under the microscope for computer and network security.\nIn this context, what can the IoT draw from well-established technologies in federated trust? Federated trust in technologies as diverse as the Kerberos and SAML has allowed large groups of users to collaborate securely, never having to share passwords with people they don\u2019t trust. OpenID was probably the first truly mass-market application of federated trust.\nOpenID and OAuth, which have proven their value on the Web, have an equally vital role in the exchange of data in health care. This task \u2014 often cast as the interoperability of electronic health records \u2014 can reasonably be described as the primary challenge facing the health care industry today, at least in the IT space. Reformers across the health care industry (and even Congress) have pressured the federal government to make data exchange the top priority, and the Office of the National Coordinator for Health Information Technology has declared it the centerpiece of upcoming regulations.more\nFurthermore, other industries can learn from health care. The Internet of Things deals not only with distributed data, but with distributed responsibility for maintaining the quality of that data and authorizing the sharing of data. The use case we'll discuss in this article, where an individual allows her medical device data to be shared with a provider, can show a way forward for many other industries. For instance, it can steer a path toward better security and user control for the auto industry.\nHealth care, like other vertical industries, does best by exploiting general technologies that cross industries. When it depends on localized solutions designed for a single industry, the results usually cost a lot more, lock the users into proprietary vendors, and suffer from lower quality. In pursuit of a standard solution, a working group of the OpenID Foundation called Health Relationship Trust (HEART) is putting together a set of technologies that would:\n\nKeep patient control over data and allow her to determine precisely which providers have access.\nCut out middlemen, such as expensive health information exchanges that have trouble identifying patients and keeping information up to date.\nAvoid the need for a patient and provider to share secrets. Each maintains their credentials with their own trusted service, and connect with each other without having to reveal passwords.\nAllow data transfers directly (or through a patient-controlled proxy app) from fitness or medical devices to the provider\u2019s electronic record, as specified by the patient.\n\nStandard technologies from OpenID used by HEART include OAuth, OpenID Connect, and User-Managed Access (UMA).\nA sophisticated use case developed by the HEART team describes two health care providers that are geographically remote from each other and do not know each other. The patient gets her routine care from one but needs treatment from the other during a trip. OAuth and OpenID Connect work here the way they do on countless popular websites: they extend the trust that a user invested in one site to cover another site with which the user wants to do business. The user has a password or credential with just a single trusted site; dedicated tokens (sometimes temporary) grant limited access to other sites.\nDevices can also support OAuth and related technologies. The HEART use case suggests two hypothetical devices, one a consumer product and the other a more expensive, dedicated medical device. These become key links between the patient and her physicians. The patient can authorize the device to send her vital signs independently to the physician of her choice.\nOpenID Connect can relieve the patient of the need to enter a password every time she wants access to her records. For instance, the patient might want to use her cell phone to verify her identity. This is sometimes called multisig technology and is designed to avoid a catastrophic loss of control over data and avoid a single point of failure.\nOne could think of identity federation via OpenID Connect as promoting cybersecurity.\nUMA extends the possibilities for secure data sharing. It can allow a single authorization server to control access to data on many resource servers. UMA can also enforce any policy set up by the authorization server on behalf of the patient. If the patient wants to release surgical records without releasing mental health records, or wants records released only during business hours as a security measure, UMA enables the authorization server to design arbitrarily defined rules to support such practices. One could think of identity federation via OpenID Connect as promoting cybersecurity by replacing many weak passwords with one strong credential. On top of that, UMA promotes privacy by replacing many consent portals with one patient-selected authorization agent.\nFor instance, the patient can tell her devices to release data in the future without requiring another request to the patient, and can specify what data is available to each provider, and even when it's available \u2014 if the patient is traveling, for example, and needs to see a doctor, she can tell the authentication server to shut off access to her data by that doctor on the day after she takes her flight back home. The patient could also require that anyone viewing her data submit credentials that demonstrate they have a certain medical degree.\nThus, low-cost services already in widespread use can cut the Gordian knot of information siloing in health care. There\u2019s no duplication of data, either \u2014 the patient maintains it in her records, and the provider has access to the data released to them by the patient. Gropper, who initiated work on the HEART use case cited earlier, calls this \u201can HIE of One.\u201d Federated authentication and authorization, with provision for direct user control over data sharing, provides the best security we currently know without the need to compromise private keys or share secrets, such as passwords.\nImage on article and category pages by erik forsberg on Flickr, used under a Creative Commons license.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/v_qwgp8i_-g/what-the-iot-can-learn-from-the-health-care-industry.html", 
  "title": "What the IoT can learn from the health care industry"
 }, 
 {
  "content": "\nUsing Monitoring Dashboards to Change Behaviour -- [After years of neglect] One day we wrote some brittle Ruby scripts that polled various services. They collated the metrics into a simple database and we automated some email reports and built a dashboard showing key service metrics. We pinpointed issues that we wanted to show people. Things like the login times, how long it would take to search for certain keywords in the app, and how many users were actually using the service, along with costs and other interesting facts. We sent out the link to the dashboard at 9am on Monday morning, before the weekly management call. Within 2 weeks most problems were addressed. It is very difficult to combat data, especially when it is laid out in an easy to understand way. \nQuiet Mitsubishi Cars -- noise-cancelling on phone calls by using machine learning to build the filters.\nNSF Requiring Public Access -- NSF will require that articles in peer-reviewed scholarly journals and papers in juried conference proceedings or transactions be deposited in a public access compliant repository and be available for download, reading, and analysis within one year of publication.\nFiltered for Capital (Matt Webb) -- It's important to get a credit line [for hardware startups] because growing organically isn't possible -- even if half your sell-in price is margin, you can only afford to grow your batch size at 50% per cycle... and whether it's credit or re-investing the margin, all that growth incurs risk, because the items aren't pre-sold. There are double binds all over the place here.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/rkXlQZsx0y8/four-short-links-19-march-2015.html", 
  "title": "Four short links: 19 March 2015"
 }, 
 {
  "content": "\nTechnology has had a cult of newness for centuries.  We hail innovators, cheer change, and fend off critics who might think new and change are coming too fast.  Unfortunately, while that drives the cycle of creation, it also creates biases that damage what we create, reducing the benefits and increasing the costs.\nFormerly new things rapidly become ordinary \"plumbing,\" while maintenance becomes a cost center, something to complain about.  \"Green fields\" and startups look ever more attractive because they offer opportunities to start fresh, with minimal connections to past technology decisions.\nThe problem, though, is that most of these new things \u2014 the ones that succeed enough to stay around \u2014 have a long maintenance cycle ahead of them. As Axel Rauschmayer put it:\n\"People who maintain stuff are the unsung heroes of software development.\"\n\nIn a different context, Steve Hendricks of Historic Doors pointed out that:\n\"Low maintenance is the holy grail of our culture. We've gone so far that we're willing to throw things away rather than fix them.\"\n\nThat gets especially expensive. Heaping praise on the creators of new things while trying to minimize the costs of the maintainers is a recipe for disaster over the long term.\nmore\nI've contributed to this disaster. I love starting new projects on open fields, full of visions for creating from scratch. I can choose my own tools, tackle problems from the angles I prefer, and, perhaps most important, choose the order in which I add new features.  In the books I write, I assume the reader is starting from scratch, able to learn features in isolation and slowly develop a sense of how it all fits together.\nEven as I've spent years telling the story of starting from a clean slate, though, I've spent most of my programming and Web time on maintenance, updating my own and other people's content, code, servers, and networks.  Maintenance constantly calls me out of my comfort zone, forcing me to remember things I did long ago or focus on parts that used to just work.  Maintenance forces me to rely on other people \u2014 the real heroes \u2014 on a regular basis, often in emergency situations.\nSo how can we fix it?\nThis is a cultural problem, not a technical problem.  More precisely, the problems have technical aspects, but those aspects vary wildly from project to project.  Maintaining AngularJS projects is very different from maintaining COBOL projects.  The cultural stories, though, may be more similar.\nSomeone at CSSDevConf last fall took the first step on that path, asking the audience how many of them moved from new project to new project and how many stayed on projects for a long time going forward.  That prompted some blunt conversations about the differences among Web developers and their priorities, though it was clear that it was just a first step.\nDevOps \u2014 I think because it values operations \u2014 takes maintenance seriously, insisting on covering the whole lifecycle of a product rather than the waterfall vision of perfection at the deployment of a project.  Iteration and continuous development models put a gloss of new and fresh on what used to seem old and blocking. \nDevOps eases the problem and gives some of us a place to talk about it.  Most of us, though, work in places whose projects have not been and likely will never be under the DevOps umbrella.  It's also possible that projects that start in full DevOps mode dwindle and find themselves in more traditional maintenance.  While I hope DevOps approaches will prove contagious, something smaller seems like it might be more important.\nWe need to place a much higher value on maintenance and maintainers.  We need to worship maintainers with the same fervor we worship creators.  We need to recognize that not having emergencies can mean more, despite being less visible than a well-handled emergency.  We need to value continuity (in a bumpy world) as much as we value growth.  We need to recognize that growth is pointless unless we provide it with a solid foundation.\n\nEditor's note: if you're interested in finding out if a DevOps approach will prove to be contagious within your organization, you'll want to check out the Effective DevOps training session at Velocity in Santa Clara May 27-29, 2015.\nThis post is part of an ongoing exploration into cross-pollinating Web communities.\nCropped switch image CC BY-SA 2.0 Andrew Hart via Flickr.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/TwH0QxFGOf8/worship-maintainers.html", 
  "title": "Worship maintainers"
 }, 
 {
  "content": "Experts from across the software architecture world came together in Boston for the O'Reilly Software Architecture Conference 2015. Below we've assembled notable keynotes, interviews, and insights from the event.\nSoftware architects: post-\"post-useful\"\nThe old notion of a software architect being a non-coding, post-useful deep thinker is giving way to something far more interesting, says Neal Ford, software architect and meme wrangler at ThoughtWorks. \"Architecture has become much more interesting now because it's become more encompassing ... it's trying to solve real problems rather than play with abstractions.\"\nmore\n\nSoftware development is an engineering discipline\n\"Software development is like an art, and it is like a science, and it is like a craft, and it is a unique thing that's never existed before,\" says Glenn Vanderburg, chief architect of LivingSocial. \"That doesn't mean it's not also engineering. And we should think of it that way.\"\n\n\nA codebase that reflects your architectural intent\nLinking software architecture with code requires collaboration from both sides, says independent consultant Simon Brown. Architects need to be on code teams and developers need to step back to see the bigger picture.\n\n\nAgile + architecture, not agile vs architecture\nIn exploring the relationship between software architecture and agile development, ThoughtWorks' Molly Dishman and Martin Fowler tackle two questions: 1. What is software architecture? 2. How do you ensure architecture is happening? \n\n\nModern business has a new goal, which means it needs a new platform\nSam Ramji, CEO of the Cloud Foundry Foundation, outlines a vision for a cloud-native application platform that helps businesses shift from the old goal of sustainable competitive advantage to the modern goal of continuous innovation.\n\n\n\"Microservices\" might become a buzzword, but the underlying ideas are important\nMicroservices appear to be in a \"buzzword bingo\" phase, says NGINX evangelist and community leader Sarah Novotny, \"but attention is being paid to a very good architectural paradigm.\" Novotny also discusses the convergence of software architecture and DevOps, and the potential for microservices to shape organizational culture.\n\n\nYou can see more keynotes and interviews in our O'Reilly Software Architecture Conference 2015 playlist.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/vEc48lKg5E4/highlights-oreilly-software-architecture-conference-2015.html", 
  "title": "Signals from the O&#8217;Reilly Software Architecture Conference 2015"
 }, 
 {
  "content": "\nWe often hear about how the tech job market is booming and has space for newcomers, but what does that mean for the developers already in the market?  In December of 2014, Fortune.com predicted that 2015 would be an excellent year for developers to change jobs. Citing Dice.com, they note that jobs are popping up all over the country. In fact, Dice\u2019s survey also reports 40% of hiring managers seeing voluntary departures, a higher number than was seen just six months earlier.\nThese are all large, general numbers. What does a job change, and the changing market, look like for individual developers? To get a better sense, I spoke with Jason Myers, who is working on our upcoming Essential SQLAlchemy, 2e title.  Jason recently went from working for the email marketing service Emma, Inc., to working for networking giant Cisco. Here, he talks about how a change like that feels, and how the market looks to him.\nmore\nHow recently have you changed jobs?\nJason Myers: I changed jobs just three weeks ago.\nWhat was your previous title, and what is your new one?\nJason Myers: My previous title was Systems Software Engineer and my current title is Software Engineer IV.\nDo you feel it's a big change between the two roles?\nJason Myers: It was quite a jump, I went from working primarily on Web and API development to work on OpenStack, CI/CD, and Linux deployment systems and installers. It was also a move from being on a smaller team in a 150 person company to a very large company. Finally, a move to working from home full-time.\nWhat other titles have you held in your career?\nJason Myers: I've been a Network Engineer, Network Design and Implementation Engineer, Systems Synergist, Manager of Information Services, Consultant, Principal Consultant, Business Consultant, Systems Architect, Intern, Software Developer, Software Engineer, Systems Software Engineer, Software Engineer IV.\nTitles do nothing for me personally.  I know they are often used to slot pay for a position, but I don't think much of them in the way of prestige.  I prefer to just be a developer who is known for his work ethic at work and in the community, and not my current title.\nWhat made you want to make this move?\nJason Myers: I've been following the OpenStack project for a while as it is a huge part of the Python world at the moment, and I've wanted to work for Cisco for quite some time.  I was a Systems Architect (Servers and Networking) for much of my professional career before switching to development.\nCan you say more about moving from a small company?\nJason Myers: I think the way people feel about that is going to vary a lot from person to person.  At a large company you are not as involved in each individual decision; however, that means you can focus more on the work in front of you.  It also leads to more opportunities for research. While smaller companies can tolerate some level of research, larger companies can enable bigger budgets for longer term research. They also can bring in talented engineers from other companies to talk to your staff and provide more opportunities for learning than just outside events.\nMany industries seem to be becoming more tolerant of working from home, do you feel this is a positive move for your industry in particular?\nJason Myers: I again think this is a personal preference to some level; however, I find the current open office design to be bad for both developer productivity and a historical record of conversation.  I find that when there is a remote centric workspace there tends to be more written communication and it is more asynchronous thus allowing you to respond when you take a break versus being interrupted in the middle of a thought stream.  I've seen this done well inside of offices as well, but it often leaves remote workers off of the impromptu meeting stream, which I think is bad.\nAre job changes common?\nJason Myers: I think it depends on the locale and the job market.  I've changed a few times in the past few years as my development career has advanced and/or the startup I was working for underwent some big changes.  I think it's fairly common because there is such a demand for development talent, and especially those that are known to deliver projects and be active in the community.\nDo people make lateral moves, or do you see a willingness to try new languages/technologies?\nJason Myers: I tend to see people staying within a particular language, but I do see a lot of people complementing that with additional languages, and the type of technologies they choose to work on varies greatly.  My view of this is probably skewed because I'm so embedded in Python. I don't think I've seen too many people make just plain lateral moves unless they where leaving a situation that wasn't right for them. In Nashville, there are a LOT of web development jobs in Python, but there is also data analysis, OpenStack, etc. What I like about Nashville is there isn't a bunch of language hate, and everyone seems to want to learn additional languages. We see a lot of interest in functional languages like Elixir and F# due to our great NashFP group.\nWhat kind of resources are you aware of for developers making a career change?\nJason Myers: There are millions of resources available for online MOOCs and bootcamps, etc.  None of those can compare to participating in the local developer community. I was a fixture at many meetups and events prior to my first official job as a developer.  This meant I knew people in the field I wanted to join.  I knew who I could meet for coffee to get a deeper understanding of something.  I had a cup of coffee with Cal Evans (famous PHP Cal), and that basically started my development career. In addition, those people can point you to how they learned or offer to teach you directly.\nOne thing I want to stress though, is to get a project.  Don't just do tutorial after tutorial.  Until you have said I want to go build a thing without a guide, then put that thing live on the internet, and done it, you don't have a good picture of the actual work of a developer.\nYou organize PyTN, do you feel there are more beginners now, or has it stayed pretty steady?\nJason Myers: There are a lot of beginners, and Nashville, like a lot of places, is hungry for mid to senior talent. Some of this is due to coding being the hot thing, and some of it is because that other fields aren't really hiring right now.  I'm glad we have this influx of beginners, and I hope they stay for the long haul.  However, I hope they continuously train because we need those mid to senior people to develop higher order solutions and help companies progress.  I'm not an idealist about open source, I know it takes money to get somewhere.\nWould you consider moving to a completely different language/technology if the opportunity was interesting enough?\nJason Myers: Absolutely, provided the community around that language/technology was supporting and open.  I gravitate towards Python because of the community and the need for it in our area, but previously I really enjoyed working in PHP and C#.  Languages and technologies are just tools for a developer, and given enough time they will ebb and flow through our career.\nI do not tend to just go jump on the hot new bandwagon. I like Node, and help organize a conference on it in Nashville. However, I don\u2019t think I'd want to work full-time in that community until the rapid pace of change slows to a more maintainable state.  I feel like we have several stable and maintainable technologies and Node will be joining them very soon. Nonetheless, my background as a systems person means that I tend to think about how to actually support, tune, and mature a piece of code rather than rewrite it in a new language, until that rewrite is also maintainable and required.\nAny final thoughts on the developer job market, or where you think it\u2019s going?\nJason Myers: I think the market is going to continue to be challenging for employers and great for developers.  There are still tons of beginners around filling that need, and I'm hoping more companies will pick them up, and devote resources to leveling them up more. It's still easy for mid to senior level engineers to move to other companies as they wish.\nHowever, my hope is that some stability occurs and there is less shuffle and a greater effort made to retaining that talent. As an older developer (I guess 35 is older), I'd love to see fewer developers demanding superficial perks, and more of them asking to have more time to contribute to the open source products we use, mentor young developers, and learning more about the space they occupy. All of those result in us growing as developers in more than just our coding skills.\nI'm not sure how preachy this sounds, so some might view it as... get off my lawn and be good humans.\n\nThis post is part of our ongoing exploration into what it means to actually be a software engineer.\nPublic domain snake illustration via Internet Archive.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/C78-mjxoQ6E/a-software-engineers-role-traversal.html", 
  "title": "A software engineer&#8217;s role traversal"
 }, 
 {
  "content": "\nI recently sat down with Khoi Vinh, vice president of user experience at Wildcard and co-founder of Kidpost. Previously, Vinh was co-founder and CEO of Mixel (acquired by Etsy, Inc.), design director of The New York Times Online, and co-founder of the design studio Behavior, LLC. Our conversation included a discussion of career paths; the much talked about new interaction model, cards; and advice for design entrepreneurs.\nCuriosity serves designers well\nVinh and I discussed the ever-evolving role of designers. He recently self-published How They Got There, a book of interviews with interaction designers who describe their career paths and offer advice and insight. Vinh explained:\n\"How They Got There is kind of like the book I wish I could have read when I was just starting out in my career. The central thesis is that very few careers are truly planned out, A to B, to C, to Z, and it's usually a lot of stuff that just happens by circumstance or blind luck, or through someone who knows someone.\n\"As I became more and more aware of that in my career, I started to find those stories really interesting, really revealing, because they say so much about the character of people who achieve notoriety in their careers; the circumstances that led them to where they are can be fascinating. In a lot of instances, the things that get these people onto these paths are very, very minor events or minor coincidences. \u2026 There's a serendipity, but I think, one thing that comes out when you read these stories is what serves these designers really well is curiosity, a willingness to be available to opportunities, so to speak. They go with the flow. They let one thing turn into another through their ability to acclimate themselves to various situations.\n\"What's that old saying from Branch Rickey \u2014 \u201cluck is the residue of design\u201d? These careers are somewhat serendipitous, but they are really the result of folks who are very conscientious about making the most of whatever situation they had and working really hard and applying themselves, and looking at the world around them with great curiosity and being really willing to study what it takes to get to the next level.\u201d\nmore\nReinventing the mobile Web, one card at a time\nVinh is working on creating the next generation of the mobile Web with his company Wildcard.\nCards, the emerging interaction model perhaps made most famous by Tinder, provide a faster, efficient, and \u2014 some would say \u2014 more natural way of accessing and interacting with information. Vinh has a post that includes a presentation from Chris Tse on just what cards are. (Here\u2019s another great talk by Chris Tse.) In our conversation, Vinh shared Wildcard\u2019s goals and vision:\n\u201cThe high-level message here is, we're trying to build a better mobile Web. Our belief is that the metaphor of pages for navigating the Internet worked really well on the desktop Web, but when it comes to your phone, navigating pages can be really painful and inefficient. That's why the mobile browser on your phone, whether it's Safari or Chrome, or even mobile Firefox, it's probably the least loved of the apps on your phone relative to how often you use it. That's because Web pages on a phone are slow, they're not particularly robust, they're constantly reloading \u2014 they're fragile.\n\"We want to bring a native-app level of richness in terms of interactivity and responsiveness, and robustness to the mobile Web; we want you to be able to call upon that kind of functionality on the fly without having to download a big native app from one of the app stores to do that. I think that's one of the fundamental drawbacks of the native app ecosystem.\n\"We're kind of trying to forge a third path, where we have our browser, it's a native browser on iOS, you open it up and you can type in a brand or a publisher and grab just the content that you need from their website; we'll pull it up and format it into what we call cards, which are standard and very fast \u2014 they're built on native code, and they give you this access without all of the performance penalties of a Web browser. We're working to create an index of countless cards both on our servers and also trying to work with publishers and merchants and brands out there to 'cardify' their content so that there's a rich Internet of cards to call upon. That's the big vision of what we're trying to do.\u201d\nAdvice for designer-founders\nMore and more, we\u2019re seeing designers start their own companies. Vinh talked about the mix of skills needed for any designer-founder to have a shot at success:\n\u201cIt's more likely that a designer will start a company today than at any time before. I think that's really, really wonderful because it signals that design is more necessary than ever, and if you have strong design skills, you have a better chance of making a successful business now than you did in the past 10 or 20 years. I think the call for designers to become founders, on the whole, is a good thing, but I think designers really have to understand what it means to become a founder and to take on the challenge of building a company, not just building a product or just designing a product. I think that's a really important distinction that can be hard for a lot of designers to traverse. Understanding all of the implications of being responsible for a company, not just being responsible for a design, really understanding that making sure that company survives from day-to-day, week-to-week, month-to-month is a whole different challenge. Design can be a big part of that and can really help it, but it's not always the complete solution. I think that's important for designers, especially young designers, to understand as the drumbeat of designers becoming founders gets louder every year.\u201d\n\nYou can listen to the entire interview on the SoundCloud player above or on our SoundCloud stream.\nThis interview is part of our ongoing investigations into Experience Design and Business and Experience Design and the Internet of Things.\nCropped image on article and category pages by Ted Eytan on Flickr, used under a Creative Commons license.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/cotJlUR6FDA/the-magic-design-sauce-curiosity-and-serendipity.html", 
  "title": "The magic design sauce: curiosity and serendipity"
 }, 
 {
  "content": "\nHow to Make Moonshots (Astro Teller) -- Expecting a person to be a reliable backup for the [self-driving car] system was a fallacy. Once people trust the system, they trust it. Our success was itself a failure. We came quickly to the conclusion that we needed to make it clear to ourselves that the human was not a reliable backup\u200a\u2014\u200athe car had to always be able to handle the situation. And the best way to make that clear was to design a car with no steering wheel\u200a\u2014\u200aa car that could drive itself all of the time, from point A to point B, at the push of a button.\nBillion-Dollar Math (Bloomberg) -- There's a new buzzword, \"decacorn,\" for those over $10 billion, which includes Airbnb, Dropbox, Pinterest, Snapchat, and Uber. It's a made-up word based on a creature that doesn't exist. \u201cIf you wake up in a room full of unicorns, you are dreaming,\" Todd Dagres, a founding partner at Spark Capital, recently told Bloomberg News. Not just cute seeing our industry explained to the unwashed, but it's the first time I'd seen decacorn. (The weather's just dandy in my cave, thanks for asking).\nWhat Impactful Engineering Leadership Looks Like -- aside from the ugliness of \"impactful,\" notable for good advice. \"When engineering management is done right, you're focusing on three big things,\" she says. \"You're directly supporting the people on your team; you're managing execution and coordination across teams; and you're stepping back to observe and evolve the broader organization and its processes as it grows.\" \ncxxnet -- \"a fast, concise, distributed deep learning framework\" that scales beyond a single GPU.\n\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/-CJkIPYfiR0/four-short-links-18-march-2015.html", 
  "title": "Four short links: 18 March 2015"
 }, 
 {
  "content": "\nData scientists frequently find themselves dealing with high-dimensional feature spaces. As an example, text mining usually involves vocabularies comprised of 10,000+ different words. Many analytic problems involve linear algebra, particularly 2D matrix factorization techniques, for which several open source implementations are available. Anyone working on implementing machine learning algorithms ends up needing a good library for matrix analysis and operations.\nBut why stop at 2D representations? In a recent Strata + Hadoop World San Jose presentation, UC Irvine professor Anima Anandkumar described how techniques developed for higher-dimensional arrays can be applied to machine learning. Tensors are generalizations of matrices that let you look beyond pairwise relationships to higher-dimensional models (a matrix is a second-order tensor). For instance, one can examine patterns between any three (or more) dimensions in data sets. In a text mining application, this leads to models that incorporate the co-occurrence of three or more words, and in social networks, you can use tensors to encode arbitrary degrees of influence (e.g., \u201cfriend of friend of friend\u201d of a user).\nBeing able to capture higher-order relationships proves to be quite useful. In her talk, Anandkumar described applications to latent variable models \u2014 including text mining (topic models), information science (social network analysis), recommender systems, and deep neural networks. A natural entry point for applications is to look at generalizations of matrix (2D) techniques to higher-dimensional arrays.more For example, the image that follows is an attempt to illustrate one form of eigen decomposition:\n[caption id=\"attachment_74836\" align=\"aligncenter\" width=\"570\"] Spectral decomposition of tensors. Source: Anima Anandkumar, used with permission.[/caption]\nTensor methods are accurate and embarrassingly parallel\nLatent variable models and deep neural networks can be solved using other methods, including maximum likelihood and local search techniques (gradient descent, variational inference, EM). So, why use tensors at all? Unlike variational inference and EM, tensor methods produce global and not local optima, under reasonable conditions. In her talk, Anandkumar described some recent examples \u2014 topic models and social network analysis \u2014 where tensor methods proved to be faster and more accurate than other methods.\n[caption id=\"attachment_74838\" align=\"aligncenter\" width=\"570\"] Error rates & recovery ratios from recent community detection experiments \u00a0(running time measured in seconds). Source: Anima Anandkumar, used with permission.[/caption]\nScalability is another important reason why tensors are generating interest. Tensor decomposition algorithms have been parallelized using GPUs, and more recently using Apache REEF (a distributed framework originally developed by Microsoft). To summarize, early results are promising (in terms of speed and accuracy), and implementations in distributed systems lead to algorithms that scale to extremely large data sets.\n[caption id=\"attachment_74837\" align=\"aligncenter\" width=\"570\"] General framework. Source: Anima Anandkumar, used with permission.[/caption]\nHierarchical decomposition models\nTheir ability to model multi-way relationships make tensor methods particularly useful for uncovering hierarchical structures in high-dimensional data sets. In a recent paper, Anandkumar and her collaborators automatically found patterns and \u201c...concepts reflecting co-occurrences of particular diagnoses in patients in outpatient and intensive care settings.\u201d\nWhy aren\u2019t tensors more popular?\nIf they\u2019re faster, more accurate, and embarrassingly parallel, why haven\u2019t tensor methods become more common? It comes down to libraries. Just as matrix libraries are needed to implement many machine learning algorithms, open source libraries for tensor analysis need to become more common. While it\u2019s true that tensor computations are more demanding than matrix algorithms, recent improvements in parallel and distributed computing systems have made tensor techniques feasible.\nThere are some early libraries for tensor analysis in MATLAB, Python, TH++ from Facebook, and many others from the scientific computing community. For applications to machine learning, software tools that include tensor decomposition methods are essential. As a first step, Anandkumar and her UC Irvine colleagues have released code for tensor methods for topic modeling and social network modeling that run on single servers.\nBut for data scientists to embrace these techniques, we\u2019ll need well-developed libraries accessible from the languages (Python, R, Java, Scala) and frameworks (Apache Spark) we\u2019re already already familiar with. (Coincidentally, Spark developers just recently introduced distributed matrices.)\nIt\u2019s fun to see a tool that I first encountered in math and physics courses having an impact in machine learning. But the primary reason I\u2019m writing this post is to get readers excited enough to build open source tensor (decomposition) libraries. Once these basic libraries are in place, tensor-based algorithms become easier to implement. Anandkumar and her collaborators are in the early stages of porting some of their code to Apache Spark, and I\u2019m hoping other groups will jump into the fray.\nTo view Anima Anandkumar\u2019s talk at Strata + Hadoop World in San Jose, Tensor Methods for Large-scale Unsupervised Learning: Applications to Topic and Community Modeling, sign-up for a free trial of Safari Books Online.\nCropped image on article and category pages by Gerwin Sturm on Flickr, used under a Creative Commons license.\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/ht1iN8Yhdt8/lets-build-open-source-tensor-libraries-for-data-science.html", 
  "title": "Let\u2019s build open source tensor libraries for data science"
 }, 
 {
  "content": "\nSirius -- UMich open source \"intelligent Personal Assistant\" (aka Siri, Cortana, Google Now, etc.).  Text recognition, image recognition, query processing components.  They hope it'll be a focal point for research in the area, the way that open source operating systems have focused university research.\nMIT DragonBot Evolving to Teach Kids (IEEE Spectrum) -- they're moving from \"Wizard of Oz\" (humans-behind-the-scenes) control to autonomous operation. Lovely example of Flintstoning in a robotics context.\nPersonal Assistants Coming (Robohub) -- 2015 is the year physical products will be coming to market and available for experimentation and testing. Pepper ships in the summer in Japan, JIBO ships preorders in Q3, as does Cubic in the fall and EmoSpark in the summer. [...]The key to the outcome of this race is whether a general purpose AI will be able to steer people through their digital world, or whether users would rather navigate to applications that are specialists (such as American Airlines or Dominos Pizza).\nIncuriosity Killed the Infrastructure -- being actively curious about \u201cfishy\u201d things will lead to a more stable and happy infrastructure.\n\n\n    \n", 
  "link": "http://feedproxy.google.com/~r/oreilly/radar/atom/~3/VM-n8kpcHpU/four-short-links-17-march-2015.html", 
  "title": "Four short links: 17 March 2015"
 }
]